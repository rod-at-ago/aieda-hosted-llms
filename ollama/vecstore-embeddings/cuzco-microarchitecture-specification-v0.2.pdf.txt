 

   

 

 

 

 

 

 

 

 

 

Condor Computing Specification 

Cuzco Processor Core 

Microarchitecture 

Version 0.21 

Based on Thang Tran’s SimplexMicro Version 1.4  

 

This document is exclusive property of Condor Computing Inc. and shall not be distributed, reproduced, or 
disclosed in whole or in part without prior written permission of Condor Computing Inc.  
July 11, 2023 

 

Cuzco Microarchitecture Specification, v0.2 

 

 

CONTENTS 

MISS-REQUEST QUEUE (MRQ) WITH DMQ AND DEQ ...................................................................................... 120 
 

 

 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               2 

Cuzco Microarchitecture Specification, v0.2 

 

 

List of Tables 
TABLE 2-1. SRAM ARRAY SIZES ............................................................................................................................ 15 
TABLE 3-1. FRONT-END EXCEPTION LISTING ............................................................................................................ 22 
TABLE 3-2. LOAD-STORE EXCEPTION LISTING ........................................................................................................... 27 
TABLE 4-1. BTB BIT FIELDS .................................................................................................................................. 39 
TABLE 4-2 BRANCH TYPE AND PREDICTED DIRECTION SOURCE .................................................................................... 39 
TABLE 4-4. RETURN STACK BIT FIELDS .................................................................................................................... 45 
TABLE 4-5. BPQ BIT FIELDS ................................................................................................................................. 51 
TABLE 4-6. BRANCH PREDICTION AND EXECUTION .................................................................................................... 57 
TABLE 5-1. ITQ ENTRY BIT FIELDS ......................................................................................................................... 79 
TABLE 5-2. IMQ BIT FIELDS ................................................................................................................................. 80 
TABLE 5-3. ICQ BIT FIELDS................................................................................................................................... 82 
TABLE 6-1. ROB BIT FIELDS ................................................................................................................................. 94 
TABLE 6-2. REGISTER SCOREBOARD BIT FIELDS ...................................................................................................... 100 
TABLE 6-3. GREATER-THAN COMPARATOR FOR TIME A AND B ................................................................................. 104 
TABLE 6-4. READ TIMES CALCULATION FOR 3 INSTRUCTIONS ..................................................................................... 105 
TABLE 8-1. XEQ BIT FIELDS ................................................................................................................................ 116 
TABLE 8-2. DATA BIT FIELDS FOR ARITHMETIC AND BRANCH XEQ ............................................................................. 117 
TABLE 8-3. LOAD/STORE XEQ WITH ADDITIONAL BIT FIELDS.................................................................................... 119 
TABLE 10-1. LSU CONFIGURATION PARAMETERS ................................................................................................... 123 
TABLE 10-2. STB BIT FIELDS .............................................................................................................................. 130 
TABLE 10-3. LDB BIT FIELDS .............................................................................................................................. 134 
TABLE 10-4. MRQ BIT FIELDS ............................................................................................................................ 136 

 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               3 

Cuzco Microarchitecture Specification, v0.2 

 

 

List of Figures 
FIGURE 3-1 HIERARCHICAL RTL MODULES .............................................................................................................. 18 
FIGURE 4-1 IFU AND BPU PIPELINE ....................................................................................................................... 28 
FIGURE 4-2 BTB PREDICTION WITH PAGE BUFFER BLOCK DIAGRAM ............................................................................ 32 
FIGURE 4-3 ILLUSTRATION OF BASIC BLOCKS ............................................................................................................ 34 
FIGURE 4-4 LARGE AND SMALL BASIC BLOCKS.......................................................................................................... 35 
FIGURE 4-5 BASIC BLOCK EXAMPLE 1 ..................................................................................................................... 36 
FIGURE 4-6 BASIC BLOCK EXAMPLE 2 ..................................................................................................................... 37 
FIGURE 4-7 BASIC BLOCK EXAMPLE 3 ..................................................................................................................... 37 
FIGURE 4-8 BASIC BLOCK WITH LOOP PREDICTION .................................................................................................... 42 
FIGURE 4-9 COMBINING MULTIPLE BASIC BLOCKS INTO LOOP PREDICTION ................................................................... 43 
FIGURE 4-10 GHT ARRAY .................................................................................................................................... 44 
FIGURE 4-11 EXAMPLE OF RECURSIVE CALLS ........................................................................................................... 45 
FIGURE 4-12 EXAMPLE 1: 4 CALLS AND 4 RETURNS WITH ALL BTB MISS AND BRANCH MISPREDICTION ............................. 47 
FIGURE 4-13 EXAMPLE 2: 4 CALLS AND 4 RETURNS WITH ALL BTB HITS AND CORRECT BRANCH PREDICTIONS .................... 48 
FIGURE 4-14 EXAMPLE 3: 4 CALLS AND 4 RETURNS AND ADDITIONAL JUMP, ALL BTB MISSES .......................................... 49 
FIGURE 4-15 EXAMPLE 4: 4 CALLS AND 4 RETURNS WITH ALL BTB HITS AND ADDITIONAL JUMPS WITH MISPREDICTION ........ 50 
FIGURE 4-16 FRONT-END PIPELINE ........................................................................................................................ 53 
FIGURE 4-17 EXAMPLE 1, BASIC BLOCK WITH MORE THAN 2 CACHE LINES..................................................................... 54 
FIGURE 4-18 EXAMPLE 2, BASIC BLOCK WITH 2 CACHE LINES ...................................................................................... 54 
FIGURE 4-19 EXAMPLE 2, BASIC BLOCK WITH SINGLE CACHE LINE ................................................................................ 55 
FIGURE 5-1 STATE MACHINE FOR READING OF THE TAG ARRAYS ................................................................................. 79 
FIGURE 5-2 WRITING CACHE LINE DATA INTO ICQ ................................................................................................... 82 
FIGURE 5-3 READING OF 4 INSTRUCTIONS (UP TO 8 ENTRIES) FROM ICQ ..................................................................... 83 
FIGURE 5-4 ICQ LOOP IMPLEMENTATION ............................................................................................................... 84 
FIGURE 5-5 TLB BLOCK DIAGRAM ......................................................................................................................... 85 
FIGURE 5-6 INSTRUCTION TLB BLOCK DIAGRAM ...................................................................................................... 86 
FIGURE 6-1 IDU PIPELINE .................................................................................................................................... 88 
FIGURE 6-2 MODULAR DESIGN OF SINGLE AND DUAL INSTRUCTION MODULES .............................................................. 90 
FIGURE 6-3 READ AND WRITE DATA PATHS FROM PRF ............................................................................................. 92 
FIGURE 6-4 EXAMPLE OF REGISTER RENAMING ........................................................................................................ 99 
FIGURE 6-5 EXAMPLE OF RESTORING THE RTL AND RAT ......................................................................................... 100 
FIGURE 6-6 TIME-RESOURCE MATRIX FOR AVAILABILITIES OF RESOURCE FOR ISSUING A SINGLE INSTRUCTION ................... 106 
FIGURE 6-7 TIME-RESOURCE MATRIX FOR AVAILABILITIES OF RESOURCE FOR ISSUING DUAL INSTRUCTION ....................... 107 
FIGURE 6-8 INSTRUCTION SLICES FOR 8-ISSUE MICROPROCESSOR.............................................................................. 108 
FIGURE 6-9 ONE READ PORT QUEUE (RPQ) ......................................................................................................... 112 
FIGURE 6-10 ONE WRITE PORT QUEUE (WPQ) .................................................................................................... 113 
FIGURE 8-1 THE ALU XEQ WITH OOO EXECUTION ................................................................................................ 115 
FIGURE 8-2 REPLAY OF DEPENDENT INSTRUCTION .................................................................................................. 118 
FIGURE 10-1 LSU DATA PATH – INCLUDING REPLAY ............................................................................................... 124 
FIGURE 10-2 LOAD DATA ALIGNMENT MODULE .................................................................................................... 128 

 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               4 

Cuzco Microarchitecture Specification, v0.2 

 

 

1  Document Information 

1.1  Preface 

This document describes the high-level specification of a scalable RISC-V processor core 
microarchitecture.  The important features of this processor microarchitecture are: 

• 

• 

• 

Time-based speculative instruction issue:  a continuously running time count used by all 
instructions which are issued at preset or adjusted times in reference to the time count. 
Configurations: decoding for RISC-V standard and extension ISA, scalable 2-instruction slices, 
SRAM arrays for all queues, physical registers, and re-order buffers 
Three target markets: automotive, AR/VR, and cloud acceleration 

•  New design methodology: system Verilog, verification and backend tools for configurations 
• 

Innovative algorithm for out-of-order superscalar microprocessor design, which can be extended 
to floating-point, vector, and neural network (NN) accelerator designs 

Microprocessors have become more and more complex to chase small increments in performance at the 
expense of power and area. The microarchitecture approach used in OOO superscalar microprocessors 
has remained basically the same for the last 30 years. In this common approach much of the power 
dissipation is from the dynamic scheduling of instructions for execution from reservation stations or 
similar mechanisms. Fetch block microarchitecture with branch prediction has been similarly consistent.  
Since VLIW microarchitectures died from their required compiler complexity and insufficient 
performance, no major change is has occurred in microprocessor design. Significant innovation has 
disappeared, replaced by the “NIH” (Not Invented Here) syndrome throughout the microprocessor world.  

The microprocessor designs for recent generations for PC/laptop and smart tablet/phone have heavily 
depended on multiple cores to scale system performance. With the new markets and the new widely 
adopted free RISC-V instruction set, the opportunity for extensible and configurable microprocessors has 
arrived. A new approach in microprocessor design is needed with the following attributes: 

• 

Simplicity 
•  Modular 
• 

Programmable features: configurable and extensible 

•  Vector processing extension 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               5 

Cuzco Microarchitecture Specification, v0.2 

 

 

1.2  Revision History 

Revision 

Date 

Changed Sections 

Summary of Changes 

04/04/2023  All 

SimplexMicro - Final review 

1.0 

1.1 

Author 

Thang Tran 

05/25/2023 

3.3.2 

Add section for writing of mispredicted branch into BTB 

Thang Tran 

1.2 – 1.4 

06/28/2023  All 

Edit for clarity of text 

Edmund Gieske 

0.0-0.01 

07/11/2023  All 

Starting point for Cuzco based on SimplexMicro 1.4 

Edmund Gieske 

0.1 

11/21/2023 

 

(Pulled in the following changes) 

SimplexMicro 1.5: More info on loop prediction, ROB 
handling of branch prediction, greater-than compare of 
times in RSB, steps when partial number of instructions 
are issued, store-to-load forwarding in details 

SimplexMicro 1.6: Add new chapter for CSR and FPU (high 
level) 

SimplexMicro 1.7: Add more details for interrupt and 
exceptions. Examples for return stack and register 
renaming 

Shashank 
Nemawarkar 

0.2 

11/25/2023 

5.4, 5.5, 5.8, 6.4 

Implementation details, Unknown replay, LDB,  

Thang Tran 

0.121 

 

3.2.3, 4.4.5 

Reconciled v0.1 changes—Exceptions and interrupts, 
Return Stack, 

Shashank 
Nemawarkar 

 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               6 

Cuzco Microarchitecture Specification, v0.2 

 

 

1.3  Conventions and Terminology 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               7 

Cuzco Microarchitecture Specification, v0.2 

 

 

Label 

CPU  

ISA 

OoO 

ALU 

AGU 

FPU 

IC 

DC 

BTB 

BEU 

GHT 

GHR 

BPU 

BPQ 

BPB 

IQ 

IMQ 

ITQ 

ICQ 

XIQ 

PLRU 

MMU 

RS 

TOS 

TLB 

Description 

Central Processing Unit 

Instruction Set Architecture 

Out-of-Order 

Arithmetic and Logical Unit 

Address Generation Unit 

Floating-Point Unit 

Instruction Cache 

Data Cache  

Branch Target Buffer 

Branch Execution Unit 

Global History Table 

Global History Register 

Branch Prediction Unit 

Branch Prediction Queue 

BTB Page Buffer 

Instruction Queue 

Instruction Miss Queue 

Instruction Tag Queue 

Instruction Cache Queue 

Scalar Instruction Queue 

Pseudo Least Recently Used 

Memory Management Unit 

Return Stack 

Top of Stack 

Translation Look-aside Buffer 

L2TLB 

Level-2 Translation Look-aside Buffer 

PC 

IDU 

LSU 

STB 

LDB 

DBQ 

MRQ 

DMEQ 

DMQ 

DEQ 

Program Counter 

Instruction Decode Unit 

Load Store Unit 

Store Buffer 

Load Buffer 

Data Bank Queue 

Miss Request Queue 

Data Miss and Eviction Queues 

Data Miss Queue 

Data Eviction Queue 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               8 

Cuzco Microarchitecture Specification, v0.2 

 

 

ECC 

RAW 

WAW 

WAR 

CSR 

ROB 

RSB 

TRM 

RPQ 

WPQ 

RFL 

RAT 

ART 

ARF 

GPR 

PRF 

XRF 

FRF 

x0-x31 

r0-r127 

f0-f31 

X0/x0 

XEQ 

LS XEQ 

FPEQ 

PMA 

PMP 

SIMD 

FLEN 

Error Correcting Code 

Read-After-Write data dependency 

Write-After-Write data dependency 

Write-After-Read data dependency 

Control and Status Register 

Re-Order Buffer 

Register Scoreboard 

Time-Resource Matrix 

Read Port Queue 

Write Port Queue 

Register Free List 

Register Alias Table 

Architectural Register Table 

Architectural Register File 

General Purpose Register 

Physical Register File 

Integer Register File 

Floating-point Register File 

Scalar registers 

Physical registers 

Floating-point registers 

X0 is the pipeline stage and x0 are the registers 0 

Scalar Execution Queue 

Load/Store Execution Queue 

Floating-point Execution Queue 

Physical Memory Attribute 

Physical Memory Protection 

Single Instruction Multiple Data 

Floating-point Length in bits (power of 2) 

Byte and bit 

32B = 32 bytes, 32b = 32 bits, 32KB = 32 kilo bytes 

Data size 

Byte = 8-bit, Half-word = 16-bit, Word = 32-bit, Double-word = 64-bit, Quad-word = 128-bit 

Configurable 

Parameters to be set at built time 

Programmable 

Parameters can be programmed through CSR 

Issue 

Dispatch 

Complete 

Instruction/data are issued from a queue 

Instruction/data are dispatched to the VIQ queue 

Instruction is completed and data is written back to register file.  Example: arithmetic instruction is 
completed by writing result data to PRF, but can be cancelled by branch misprediction.  

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               9 

Cuzco Microarchitecture Specification, v0.2 

 

 

 

Commit 

Retire 

Queue 

Buffer 

Instruction is committed, PC is incremented, all earlier branches are completed. Example: store instruction 
writes data to data cache only when all earlier branches are completed and all earlier load/store 
instructions are committed (can be cache miss with pending data) 

When the instruction is retired from the ROB. Example: cache miss load instruction can be retired without 
writing load data to PRF, the bus error or L2 ECC error is imprecise exception.  The RSB keeps track of 
writing load data to the PRF and stall RAW instructions 

Storage of data with many entries, mostly (but not all) FIFO structure 

Storage of data with many entries, input/output to the buffers are out-of-order 

1.4  Related Documents 

Title 

Link 

Description 

 

 

 

 

 

 

 

 

 

 

1.5  Notes/Open Issues 

Tables 9-3 & 9-4 have the same name “MRQ Bit Fields”. 

Many instances of random styles remain. 

 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               10 

Cuzco Microarchitecture Specification, v0.2 

 

 

2  Overview 

This document is the MicroArchitecture Specification (MAS) of a processor based on the RISC-V ISA.  The 
purpose of this specification is to provide a general description of the overall microarchitecture for the 
implementation of out-of-order superscalar microprocessors. Processors have become increasingly 
complex chasing small increments in performance at the expense of power consumption and 
semiconductor chip area.  The approach in designing out-of-order (OOO) superscalar microprocessors has 
remained basically the same for the last 30 years, with much of the power dissipation attributable to the 
dynamic scheduling of instructions for execution from reservation stations or central windows.  Designing 
an OOO superscalar microprocessor is a huge undertaking.  Hundreds of instructions are dispatched into 
the execution pipeline where the data and resource (e.g., register file read ports, forwarding busses, 
functional execution units) dependencies are resolved and arbitrated using complicated priority schemes 
to ensure optimal performance.  Further arbitration may occur as result data from the functional units 
later arbitrate for the register file write buses.  If the data cannot immediately be written back to the 
register file, then the result data are kept in temporary registers adding the complication of backpressure 
to the determination of functional unit availability.  The temporary registers are added to result data 
forwarding and to scheduling of dynamic instructions.  Another desirable but elusive processor 
microarchitecture attribute is scalability.  Despite repeated efforts in design reuse, each generation of 
microprocessor has required much effort to move from scalar, to 2-wide, 4-wide, 5-wide, … A truly 
scalable processor microarchitecture does not yet exist. 

The Cuzco processor architecture solves the above problems with a revolutionary innovation toward a 
“plan ahead” mechanism: 

• 

• 

• 

The time concept, employing a free running time counter, to schedule instruction execution at a 
“plan ahead” time in reference to the time count 
The “plan ahead” is partially based on the performance model concept where each new instruction 
has a perfect or oracle view of all previous instructions including their exact time of execution.   
The latest evolution of the register scoreboard (RSB) to keep track of write times of destination 
registers.  All instructions have fixed throughput and latency times, so in principle, when an 
instruction is in the decode stage, the issue logic can pre-determine the exact execution time before 
sending the instruction to the appropriate execution issue queue.  The RSB technique goes back to at 
least the CDC6600 and has continued to evolve from Thang Tran’s dissertation in 1991 to the ARM 
Cortex-A8 and the vector processor of NX27V. 

•  Another important concept is the time-resource matrix (TRM) in which a 2-dimensional matrix of 

• 

time and functional resources indicate the availability of the resources with respect to time.  In a way, 
this is the time scoreboard for various resources. 
Time-resource matrices are grouped into 1-instruction or 2-instruction slices.   Each slice includes the 
time-resource matrices, the execution queues, the functional units, a load/store port, and the read 
and write ports to the register file.  The design of a slice is simpler for both timing and logic.  
Processor scalability is based on combinations of slices to create any desired issue width.  Note that 
the amount of certain functional units might be overprovisioned but the routing complexity is 
minimized which matches technology scaling. 

Time (clock) concept: A time counter is used to indicate the current time in the processor.  It is a rotating 
counter. All times in the processor are relative to this time count. For example, if the time count is 22, and 
an instruction will be executed and written back to PRF in 5 clock cycles, then the time set for the 
instruction is 27, when the time count is 27, the instruction will be executed and write back to PRF.  The 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               11 

Cuzco Microarchitecture Specification, v0.2 

 

 

write time (27) of pending destination register in execution pipeline are recorded in the register 
scoreboard which is the read time for RAW dependent instructions.  The time-resource matrices are used 
to record the busy status of the resources in the future. If the instruction is free of RAW data dependency 
and the resources are available, then the instruction is issued with preset execution time. The 
combination of RSB and TRM provides the time view of all previous instructions which have been 
scheduled for execution. Put differently, the “plan ahead” scheduling using the RSB and TRM enable the 
usually complex and power intensive wakeup and selection of instructions among all those in the issue 
queue(s) to be simplified into a single equality comparison with the current time.  

Out-of-order execution concept: Conditions for execution instructions regardless of instruction ordering: 

• 

Free of data dependency, in this case, only RAW (renaming handles WAW and WAR) which based 
on the RSB 

•  Availability of read bus(es) to read data from the PRF 
•  Availability of functional unit to execute the instruction 
•  Availability of write bus to write data to the PRF 

The goal for this project is the best PPA (performance includes both IPC and clock frequency) for range of 
configurations for high end compute applications. The microprocessor includes the extensible 
instructions, both custom and standard extensible instructions such as multiply, divide, floating point, 
DSP, and vector. The objectives of the design are listed below: 

• 

Configurability: necessary for IP to attract a wide range of customers. The RISC-V ISA allows 
custom extensible instructions which are added by customers. This enables customers to create 
proprietary capabilities and end-product differentiation 

o  Tools such as lint should be adapted to such configurability. Fixing a bug in one 

o 

configuration should not break lint in other configurations 
Simple regression code for all configurations should be continually running in the 
background 

• 

Scalability: This is also part of the configurability.  The design parameters can be modified for 
specific application performance:  

o  Configurable number of instruction-slices 
o  Extensive use of local parameters for scalability where local parameters are from an 

Excel table 

• 

Low power and area: necessary for embedded market 

o  Multiple levels (4-6) of clock gating 
o  Plan-ahead scheduling of instructions: preset exact time for read ports, write ports, and 

execution time by specific functional unit 

• 

• 

Performance: out-of-order execution provides the highest possible performance 
Simplicity:  It is the most difficult to have a simple design with highest performance: 

o 

Innovative yet simple pipeline control using scoreboard and time-resource matrices 
Issued instructions are “fire-and-forget” replayed only by issue logic 

o 
o  Excel work sheet used to generate RTL, define parameters, instruction decode and state 

machine – easy to modify and simple to debug 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               12 

Cuzco Microarchitecture Specification, v0.2 

 

 

2.1  Features 

The plan-ahead scheduling algorithm is based on the known latency and throughput times of the 
instructions.  Each issued instruction has 3 associated times: (1) read time to fetch data from register file 
to the execution unit, (2) execution time is 1 cycle later, and (3) write time to write result data back to the 
register file.  The write time is the latency time of the instruction added to the read time.  Load 
instructions use DC hit latency as their execution time.  For an instruction with a RAW data dependency 
on a prior instruction, the read time of the dependent instruction is the write time of the prior instruction 
as recorded in the RSB.  This “plan-ahead” algorithm is very power efficient in comparison to the dynamic 
scheduling. The SM processor microarchitecture focuses on the pipeline execution and control for various 
type of instructions: 

• 

Time is from a 5 to 7 bits time counter, (i.e., 32 to 128 time counts) 

•  RV64IAM is implemented as the basic ISA.  The data width is configurable and can be replaced 

• 

• 

• 

• 

• 

• 

with RV32IAM 
1-8 wide OOO superscalar microprocessor. A 1-wide OOO microprocessor can outperform a 2-
wide in-order microprocessor.  The RTL design consists of modular 1-instruction and 2-instruction 
slices. 
The resources (buses and functional units) in the 1-wide slice: 2 read ports, 1 write port, 1 ALU, 1 
BRN, 1 LS port, and the extensible functional units MUL and DIV.  Note that DC tag array is 
duplicated for each LS port 
The resources (buses and functional units) in the 2-wide slice: 4 read ports, 2 write ports, 2 ALU, 
1 BRN, 1 LS port, and the extensible functional units MUL and DIV.  Note that DC tag array is  
duplicated for each LS port 
The TRMs are designed for the above 1-wide and 2-wide slices 
The execution queues (XEQ) are designed for the above 1-wide and 2-wide slices. The number of 
entries in the XEQ is configurable (2, 4, 8, 16) based on the functional unit types.  The XEQ 
consists of the instruction read time to issue instruction to the functional units.  The instruction 
can be replayed by modifying the read time of the instruction in the XEQ 
The RSB records the write times of instructions with valid destination registers.  The RSB has 
same number of entries as the PRF. The number of registers in the PRF is configurable 

•  Register renaming is used to avoid WAW and WAR data dependency while RAW is handled by 

RSB 

•  ROB is used to allow OOO branch and load execution 
• 

“Fire-and-forget” execution, instructions once issued will be executed and the results may be 
discarded if the instructions are replayed or cancelled by branch misprediction or 
interrupt/exception 
The DC data array has configurable number of banks to minimize bank conflicts 
ECC or Parity is implemented for all caches and other large bit-cell array structures (e.g., L2 TLB) 
•  Advanced basic-block branch prediction unit (BPU) along with loop prediction (a natural part of 

• 

• 

the basic block branch prediction) are implemented 
L2/L3 cache is configured to be implemented as back-side L2, shared L2, and shared L3. 
Coherency should be implemented in L2/L3 cache to minimize DC tag contention 
PMA/PMP, iTLB (small), dTLB (small), and L2 TLB are accessed prior to tag array to avoid address 
aliasing 
4-5 levels of hierarchical clock gating 

• 

• 

• 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               13 

Cuzco Microarchitecture Specification, v0.2 

 

 

2.2  Design Configurations 

All configuration parameters are set in an Excel file which is converted to the defines.vh file.  
Configuration is performed and fixed at RTL generation time and cannot change during run time: 

•  Number of instructions: 1-8, 4 (default),  

• 

• 

Instruction cache size: 8KB, 16KB, 32KB (default), 64KBData cache size: 8KB, 16KB, 32KB (default), 
64KB 
Cache line size: 128b, 256b (default), 512b, 1024b 

•  Bus width: 128b, 256b (default), 512b 
• 

L2 TLB size: 512, 1K (default), 2K, 4K 
TLB translation: on, off (default) 

• 

•  Branch target buffer size: 1K, 2K, 4K, 8K  (default) 
•  BTB page buffer:  32, 64, 128 (default), 256 
•  Branch history table size: 2K, 4K, 8K, 16K (default) 
•  All arrays error detection: none (default), ECC, parity-per-byte 

The design configuration is based on performance model for the optimal performance which is different 
than the customer-built configuration.  Note: in most cases, the queue full is part of the performance 
monitors and verification and should be printed out in order to check for unpredictable conditions; the 
RS, BPQ, XEQ, STB, LDB, MRQ, IMQ, DMQ, DEQ, ROB, and RFL.  The initial list of design configuration 
which should be validated by performance: 

•  Global history register (GHR): 10-bit 
•  Return Stack (RS): 8 (default), 12, 16 entries 
•  Branch prediction queue (BPQ): 8, 16 (default), 32 entries 
• 

• 

• 

• 

• 

Instruction tag queue (ITQ) and Instruction miss queue (IMQ): 4 entries of each cache line 
Instruction prefetch queue (IPB): 1 entry of cache line size 
Instruction cache queue (ICQ) entry: 16 entries of 16-bit instructions (same as cache line size) 
Integer instruction queue () entry: 8 (2X issue width) 
Physical register entries: 48, 64, 96, 128 (default), 31 architectural registers and 17/33/65/97 
renamed registers 

•  Number of read ports: 2X issue width 
•  Number of write ports: 1X issue width 
•  Re-Order Buffer: 16, 32, 64, 96, 128 (default) 
•  ALUs: 1X issue width 
•  Branch, multiply, and divide functional unit: 1 per instruction slice 
•  Number of data cache ports and tag arrays: 1 per instruction slice 
• 

Execution Queue (XEQ) entries per 1-wide slice: 2 (MUL/DIV), 4 (Branch), 8 (Load/store), 8 (ALUs) 
Execution Queue (XEQ) entries per 2-wide slice: 4 (MUL/DIV, 8 (Branch), 16 (Load/store), 16 
(ALUs) 
Store buffer (STB) and load buffer (LDB): 16, 32 (default), 64 entries 

• 

• 

•  Miss request queue (MRQ): 8, 16 (default), 32 entries 
•  Data Miss Queue: 2, 4, 8 (default) entries of cache line size 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               14 

Cuzco Microarchitecture Specification, v0.2 

 

 

•  Data Eviction queue: 1, 2 (default), 4 entries of cache line size 
•  All arrays way associative: 2, 4 (default), 8 
•  All arrays way replacement algorithm: random (initial), PLRU (default) 
• 

Instruction and data L1 TLB (fully associative): 8, 16 (default) 

•  Return stack: 4, 8 (default), 16 

Note that the sizes for BTB, cache tag array, L2 TLB has the same organization (number of SRAM rows) for 
simplicity. The sizes for all SRAM arrays are in the below table: 

Name 

Size  

Description  

Cache size 

32KB 

Both instruction and data caches for simplicity of initial implementation 

Cache line size 

32B 

Same as instruction and data bus width to external memory for simplicity 

Cache banks 

Way associative 

iTLB & dTLB 

L2 TLB 

BTB  

4 

4 

16 

1K 

2K  

16 sets of 256x64 

For all arrays, random replacement, change to PLRU later 

Register file structure, not SRAM, page size is 4KB 

4 sets of 256x52 (physical address) and 256x55 (virtual address + states) 

4 sets of 256x96 (2 banks of 4 ways of 48 bits) 

GHT 
Table 2-1. SRAM Array Sizes 

16K 

2 sets of 256x128 for taken-bit and strength-bit arrays, GHR is 12 bits 

2.3  Key core microarchitecture performance targets 

A set of initial core microarchitecture performance targets are: (from Shashank’s original presentation) 

•  Nominal Branch Predictor Accuracy: 99.9% (i.e., 1 incorrect fetch address per 1000 branches) 
•  Branch misprediction fetch redirect latency: <=10 cycles 
•  Branch Prediction to Instruction Dispatch latency: <=8 cycles 
• 

L1D$ access load to use latency: <=4 cycles (NOTE: Should this be restated as overall average 
load-use latency for a chosen set of workloads?) 
L2 access latency: 8 cycles for “critical” L1I$ or L1D$ misses, <=11 cycles for non-critical 

• 

•  Bandwidth between the core and L2: 64B Read + 64B Write / core cycle 
• 

Core frequency: >=2.75GHz (NEED to specify: process, process corner, junction temperature, 
margined supply voltage) 

2.4  Changes Relative to Other Andes RISC-V Processors 

TBD. 

2.5  Audience 

For architects, block microarchitects, verification, OS and SW enablement, and eventually the source 
material for customer integration and user manuals. 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               15 

Cuzco Microarchitecture Specification, v0.2 

 

 

2.6  Scope 

<As needed, document topic scope>  

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               16 

Cuzco Microarchitecture Specification, v0.2 

 

 

3  Pipeline Overview 

The heart of the Cuzco microprocessor is a time counter for plan-ahead scheduling of instructions, a RSB 
for resolving data dependency of registers, and a TRM to provide the future availability of all resources. 
The RSB and TRM provide the time view of all prior issued instructions. An instruction is scheduled for 
execution at a preset time based on the time count and dispatches to the execution queue. Once the 
instruction is dispatched to the execution queue, it will be executed by the associated functional unit at 
the preset time. Prior to execution, instructions will check their source operands in the RSB again to 
ensure that the register status has not been changed due to data cache delay or miss.  If the register 
becomes “not ready”, then the instruction will be rescheduled with a new, later ready time at which it will 
replay.  The same algorithm used for first issuing an instruction is used for replaying the instruction until it 
successfully executes. 

Using the default parameters the notional Cuzco microprocessor pipeline is: 

• 

• 

• 

• 

• 

• 

• 

• 

• 

IF0: Calculate next address calculation and access iTLB  
IF1: Access IC tag array (IC hit/miss), BTB, and GHT 
IF2: Access IC data array access, BTB hit/miss and taken/non-taken 
IF3: Write IC cache line to ICQ and bypass/read N instructions to XIQ 
ID0: Read N instructions from XIQ, first instruction decode, and access RFL and RAT 
ID1: Access RSB to calculate execution times, 2 (read_time and read_time+1) for each 
instruction, since there are two instructions per slice. 
ID2: Access TRM to issue instruction, second instruction decode, write issued instructions to XEQ 
EX0: Read register file or forward data from functional units, check RSB again, and dispatch 
instruction from XEQ to functional unit 
EX1/DC1: Execute instructions, write result data from functional unit to PRF.  AGU and access 
dTLB 

•  DC2: Access DC tag array (DC hit/miss) 
•  DC3: Access DC data array 
•  DC4: Align and send load data to write back to PRF 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               17 

Cuzco Microarchitecture Specification, v0.2 

 

 

Figure 3-1 Hierarchical RTL Modules  

3.1  High-Performance Features 

 

The OoO processors are designed to get around the possible delays for load/store instructions and branch 
instructions. In the Cuzco microprocessor, branch mispredictions are executed OoO and load/store 
instructions are non-blocking. In addition, basic-block branch prediction is used instead of the fetch-block 
based branch prediction commonly used in most microprocessors. Loop prediction is a natural part of 
basic block prediction and is included in the Cuzco microprocessor to save power by avoiding repeatedly 
fetching loops.  

Definition of a basic block: a straight-line code sequence with no branches in except to the entry and no 
branches out except at the exit.  A basic block can be a single instruction or a hundred instructions.  The 
basic block entry and exit point addresses are referred throughout the specifications as start and end/next 
addresses.  The entry point address is the lookup address to the branch target buffer (BTB) which includes 
the exit point address, target address at the exit point, and the loop prediction.   

The basic block is a more natural basis for branch prediction than fetch blocks. The shortcoming of fetch 
block based branch prediction are: (1) limits the number of branches within the fetch block, (2) a BTB 
lookup is required for every fetch block potentially wasting power, (3) more difficult to set up loop 
prediction.   

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               18 

sm_lsu_xtagsm_ifu_tagsm_bpu_bpqsm_lsu_xdatsm_sram_arrayssm_ifu_datsm_bpu_btbsm_lsu_varray    sm_sram_dbanksm_ifu_varray   sm_bpu_pagesm_lsu_stb    sm_sram_tbanksm_ifu_imqsm_bpu_ghtsm_lsu_mrq    sm_sramsm_tlb_instsm_bpu_rssm_lsu_dmeqsm_lsu_ldbsm_tlb_l2sm_tlb_datasm_idu_d0sm_reg_trm2    sm_d0_decode   sm_trm_read1    sm_reg_rfl    sm_trm_ex1    sm_reg_rename    sm_trm_write2sm_reg_ctrlsm_idu_d1    sm_trm_share2    sm_reg_rdport    sm_time_countsm_xeq2    sm_reg_wrport    sm_d1_fifo    sm_xeq_alu2    sm_reg_fwd    sm_reg_rsb    sm_xeq_alu    sm_reg_prfsm_idu_d2    sm_xeq_lssm_rob    sm_d2_decode    sm_xeq_unkn    sm_idu_pcsm_xfunit2sm_macro_fifo    sm_ex_slice2    sm_xalu64sm_macro_reg    sm_xmac64sm_macro_reg1    sm_xdiv64sm_macro_rfl    sm_xbeusm_macro_sramSM_IFUSM_BPUSM_CORE_FRONTENDSM_IDUSM_PIPE_CTRLSM_LSUSM_CORESM_CORE_TOPSM_CORE_ISSUECuzco Microarchitecture Specification, v0.2 

 

 

Features of basic block BPU: 

•  On BTB miss, the BPU stops prediction and waits for miss prediction from branch execution unit 

(BEU) 

•  All basic blocks are tracked in the branch prediction queue (BPQ) 
•  All instructions in a basic block have the same pointer to the BPQ entry.  The BPQ pointer in the 

• 

• 

• 

• 

BEU is used to read the prediction information from BPQ. 
Predicted loop is non-taken as the BTB continues to predict the next basic block from the next 
address 
The BPQ loop entry keeps track of the loop count and provides “taken prediction” to the BEU for 
loop iterations 
Small loops that fit in the ICQ are handled by the ICQ where the start pointer and end pointer are 
used for sending loop instructions to XIQ 
Large loops are handled by the BPQ where new tag addresses are generated for every iteration 
and sent to the tag array 

OoO branch misprediction: Has several complicating factors: 

• 

• 

• 

• 

The BPQ provides the order of branch instructions to multiple BEUs in case of multiple branch 
mispredictions 
The ROB keeps track of instructions cancelled by branch misprediction. The cancelled branch is 
executed by the BEU but cannot cause branch misprediction 
The ROB pointer is kept with every instruction in the execution pipeline.  The load/store 
instructions should be cancelled as early as possible to avoid fetching to external memory. 
For register renaming, the ROB performs two actions: (1) undo the renaming of a register in the 
register alias table (RAT) and (2) retire any cancelled instructions in the register free list (RFL) and 
the architectural register table (ART).  For example, the architectural register x5 is renamed to 
r41 by a first instruction and then is later also renamed to r73 by a second instruction which is 
later cancelled by branch misprediction.  The RAT must be restored to r41 before the correct 
path stream of instructions can be processed in ID0.  As the cancelled instruction is retired from 
ROB, the ART should have r41 for x5 and not be modified by the cancelled instruction while r73 is 
released back to the RFL 

Non-blocking load/store instructions: upon a DC miss, the load/store instruction is committed and 
retired in-order without waiting for the fetched cache line.  If the DC miss takes hundreds of cycles, then 
the ROB may fill and block further execution of instructions. 

• 

• 

For a load instruction writing to register X, the write back time of X in the RSB is set to the default 
latency time of the DC hit.  On DC miss, the write back time of X is revised to the L2 cache hit 
time.  If load data is not returned at the revised time from L2 cache, then the write back time of X 
is set to “unknown”.  The setting of the write back time for X in the RSB blocks any RAW 
dependent instructions so that they stall in their XEQ.  The ROB can in-order retire the DC miss 
load instruction. 
For store instructions, the store buffer (STB) keeps the store data and writes to the DC in order as 
indicated by the ROB.  STB requests to DC to write data are generally low priority.  The STB entry 
can be valid longer than the store entry in the ROB.  For DC miss, the store data is written to the 
miss request queue (MRQ) until the cache line is received from the data miss queue (DMQ).  The 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               19 

Cuzco Microarchitecture Specification, v0.2 

 

 

STB, MRQ, DMQ, and data eviction queue (DEQ) can forward data to subsequent load 
instructions.  

3.2  Programming Model Overview 

3.2.1  Register Model 

The Cuzco processor implements the RV64I architecture and thus maintains a 64bit program counter (PC) 
register. 

Consistent with the RISC-V architecture definition the Cuzco processor has 32 architected general purpose 
register names, with x0 hardwired to the value 0x0. 

Consistent with the double-precision floating-point extension of the RISC-V architecture definition the 
Cuzco processor has 32 architected 64bit floating-point register names and a FCSR.  

The Cuzco processor implements all standardized RISC-V counters, timers, control and status registers 
(CSRs) for hypervisor managed operation. There are also Cuzco specific CSRs. 

Cuzco implements these registers so as to optimize the performance of RV64I lp64d ABI compliant 
binaries and register usage conventions.   

3.2.2 

Instruction Model 

The Cuzco processor and cache implements the RV64 ISA [REF] with the following extensions: 

• 

Integer multiply and divide 

•  Double-precision floating-point 
•  Atomic memory operations 
Control and Status Registers 
• 
Counters 

• 

•  Hypervisor 
• 

Interrupt facility 
Compressed instruction format 

• 

•  Bit manipulation 
•  Vector instruction 
Custom instruction 

• 

•  Note: RV32 ISA is not implemented. 

3.2.3 

Exception and Interrupt 

The RISC-V ISA can ignore all exceptions from the execution of integer and floating-point instructions. 
Even a divide by zero operation need only set an error status bit without initiating an exception in the 
RISC-V architecture. Similarly floating-point exceptions set sticky bits in the CSR floating point exception 
register. If necessary, software must detect and recover from exceptions. This section describes the 
procedure for taking exception and interrupt. 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               20 

Cuzco Microarchitecture Specification, v0.2 

 

 

The RISC-V ISA allows both precise and imprecise exceptions. A precise exception is where the processor 
pipeline will stop and an exception routine must be executed. An imprecise exception allows the 
exception error bit(s) to be set in a CSR, and then let an exception handler execute for processing at a 
later point of time, thus the processor execution may not stop at the offending instruction in a program at 
the very instant the exception occurs.  

Interrupt handling in a RISC-V processor is somewhat similar to imprecise exception handling. 

In embedded application, interrupt latency is a main concern. The conditions for taking an interrupt are: 

1. 

Interrupt is based on the last valid branch or load/store instruction in the ROB  
2. 

If there is a pending fetch from external memory for the load/store operation, then the interrupt 
is delayed until completion of the DC miss, otherwise the interrupt is taken immediately 
3.  At an instruction boundary, for instructions with multiple micro-ops, all micro-ops should 

complete before taking an interrupt.  Store instructions have 2 micro-ops: (1) address calculation 
and accessing tag array for hit/miss determination, and (2) write store data to data array at retire 
time.  Any custom instructions may also have multiple micro-ops 

The microprocessor can continue to execute the current instruction stream when an interrupt is taken.  
The instructions can be executed in parallel with the interrupt routine.  For example, the divide 
instruction may still be in execution during the interrupt routine, the RSB and write port control still keeps 
track of the result data for the divide instruction and allows writing back to register file.  The valid pre-
interrupt instructions complete execution and write back to the register file regardless of the interrupt 
status. If non load/store instructions are issued, they are allowed to complete and write back to the 
register files. An interrupt stops new instructions from issuing. 

Table 3-1 and Table 3-2 show lists of exceptions for the Front-End and Load-Store of the Cuzco design as 
specified by the RISC-V ISA. 

Assumption for parity implementation: 1 parity bit per byte for simplicity of store operation, avoiding 
read-modify-write. 

Assumption for ECC implementation: 1 ECC bit per 64-bit of data or per tag address.  The store will require 
read-modify-write operation. 

 

Error Code 

Error Description 

00001 

Instruction cache data array 1-bit ECC error 

00010 

Instruction cache data array ECC/parity error 

00011 

Instruction address break point 

00100 

iTLB page fault (if TLB enable) 

00101 

PMP fault (PMP enabled for instruction) 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               21 

Cuzco Microarchitecture Specification, v0.2 

 

 

00110 

PMA fault (PMA enabled for instruction) 

00111 

Bus error 

01000 

Illegal instruciton 

01001 

Instruction fence 

01010 

Ebreak - to debugger 

01011 

Ecall - to OS 

01100 

WFI - to idle (wait)  

01101 

Trap to supervisor 

01110 

Trap to hypervisor 

01111 

Trap hypervisor to supervisor 
Table 3-1. Front-End Exception Listing 

The same error subroutine address can be used by the ROB to deal with all errors, the error code can be 
written into an error status CSR for the subroutine to exact error type. When the instruction with error is 
committed by the ROB, the error code is written into the CSR and error subroutine address is sent to the 
BPQ. 

All instructions in the core are completed and retired in-order by the ROB, the exception is the load and 
store instructions.  The load can have pending result data from external memory to write to VRF at later 
time.  All the queues should be flushed or empty at the time of exception: 

• 

• 

• 

Flush: IFU queues, IDU queues, BPQ queue (the pending basic block may be flushed as the loss of 
branch prediction accuracy). 
Empty: Arithmetic XEQ 
The ART is copied to the RAT, and the RFL write pointer is copied to the read pointer (necessary 
for load/store instruction exception) 

•  Remain valid; STB, MRQ, DMQ, XEQ-LS; some exceptions must wait for load/store queues to be 

empty. 

In general, the error instructions from IFU or IDU are written into ROB in ID1 stage and converted to NOP 
in ID2, the error instruction should be the last instruction to be issued in ID2.  The error instruction set the 
“stall” bit to block all subsequent instructions.  The stall bit is piggy back on the STB full to stall 
subsequent instructions in ID1.  The stall bit is clear when the exception is taken by ROB. 

Interrupt is an asynchronous event, normally, the interrupt is taken after the last load/store instruction in 
the pipeline. For simplicity, the interrupt can assert itself on the last valid instruction in ID1 stage, 
interrupt is written into ROB with instructions in ID1 stage.  

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               22 

Cuzco Microarchitecture Specification, v0.2 

 

 

3.2.4 

Front-End Exception 

In general, if an exception is detected in the ifu_dat, then the stall signal is set and forced xiq_full to be 
asserted.  The BPU and IFU are eventually stop fetching instructions when the queues are full.  No further 
instruction is sent to the XIQ.  The XIQ empty is basically post-serialization.  Exceptions are rare events 
which are okay for the IFU and BPU to continue until the queues are full.  In general, the BPU predicts the 
branches for performance, thus the BPU should not cause any stall in the pipeline due to error, instead 
the BPU will force BTB miss. The IFU includes the possible exceptions: 

• 

Instruction tag array ECC/parity error, with 4-way associative, 4 tag addresses are fetched: 

o 

o 

If there is a tag hit, then the errors in other tag addresses are ignored. 

If there is no tag hit and there is a tag error (including 1-bit ECC error), then it is tag miss 
and the request is sent to IMQ.  It is simpler to invalidate the valid bit in the valid array 
for tag error including 1-bit ECC error.  No ECC 1-bit error correction is implemented for 
instruction cache. 

• 

Instruction data array ECC/parity error 

o  The ECC/parity error is attached to the affected bytes.  The fetched instructions may not 

include the error bytes in which case, no action is required.  The error instructions may 
be flushed by branch misprediction or other exceptions or other interrupts in the 
backend.  

o  The ECC/parity error eventually is committed by the ROB and sends back to the BPQ. 
o 

It is simpler for the BPQ to invalidate the cache line and re-fetch the cache line again.  It 
is an extra cycle in comparison to the normal re-direction from the ROB.  Note: it is 
simpler to force cache miss and re-fetch of the cache line for ECC 1-bit error. The BPQ is 
piggy back on the CM signals to the instruction valid array. 

o  The BPQ must retain the basic block and reverse to the cache line with and cache line 

address where the fault is at.  The tag fetches the same cache line again.  The data start 
address is the error PC.  The BTB is idle and the BPQ is converted to BTB miss to force 
the update from the BEU. 

• 

Instruction break points (IBP): the number of instruction-address break point registers are 
configurable: 

o  The IBP registers are CSR to be programmed by CSR instruction, the IBP registers can be 

enabled individually. 

o  The cache line address is compared to the IBP registers, the exact byte match is written 

into the ICQ. 

o  When the instruction is committed by the ROB, the debugger is enabled.  The debugger 
is responsible to resume execution of the program (by execute an instruction to jump to 
the break point instruction) 

o  Need to review riscv_debug specs to confirm the address is virtual address 

• 

PMA/PMP violation and L2TLB miss/page-fault: 

o  The tag state machine is designed to wait for L1TLB miss until the valid physical page is 

returned.  The BTB is independent to continue with the predictions ahead of the IFU.  
When iTLB sets itlb_valid, then the tag resumes accessing the tag array. 

o  The BTB Branch Page Buffer (BPB) can also miss and looking up on L2TLB, in this case, 
the start address of the basic block is BTB miss and the basic block is updated on the 
first taken branch.  The BPQ entry needs a bit to indicate that the BPB page is valid, else 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               23 

Cuzco Microarchitecture Specification, v0.2 

 

 

the BTB is not updated.  The BPQ will not update the BTB entry if the BPB page is not 
valid. 

o  The PMP or PMA can return error in IF0 stage and the L2TLB can return page fault.  The 
error is attached to the first instruction of the cache line.  When the error instruction is 
committed by the ROB, the error subroutine address is sent to BPQ. 

• 

Instruction bus error: 

o  The bus error can be from different external sources, from L2 cache ECC error, bus ECC 

error (for 512b to 1024b buses), error from outside of L2 cache. 

o  The bus error is attached to the first instruction of the cache line in the ICQ.  The error 

cache line is discarded and not written into IC. 

3.2.5 

Instruction Decode Exception 

The instruction decode exception is not known until ID0 stage.  Handling of the error instruction is 
somewhat different than IFU. The exception instruction is post-serialization, so the IDU causes stalling for 
all subsequent instructions. The stalling can be done in ID0 or ID1 stage.  In ID1 stage, the error instruction 
is piggyback on the STB or ROB full (default option).  In ID0 stage, the error instruction is piggyback on 
ROB restore signal.  In general, the error instruction is written to ROB in ID1 and treated as NOP in ID2 
stage.   

• 

Illegal instruction in decoding: many different types (IAM, C, FDQ, …) of instructions can be 
decoded at the same time, each type has a “legal” valid bit output.  If no legal valid bit is 
asserted, then it is illegal instruction. 

o  The illegal error bit is attached to the current instruction and written to ROB in ID1 and 

converted to NOP for issuing in ID2 stage. 

o  The illegal instruction is eventually committed by the ROB and exception address is sent 

to BPQ. 

• 

• 

• 

• 

• 

Software exception: Ecall, Ebreak, Trap to supervisor/hypervisor.  The error instructions are 
decoded in ID0 and written into ROB in ID1.  No further instruction will be sent to ROB or 
execution pipeline (ID2).   

o  When the ROB commits the Ebreak instructions, the debugger becomes active. 
o  Other traps cause the ROB to send exception subroutine address to the BPQ. 

Software exception: IFence, the instruction is decoded in ID0 and written into ROB in ID1. No 
further instruction will be sent to ROB or execution pipeline. When the Ifence is committed by 
the ROB, then the IFU and IDU are flushed, the PC after Ifence is sent to BPQ to start fetching 
again. 
Software exception: DFence, the instruction is decoded in ID0 and stalls in ID1 stage. No further 
instruction will be sent to ROB or execution pipeline. When XEQ-LS, MRQ, and STB are empty, 
then the DFence is written to ROB as NOP. 
Software exception: SFFence-VMA (privilege riscv specs), -- need to add. 
Software exception: WFI, the instruction is decoded in ID0 and written into ROB in ID1.  When 
the WFI is committed by the ROB, then the IFU and IDU are flushed.  The core is idle until 
interrupt is asserted.  The clock gating is naturally shutting down the idle modules.  LSU may still 
be active until all the load/store instructions are completed.  The idle signals from all the units 
are combined to shut down the core clock (this process is normal regardless of WFI instruction).  
The core is waken up by valid interrupt signal. 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               24 

Cuzco Microarchitecture Specification, v0.2 

 

 

• 

Interrupt: a fake instruction is set with interrupt and write to ROB in ID1.  The interrupt 
instruction is similar to the error instruction.  When the interrupt instruction is committed by 
ROB (the PC is not incremented), then the interrupt unit will send the interrupt address to the 
BPQ. Note: LSU may still have pending load/store waiting for external fetches.  The external-bus 
error causes by the load/store instruction is imprecise exception will set the CSR error 
address/status. 

o 

Fake instruction is overwriting of the first instruction in ID1 stage as interrupt 
instruction. 

o  Need to revisit to take interrupt after the last load/store instruction rather than ID1 

stage, even the load can be committed by the ROB ahead of the load miss data but the 
add with data dependency can stall retirement of instructions before the “fake” 
interrupt instruction. 

3.2.6 

Load/Store Exception 

The load and store instructions can cause both precise and imprecise exceptions.  If the instruction must 
access the external data outside of the core, then the exception is imprecise.  The precise exceptions are: 

•  DC tag array ECC/parity error, with 8-way associative, 8 tag addresses are fetched: 

o 

o 

If there is a tag hit, then the errors in other tag addresses are ignored. 

If there is no tag hit and there is 1-bit ECC error, then the load/store is replayed from 
the XEQ in next cycle.  Since the tag read is in DC2 stage, the replayed load/store takes 3 
cycles at the earliest to come back to the tag array.  If the corrected tag is not written 
back to the tag array, then the corrected tag is used for address comparison.  

o  The corrected tag is written to tag array when all tag arrays are idle.  This is new logic.  
All subsequent tag compares should include matching index with the corrected tag.  A 
second 1-bit ECC error can overwrite the corrected tag. 

o 

For 2-bit ECC or parity error, the load/store instruction is written to ROB with error code 
for DC address error.  ROB will take exception when the entry is committed. 

•  DC data array ECC 1-bit error, the error can be for both load and store: 

o  The LSU asserts delay to the RSB and XEQ which is similar to data bank conflict, the 

correct data is available in next clock cycle.  If a write port is available, then the data is 
written to PRF, else it is written to XEQ and replayed at later time.  In the later case, the 
RSB is updated accordingly.  

o  The corrected data are piggy back on the STB by taking an entry in STB to write to data 

array.  The extra entry is added by using the STB retire pointer.  If the STB is full (the 
retire pointer previous entry is valid), then the corrected data are lost and not write 
back to DC. 

o 

If the store data is less than 64-bit, then the store data is read-modify-write.  The ECC 1-
bit error is corrected and merged with the store data before writing back to the data 
back to data array.   

•  DC data array ECC 2-bit or parity error, the error can be for both load and store: 

o  The load/store instruction is written to ROB with error code for DC data ECC/parity 

error.  ROB will take exception when the entry is committed. 

• 

L2TLB miss/page-fault: 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               25 

Cuzco Microarchitecture Specification, v0.2 

 

 

o  The dTLB miss causes the load/store instruction to be replayed with the latency of 

reading L2TLB.  If the dTLB miss happens again, then the load/store instruction is set to 
unknown and wait for the ROB to wake up the entry.  The dTLB includes a miss address 
register to monitor if the dTLB miss is updated.  The unknown entry (xeq_unkn.sv) 
checks the dTLB miss address register before replaying of the load/store instruction.   

o  The status of the dTLB miss address register can be L2TLB fault in which case, the XEQ 

entry writes the error code to the ROB. 

• 

PMP or PMA violation can return error in DC1 stage.  The PMP/PMA error code is written to 
ROB. 

•  Data watch point (address and data): 

o  Data address match is in DC2 stage, the exception code is written to ROB (complete the 

load/store instruction to take exception?) 

o  The load result data can be precise or imprecise.  The load result data is compared to 

the data watch point (data) in DC5 stage as the instruction, 1 cycle after the load data is 
written back to the PRF.  The exception code is written to ROB in DC5 stage.  The data 
watch point is implemented in reg_fwd.sv to compare all load data.  The ROB delays 
retiring of the load entry by 1 clock cycle.  

o  Treat imprecise data watch point match using interrupt protocol?  By asserting a fake 

instruction in the ID1 stage?  Need to review riscv_debug specs 

o  Need to review riscv_debug specs to confirm the address is virtual address 

•  Data bus error is an imprecise exception to allow non-blocking loads.  The data bus error and 
address are written to the error status and error address CSR without taken exception.  If AX65 
changes the protocol for imprecise exception, then the imprecise exception can be asserted with 
fake instruction similar to interrupt. 

•  Atomic instruction is sent to XEQ as normal, if hit in DC, then the entry is invalidated or evicted 
to DEQ and invalidated.  The XEQ entry status is cache miss which will eventually get the result 
data from L2 cache as with the normal load instruction. 

o  New module: Atomic Buffer (ATB) is implemented similar to STB, the atomic instruction 

allocates an entry in ID1 stage and reading any source operand data as needed.  The 
destination register is set to unknown in the RSB.  The entry in ATB is executed when the 
ROB commits the ATB instruction at which time, the ATB address is sent to MRQ as miss 
request to L2 cache.  The L2 cache is responsible to execute the atomic instruction and 
return the cache line to DMQ where the cache miss protocol is used to write the atomic 
data to PRF. 

o  The atomic entry in ROB can be committed while waiting for data from L2 cache. 

o  Only 1 atomic instruction at a time 

o 

o 

Subsequent load/store instructions – hit in DC is okay to execute out-of-order with 
respect to the atomic instruction. 

Subsequent load/store instruction – miss in DC must be stalled in MRQ until the earlier 
atomic instruction is done. If it is later than the ATB instruction, all load/store 
instructions have ATB pointer which can be used to keep the ordering.  Another option: 
only 1 ATB instruction is allowed in the LSU at a time, and all subsequent load/store 
instructions have the ATB bit set, they cannot be sent external until the ATB is flashed 
clear. 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               26 

Cuzco Microarchitecture Specification, v0.2 

 

 

 

Error Code 

Error Description 

00001 

Data cache address array ECC/parity error 

10010 

Data cache data array ECC/parity error 

10011 

Data address/data watch point 

10100 

dTLB page fault (if TLB enable) 

10101 

PMP fault (PMP enabled for data) 

10110 

PMA fault (PMA enabled for data) 

10111 

Imprecise data exception 

11000 

Reserved 

11001 

Reserved 

11010 

Reserved 

11011 

Reserved 

11100 

Reserved 

11101 

Reserved 

11110 

Reserved 

11111 

Reserved 

Table 3-2. Load-Store Exception Listing 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               27 

Cuzco Microarchitecture Specification, v0.2 

 

 

4  Branch Prediction Unit (BPU) 

A microprocessor can function correctly without branch prediction but a BPU is necessary to achieve high 
performance.  A functional bug in the BPU can impact only the performance but not ISA functional 
correctness, therefore, BPU performance verification is important. Basic block branch prediction is fairly 
unfamiliar to most designers, but it was first implemented more than 20 years ago.  In addition, as the 
instruction address increases from 32-bit to 64-bit the size of the BTB is at least doubled.  The upper bits 
of the address can be mapped to an index of a buffer thus reducing the size of the BTB.  This chapter 
includes description of many new concepts in BPU design. Figure 3-1 shows the BPU in relation to the IFU. 

 

 Figure 4-1 IFU and BPU Pipeline 

• 

• 

• 

IF0: the PC address from branch misprediction and redirection (exception, interrupts) from the 
execution pipeline are flopped in BPQ before accessing the IC tag array, BTB, branch page buffer 
(BPB), and GHT arrays.  The iTLB is accessed in this cycle to avoid address aliasing.  The BPB 
contains only virtual page numbers. 
IF1: the prediction data are returned from the BTB SRAM array. The BTB prediction is 2 clock 
cycles. Since branch prediction is per basic block, the next address (i.e., the exit point of the basic 
block) is not known until after the prediction, so it is equivalent to a 2-cycle blocking BTB.  The 
BTB hit/miss is known in this cycle while the prediction and target address are calculated in the 
next cycle.  A BTB-miss puts the BTB into an idle state, waiting for misprediction resolution from 
a branch execution unit (BEU). All BTB-hit/miss, target address, start address, next address, 
branch type, and loop prediction are flopped for the next cycle. The GHT is also accessed in this 
cycle to generate taken/non-taken prediction; data from the GHT can be flopped before 
determining taken/non-taken in next cycle for timing. 
IF2: A BTB-hit provides a target address prediction. The BTB provided partial target and next 
addresses are translated into full addresses by reverse lookup of the page buffer. The BTB 
includes the “same page” bit to skip accessing the page buffer. The BTB target prediction is based 
on branch type (1) return, target address is taken from the RS (2) taken, target address is taken 
from the BTB and if indicated, the page buffer, (3) non-taken, then the next address is taken from 
the BTB and page buffer.  The predicted target address is written to the BPQ and routed to the IC 
tag array for next instruction address. 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               28 

IF0IF1IF2IF3ID0Inst PCItagIdatAlignDecodeDecodeInst Miss QueuePrefetch BufferIC Tag ArrayIC Data ArrayInstruction AlignIA generationBTBPredictionBPQGHTRSiTLB4GHRPageBufferICQITQXIQCuzco Microarchitecture Specification, v0.2 

 

 

• 

IF3: The BPQ controls the next address to the IC tag array and ICQ. 

4.1  Interfaces 

Front end block consisting of branch prediction unit and instruction fetch unit has following interfaces to 
the rest of the core and the memory system: 

•  Memory system (L2 cache) 
•  Core top 
• 
•  Mid-core (branch execution and ROB) 
•  Load Store Unit (including main TLB) 

Instruction Decode Unit 

Comments 

Instruction fetch/prefetch requests are made, Snoop 
responses without data 
Data channel for victim (L2 BTB, FE state, etc.) 
Physical address requests on byte boundary (taking 
care of 16 bit instruction and misalignment) 
User bits to indicate instruction or BTB data 
Instruction cache line returns, snoops (to invalidate). 
Demand request sector transfer first. 
L2BTB provides data as requested 
User bits to differentiate instruction or BTB 
 

4.1.1  Memory system: 
Signal 

Direct
ion 

Width/Phy
sical 
Attributes 
7 Channels 

FE_L2_ACE 

Out 

L2_FE_ACE 

In 

7 Channels 

 

 

4.1.2 
Signal 

 

 

Core Top 

Direction  Width/Physical 

Comments 

Vdd, Vss 
Power_gate 
Reset 

Reset_done 
Clk 

Clk_gate 

Input 
Input 
Input 

Output 
Input 

Input 

Attributes 
 
 
Async 
assert/sync 
de-assert 
(default) 
One-bit status 
High speed 
clock 
One-bit 

Power signals 
(Optional) Gate supply voltage 
3 types: 
•  System reset, 
•  SoC reset, 
•  Core reset, 
Status on pin and in register 
Unit can use high speed clock for scan/test mode 
as well 
(Optional) Full core in clock gated mode, 
conserve power, maintains state, but not shut 
down 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               29 

Cuzco Microarchitecture Specification, v0.2 

 

Intialize_FE 
Initialize_FE_done 

Input 
Output 

One-bit 
One-bit 

Initialize after a reset 
Initialization complete 

 

 

 

4.1.3 
Signal 

Instruction Decode Unit 

Direction 

InstructionID 

Output 

Width/Physical 
Attributes 
 

Instruction bus 
Instructions valid 

Output 
Output 

 
 

DecodeQueue Credits 

Input 

 

 

 

One per 
instruction 
 

Comments 

Identifies instruction to track progress 
through the pipeline (possibly RoBID 
allowing roll over) 
 
(Easiest )Only needed for last 
instruction marker 
(Better) One bit for each valid 
instruction 
Accounts for packed instructions, since 
the number of decoders do not change  
 

4.1.4  Mid-core (Branch Execution, Retire including RoB) 
Signal 

Direction 

 
 
 
 
 
 

Width/Physical 
Attributes 
 
 
 
 
 
 

Comments 

 
 
 
 
 
 

 
 
 
 
 
 

 

4.1.5 
Signal 

Load-Store Unit 

Direction  Width/Physical 

Comments 

PC 
LS_FE_Credit 

FE_LS_RegData 

iTLB_miss_request 
iTLB_miss_request_id 

Out 
In 

Out 

Out 
Out 

Attributes 
 
 

 

 
 

Use for prefetch 
Only a certain number of PCs can be tracked 
for prefetch 
Register data associated with PC to create 
prefetch 
iTLB miss with virtual address 
Allows multiple TLB misses for instruction 
demand and prefetch requests 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               30 

Cuzco Microarchitecture Specification, v0.2 

 

iTLB_miss_response 
iTLB_miss_response_id 
 

In 
In 
 

 
 
 

Virtual-to-physical translated address 
iTLB_miss_request identifier returned 
 

 

 

 

4.2  Branch Target Buffer Page Buffer (BPB) 

The BPB is a subset of the virtual address tag array of the L2 TLB.  The BPB is designed to be the same 
way-associativie as the L2 TLB.  An entry in the BPB must be in the same way as the entry in the L2 TLB.  
The L2 TLB is the second level of page buffer for the BPB. 

 

A BTB can be very expensive, as the virtual address size increases from 32-bits to 64-bits, size of the BTB 
can be more than double. The BPB is used to reduce the BTB array size. The upper bits of the instruction 
address do not change often.  Considering 4KB pages, the page number address bits [63:12] are different 
only for typically infrequent long jumps.  For example, a page buffer of 128 4KB pages represents 512KB 
of instruction memory which is sufficient for most application program instruction working sets.  The BPB 
is a subset of the virtual address tag array of the L2 TLB.  The BPB is designed to be the same way-
associative as the L2 TLB.  An entry in the BPB must be in the same way as the entry in the L2 TLB.  The L2 
TLB is the second level of page buffer for the BPB. 

The BPB is organized as 4-way of 32 entries. (Again, the BPB should have the same way associativity as the 
L2 TLB): 

• 

• 

• 

Page size is 4KB where the Address[16:12] + Bway[1:0] are used to look up Address[63:17] in the 
BPB 
The number of entries in BPB is 128 in comparison to the 8K entries BTB for storing of the upper 
address bits.  The BPB can be implemented as register file instead of SRAM for faster access and 
multiple accesses. 
For BTB target and next addresses: 

o  Address[16:12] + Bway[1:0] are 7 bits in BTB to replace 52-bit address in traditional BTB 
o  Address[16:1] + Bway[1:0] are 18 bits in BTB to replace 63-bit address in traditional BTB, 

o 

a 71% reduction in BTB storage 
Scalability: the BTB storage for target and next addresses remain the same regardless of 
instruction address width 

• 

The BTB is a cache array where the tag array consists of: 

o  Bway[1:0] are 2-bits in BTB to replace the 52-bits address in traditional BTB, a 96% 

reduction in BTB storage; note that it is scalable regardless of instruction address width 

o  The page buffer comparison of the 47-bits following with bway[1:0] comparison to 

replace the 52-bit tag comparison in traditional BTB 

o  Timing advantage: reading from a 128-entry register file for tag address comparison is 

much faster than reading from a 8K-entry SRAM BTB.  Furthermore, the BTB size is much 
reduced in comparison to the traditional BTB implementation 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               31 

Cuzco Microarchitecture Specification, v0.2 

 

 

Figure 4-2 BTB Prediction with Page Buffer Block Diagram 

The BTB branch prediction is 2-cycle blocking, but the BPB is looked up in every cycle: 

IF0: the PC address (start address) is sent to read the BTB SRAM array, BPB, and taken GHT SRAM arrays. 

IF1:  

•  Use start address[16:12] to read 4 ways of one of the 32 page buffer sets and compare those 

entries to address[63:17] for BPB way hit 

•  Use start address[11:1] to read 4 ways of the BTB SRAM arrays, and compare bway[1:0] to the 

BPB way that was hit to determine BTB way hit.   
The BTB way hit is used to select 1 of the 4 ways of the BTB data.  The selected target and next 
address[16:12] and bway[1:0] are used to access the BPB in the next cycle.  Note that, the “same 
page” bit in the BTB array is used to avoid the BPB lookup. 
The output of the taken GHT SRAM array predicts taken/non-taken 

• 

• 

IF2:  

• 

• 

If the “target same page” bit is set, then the start address[63:12] is used for the target address 
instead of the target address from the BPB. 
If the “next same page” bit is set, then the start address[63:10] is used for the next address 
instead of the next address from the BPB. 

The branch type and GHT prediction bit are used to select the next PC (start) address from the RS, the 
target address, or the next address.  This stage is equivalent to the IF0 stage. 

•  Notes: 

o  BPB miss = BTB miss, no action for branch prediction in this cycle. The start address is 

saved in the BPQ to write a new entry into BTB and BPB arrays. 

o  The same address may miss in the iTLB which would cause a L2 TLB lookup wherein the 

L2 TLB hit way is the replacement way for the BPB. Otherwise the BPB miss address is 
looked up in the L2 TLB for its replacement way. 

o  The L2 TLB way could be kept in the iTLB for the BPB miss and iTLB hit cases.  

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               32 

Redirect Address2047Index[11:1]2046Start Address...14x36 bits0bway[1:0]bway[1:0]bway[1:0]bway[1:0]Tar/Nxt[16:1]Tar/Nxt[16:1]Tar/Nxt[16:1]Tar/Nxt[16:1]Target AddressAddress[63:17]31Target/Next[16:12], Bway[1:0]Address[16:12]30.Target/Next[63:17]Target/Next[16:1]Next Start Address0Addr[63:17]Addr[63:17]Addr[63:17]Addr[63:17]BTB Array, 8K-entry, 4-wayBTB Tag Way 0BTB Target Way 3BTB Target Way 0BTB Target Way 1BTB Target Way 2Tag[63:12]Page Buffer Way 0Page Buffer Way 1Page Buffer Way 2Page Buffer Way 3BTB Tag Way 1BTB Tag Way 2BTB Tag Way 363:1=?63:1mux=?=?=?Hit=?=?=?=?HitCuzco Microarchitecture Specification, v0.2 

 

 

Since the BPU 4KB page size is the same as the TLB page size, the Bway[1:0] which is the BPB way and is 
the L2 TLB way.  A BTB miss can be from 2 sources:  a BPB miss or the BTB Bway[1:0] does not match with 
the hit way from the BPB.  In either case, the BTB miss causes the prediction to stall in the BPQ until a 
taken branch is executed in the branch execution unit.  The BTB needs the Bway[1:0] for the entry-point 
address, the exit-point (next) address, and the target address.  

• 

• 

• 

The same entry-point address must eventually access the iTLB for translation.  For an iTLB hit, the 
Bway is read from the iTLB.  For an iTLB miss, the iTLB must access the L2 TLB to replace an entry 
in the iTLB which includes the physical address and the L2 TLB way.  In either case, the L2 TLB 
way from the iTLB is to be kept with the entry-point address prediction in the BPQ (not yet 
implemented, need to add). 
Similarly, starting with the entry-point address, the subsequent cache lines must also be 
translated by the iTLB.  The BPQ entry keeps the L2 TLB way for the next TLB page (not yet 
implemented, need to add).  If the exit-point address is in a different page than the entry-point 
address, then the L2 TLB way of the next TLB page is used, otherwise the “same page” bit will 
ignore the L2 TLB way.  
The target address is from a taken branch instruction.  In IF0, the iTLB is accessed for translation 
if the target address is in a different page than the entry-point address.  The L2TLB way from the 
iTLB will be used for the target address. 

The BPB replacement way is taken from the iTLB hit when replacing a BPB entry.  If no MMU is configured, 
then the BPB replacement way is randomly selected. 

For implementation, the BPB includes valid bits which are reset during initialization.  The BPB miss results 
in a BTB miss and no prediction.  The BPU stops making any further prediction and waits for branch 
misprediction determination from a taken branch in a BEU pipeline.  Upon misprediction, the new PC 
(start) address can access the BTB, BPB, and GHT arrays as above.  The GHT is based on bimodal branch 
prediction, the MSB indicates direction of the branch (taken or non-taken) and the LSB indicates strength 
of the prediction (weakly or strongly).. The GHT is organized as two independent arrays for the MSB and 
the LSB.  The extra functions for the BPU pipeline are: 

IF0: the miss start address is sent to BPB and to read the strength GHT SRAM arrays.  

IF1:  

• 

• 

• 

The BPB selects a way (from L2 TLB) to write the missed basic block start address, target address, 
and next address 
The output of the strength GHT SRAM array is used to determine what to update in the GHT 
The miss start address is sent to the BTB and the BPB 

IF2:  Note: in this cycle, the BTB SRAM, GHT SRAM arrays are not accessed for branch prediction 

• 

• 

• 

The mispredicted branch is written to the BTB SRAM array 
The miss start address, target address, and the next address are written to the BPB.  Since the 
BPB is a register file, reading of the BPB for comparison is done in the first half of the cycle and 
writing of the BPB by the second half of the cycle is feasible 
The GHT is updated accordingly 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               33 

Cuzco Microarchitecture Specification, v0.2 

 

 

4.3  Basic Block Branch Prediction 

Definition of a basic block: a straight-line code sequence with no branches in except to the entry and no 
branches out except at the exit. 

Modified definition: from the execution point-of-view, the exit point is when the branch is taken. The 
basic block can contain non-taken branches as long as the non-taken branches remain non-taken.. 

 

Figure 4-3 Illustration of Basic Blocks 

Compared to the traditional fetch-block branch prediction where the BTB is looked up for every cache 
block fetches from the IC, basic-block branch prediction has several advantages since it is more natural 
than the fetch-block branch prediction: 

• 

In Global Share branch prediction, a branch can be reached by different paths in which the paths 
are different basic blocks with different entry points 

•  A loop is a basic block which is much simpler to predict by basic block branch prediction.  A loop 

• 

• 

• 

is detected when the target address matches the start address of the basic block 
For a large basic block of a hundred instructions as shown in thefigure below, the BTB is looked 
up once and is then idled and clock gated off while waiting for the IFU.  In EEMBC, there are basic 
blocks greater than 1KB size.  The active power savings in combination with using the BPB is 
more than 70% 
For small basic blocks as shown in the figure below, all basic blocks are kept in the BTB while the 
traditional fetch-block based branch prediction can keep only 1 or 2 branches and can thrash if 
there are more branches in the same fetch block 
In term of performance verification, every basic block in execution must be in the BPQ and is thus 
simpler to track 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               34 

Entry point:i1i2i3Exit point:Br T1 (taken)Entry point:i4i5Exit point:Br T2 (taken)Entry point:i6i7Br T3 (not-taken)i8i9i10i11Exit point:Br T4 (taken)Entry point:Basic BlockBasic BlockBasic BlockCuzco Microarchitecture Specification, v0.2 

 

 

 

Figure 4-4 Large and Small Basic Blocks 

 

The following examples show how basic blocks are created for BTB entries. The first example shows how 
the non-taken branch in one basic block can be later converted to two basic blocks prediction. The second 
example shows two basic blocks with the same exit point, put differently, two different entry points for 
the same branch instruction. The third example shows nested basic blocks. 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               35 

32103210Entry PointExit PointBasic BlockFetch BlockFetch BlockFetch BlockFetch BlockFetch BlockFetch BlockFetch BlockFetch Block3210321032103210321032103210Entry PointExit PointEntry PointExit PointEntry PointExit PointEntry Pointswitch, case statementsCuzco Microarchitecture Specification, v0.2 

 

 

Figure 4-5 Basic Block Example 1 

 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               36 

Entry point 0:i1i2Br T1 (not-taken)i3i4i5Start AddressEnd AddrTarget AddressExit point 0:Br T2 (taken, mispredict)Entry point 0Exit point 0T2Entry point 0:i1i2Start AddressEnd AddrTarget AddressExit point 0:Br T1 (taken, mispredict)Entry point 0Exit point 0T1i3i4i5Br T2Entry point 0:i1i2Start AddressEnd AddrTarget AddressExit point 0:Br T1 (not-taken, mispredict)Entry point 0Exit point 0T1Entry point 1:i3i4i5Start AddressEnd AddrTarget AddressExit point 1:Br T2 (taken, mispredict)Entry point 1Exit point 1T2No change for Entry point 0, BHT is updatedBTB entry for Entry point 1 is createdFirst time, no entry in BTB for Entry Point 0BTB entry for Entry point 0 is createdSecond time, Br T1 is takenBTB entry for Entry point 0 is modifiedThird time, Br T1 is non-taken and Br T2 is takenBasic Block 0Basic Block 0Basic Block 0Basic Block 1Cuzco Microarchitecture Specification, v0.2 

 

 

Figure 4-6 Basic Block Example 2 

 

 

Figure 4-7 Basic Block Example 3 

The differences between the basic-block branch prediction and the fetch-block base branch prediction are 
highlighted in boldface below: 

• 

Start address: the start address is the entry-point, the address of the first instruction of the basic 
block (not the address of the branch) 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               37 

Entry point 0 is replaced by another branch with same indexi1i2Br T1Entry point 1:i3i4i5Start AddressEnd AddrTarget AddressExit point 1:Br T2 (taken)Entry point 1Exit point 1T2Entry point 0 is BTB miss, no entryEntry point 0:i1i2Br T1Entry point 1:i3i4Start AddressEnd AddrTarget Addressi5Entry point 1Exit point 1T2Exit point 1:Br T2 (taken, mispredict)Entry point 0Exit point 1T2BTB entry for Entry point 0 is createdBTB entry for Entry point 1 is createdBasic Block 0Basic Block 1Basic Block 1Entry point 0:i1Entry point 1:i2i3i4I5Start AddressEnd AddrTarget AddressExit point 0:Br T1 (taken, mispredict)Entry point 0Exit point 0Entry point 1Br T2 (taken, mispredict)Entry Point 1 is the target of another branch, no entry in BTB for Entry Point 1Entry point 0:i1Entry point 1:i2i3i4Start AddressEnd AddrTarget AddressI5Entry point 0Exit point 0Entry Point 1Exit point 0:Br T1 (taken, loop0)Entry point 1Exit point 0Entry Point 1LoopEntry point 2:Br T2 (taken, mispredict)Entry point 2Entry point 2Entry Point 0SingleThe BTB entry can be nested basic blocksEntry point 2 is single instruction basic blockBTB entry for Entry point 1 is createdBTB entry for Entry point 0 is createdFirst time, no entry in BTB for Entry Point 0Basic Block 0Basic Block 0Loop0Cuzco Microarchitecture Specification, v0.2 

 

 

• 

Target address: calculated from BEU 

•  Next (sequential) address: the address of next instruction after the basic block for non-taken 

prediction is included in the BTB 

•  Branch type: conditional, unconditional, call, return, loop (unconditional non-taken branch) 
• 

“Same TLB page” bit is in included in BTB, if start_address[63:12]==target_address[63:12] 
The start address is used for GHT; the full address is known at the beginning of prediction unlike 
fetch-block based prediction where the branch offset is fetched from BTB 

• 

•  Basic block branch prediction matches the Gshared branch prediction concept; same branch 

• 

with 2 different start addresses (see Example 2) 
Performance improvement is at about 15% (from performance model) in comparison to fetch-
block based branch prediction 

4.4  Branch Target Buffer (BTB) 

The BTB array is 8K entries, organized as 4 ways of 2048x52. The numbers of configurations for BTB and 
GHT sizes are limited for verification.  

The valid array is implemented using register cells, 4 sets of 2048 valid bits. The valid array is reset in 8 
cycles, 128 bits per set are reset in each cycle (see 5.1).  

The BTB should provide predictions for all branches in the processor; at least all taken branches in the 
instruction cache. A rough estimate of the required BTB capacity is: 

• 

Instruction cache size: 32K bytes. 

•  Average instruction size: 24-bit, total instructions in IC = 32KB/3B = 10923. 
•  Branches average 20% of total instructions, or 0.2 * 10923 = 2185 branches in IC. 
•  An average of 70% of branches are taken, or 0.7 * 2185 branches in IC = 1529 taken branches in 

the IC.  

8K-entry BTB should be able to predict most branches for a 32KB instruction cache.  The BTB 
independently makes branch predictions until BPQ is full, but it stalls on a BTB miss and waits for branch 
misprediction resolution. On misprediction and execution redirection, the BPQ controls the write 
operation to the BTB which is 1 cycle later. The companion GHT and RS are also updated on misprediction, 
so it is simpler for the BPQ to control the writes to the BTB, GHT and RS.  

Each BTB entry consists of the following data: 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               38 

Cuzco Microarchitecture Specification, v0.2 

 

 

Name 

Bits  Description 

Tag Bway 

2 

Lookup BPB for Bway[1:0] for tag hit comparation with BPB hit way 

Target Addr 

16 

Target_address[16:1]  

Target Bway 

Target same BPB 

Target same TLB 

2 

1 

1 

BPB way combining with target_address[16:12] to generate target_address[63:17] 

Set if start_address[63:17]==target_address[63:17] 

Set if start_address[63:12]==target_address[63:12] 

Next Addr 

16 

Next_address[16:1]  

Next Bway 

Next same BPB 

Target same TLB 

Same cache line[0] 

Same cache line[1] 

Branch type 

2 

1 

1 

1 

1 
3 

BPB way combining with Next_address[16:12] to generate Next_address[63:17] 

Set if start_address[63:17]==next_address[63:17[] 

Set if start_address[63:12]==next_address[63:12] 

Set if start_address[63:5]==next_address[63:5] 

Set if (start_address[63:5]+1)==next_address[63:5] 
000: Conditional branch                               001: not-taken, small   (ICQ) 
010: not-taken, large loop                            011: Reserved 
100: Unconditional return                            101: Unconditional call,  
110: Unconditional jump                              111: Reserved 

Loop count 

5 

Loop count, the number of bits is configurable for application 

Total bits 
Table 4-1. BTB Bit Fields 

52 

Data width of BTB array 

4.4.1 

BTB Prediction and Misprediction 

The branch prediction takes 2 cycles.  The first cycle is to read the data from BTB SRAM array to 
determine BTB hit/miss.  The second cycle is to calculate the target and next addresses and to determine 
if the branch is taken or not-taken.  The 2 cycles branch prediction is necessary for clock frequency and 
also to simplify the procedure for branch misprediction.  The first cycle is to generate the BPB ways for 
start, target, and next addresses.  The second cycle is to write the new predicted data to BTB SRAM array. 

Implementation note:  BTB hit and way muxing of BTB SRAM data are in the first cycle.  The pipeline 
register is 52-bits instead of 4x52-bits.  In the second cycle the BPB is looked up with 1 set of target/next 
addresses.  The alternative is looking up the BPB with 4 sets of target/next addresses for timing. 

If the start address is a BTB hit, then the prediction includes (1) the taken/non-taken prediction and (2) 
target and next addresses.  The taken prediction should be based on the branch type: 

Type 

000 

Prediction 

prediction is based on GHT 

001, 010 

non-taken prediction 

100, 101, 110 

taken prediction 

011, 111 
Table 4-2 Branch Type and Predicted Direction Source 

illegal prediction, treated as BTB-miss 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               39 

Cuzco Microarchitecture Specification, v0.2 

 

 

The next and target addresses are calculated from the BTB and BPB where the next/target 
address[16:12] and bway[1:0] are used to lookup the page buffer address[63:17]: 

• 

Target address = {{target same BPB ? start_address[63:17] : page_buffer_address[63:17]}, 
target_address[14:1]} 

•  Next address = {{next same BPB ? start_address[63:17] : page_buffer_address[63:17]}, 

• 

next_address[14:1]} 
If branch type is return and the return stack is not empty, then the target address is popped from 
return stack (RS) 

In the second cycle, the next start address is cycled back to access the BTB SRAM array.  The branch type 
and taken/not-taken prediction are used to select the next start address. 

The BTB provides additional information: 

• 

• 

• 

• 

Same TLB page: to avoid look up in the iTLB if the next start address is in the same TLB page.  
Implementation note: if timing allows, the same TLB page check can be done in the second cycle 
to save 2 bits in the BTB SRAM array 
Same cache line[0]: to indicate that the end address (end_address[63:5]=next_address[63:5]-1) is 
in the same cache line.  This signal is used by the ifu_itag to cancel the speculative fetch of next 
cache line.  The IC tag array is a single cycle pipeline and thus it could be 1 cycle ahead of BTB 
prediction.  Note that the ifu_idat also uses this signal to select the end index from next_address 
instead of end-of-cache-line index 
Same cache line[1]: to indicate that the end address is in the next cache line.  The ifu_itag uses 
this signal to get the next address from BTB instead of the incremented address in the next cycle 
Loop count: is validated by the branch type which will be described in next section. Note that the 
number of bits is configurable 

For a branch misprediction, the action is according to Table 4-5.  If a new entry is written to the BTB SRAM 
array, then the BPQ sets up all the fields with the exception of the start/target/next addresses.  In the first 
cycle, the addresses access the BPB. BPB way hit is used as bway[1:0] for writing to the BTB while a BPB 
miss uses random or PLRU to select bway[1:0].  In the second cycle, a BPB miss writes the addresses to 
the selected ways of the BPB. 

4.4.2 

BTB Miss Procedure to Set Up a BTB Entry 

On a BTB miss, the BTB cannot make any further predictions because the basic block is not known and the 
IFU will fetch instructions sequentially until mispredicted by a taken branch from branch execution units.  
There are 3 cases of BTB miss: (1) No valid entry in the BTB, (2) BPB miss, and (2) BPB hit but the way 
associative does not match any Bway (L2TLB way) in the BTB.  Bways must be fetched (not yet 
implemented) from L1 or L2 TLB for the start, target, and next addresses in order to write a BTB entry. 

The BTB-miss can be speculatively mispredicted where the second branch is mispredicted before the first 
branch is mispredicted.  The BPQ entry waits until the mispredicted branch is retired before writing into 
the BTB.  Another BPQ pointer is used for writing the BPQ entry to the BTB.  In case that the BPQ is still 
waiting for Bway from L2 TLB, then a BTB-write entry is used to keep BTB write data. 

• 

Start address Bway: upon BTB miss: 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               40 

Cuzco Microarchitecture Specification, v0.2 

 

 

o 

o 

(Current) the BPB selects a way (first cycle) to write the start address in the BPB (second 
cycle).  The selected way can be either randomly selected if the BPB also misses or from 
BTB hit way.  The selected way is the Bway for the start address and will be written into 
the BTB miss entry in the BPQ. 
(Future) the start address is sent to L1 TLB (first cycle) to read the L2TLB way which is 
needed to update the Bway for the target address in the case of a BPB miss.  The L2TLB 
way is used to write the start address into the BPB and is sent to the BPQ.  If the start 
address is not in the L1 TLB, then the start address is sent to the L2 TLB.  When a BPQ 
wrptr entry is sent to L2 TLB, the L2 TLB returns the address and the L2TLB way in which 
to write to BPB and BPQ. 

• 

Target address and next address Bway: upon branch misprediction from the branch execution 
unit: 

o 

o 

(Current) Target address (the start address of the next basic block) is sent to BPB and 
BTB to lookup its current prediction.  Note that, if the target address is in the same page 
with start address, then Bway of start address in BPQ entry is used instead of accessing 
the BPB for Bway. At the same time, if the page address of the next address is different 
than the start address, then it is sent to the BPB to read Bway in the first cycle.  The 
Bway for the start and next addresses are either from the Bway of the start address in 
the BPQ entry or from the BPB look up of Bway in the first cycle. 
(Future) The page addresses of the start and next addresses are compared to the start 
address in the BPQ for Bway, if they do not match, then the page addresses are sent to 
the BPB and L1 TLB to read the L2 TLB way.  If access of the L2 TLB is necessary, then a 
BTB write entry is setup for delayed writing of the BTB-miss BPQ entry.  The BPQ entry 
should have additional bits to ensure that the Bway is updated before the entry can be 
written to the BTB. 

Note: The BPQ entries may have the same start address (such as entries before loop is set in the BTB 
entries), we may try to collapse the entries if writing to BTB is delayed due to accessing the L2 TLB. 

4.4.3 

Loop Prediction 

The basic-block branch prediction concept is well suited for loops. A loop is detected if the target address 
iis the same as the start address. In the BEU, the branch instruction compares: 

• 

• 

The branch (bge) compares rs1 to rs2 and branches to re-execute the loop if rs1>=rs2. In this case  
rs1-rs2 is the loop count which can be used as the loop count and to update the actual loop 
count in the BPQ 
The branch (blt or bltu) compares rs1 to rs2 and branches to re-execute the loop if rs2>rs1. In 
this case rs2-rs1-1 is the loop count which can be used as the loop count and to update the loop 
count value in BPQ 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               41 

Cuzco Microarchitecture Specification, v0.2 

 

 

 

Figure 4-8 Basic Block with Loop Prediction 

The above Figure, is an example of nested loops in which exit point 2 goes back to the beginning of the 
start address of the inner-most loop (basic block 1). When the loops are first executed, basic block 0 is 
mispredicted (first) and written into the BTB. Basic block 0 is not a loop because the target address and 
start address are different.  When basic block 1 is mispredicted (second), it is a loop because target 
address = start address.  The loop count is already decremented by 2, the loop count field of the BTB entry 
is the loop_count+2.  Since the loop prediction is not yet in the BTB, the target address from the second 
misprediction is BTB miss which will cause a third misprediction which is the third iteration of the loop.  
The start address of the third misprediction is a BTB hit for the loop prediction.  However the loop count 
from the BTB is no longer correct.  An optimization is added to the BTB: if the misprediction is a loop, then 
the “mispredicted” loop count is used instead of the loop count from the BTB.  The loop prediction is not-
taken where the exit point 2 is the start address with BTB miss.  The fourth misprediction is when the BEU 
exits the loop and executes the exit point 2 instruction and the second branch instruction.  The target 
address of basic block 2 is to the entry to basic block 1 which is BTB hit with loop prediction and then 
basic block 2 will also be a BTB hit.   

In implementation, the BTB nominally uses 5 bits for the loop count.  The number of bits for loop count is 
configurable.  If the actual loop count is greater than 5 bits, then the predicted loop count is set to 
maximum.  Basically, every 32 iterations, the loop is mispredicted.  Increasing to 7 bits cause misprediction 
every 128 iterations. The loop prediction can also be from an unconditional branch instead of conditional 
branch where the loop count is incremented or decremented.  The loop exits by branching out in the 
middle of the loop in which case the mispredicted branch is taken but the mispredicted branch does not 
change the entry in the BTB.  The loop basic block with the “mispredicted middle” branch does not change 
the loop prediction.  The loop count with unconditional branches are sets to all 1s, maximum count, and 
exit the loop only on misprediction with middle branch instruction. Note: if the loop count is all 1s, then 
there is no need for the BTB nor the IFU to fetch anymore instruction.  The valid loop prediction with 
maximum loop count will put the BTB and the IFU tag array into idle mode. All instructions are from ICQ 
until misprediction. 

The loop may contain more than 1 back block. The BEU compares the target address with the start 
addresses of the current basic block and the previous basic block.  As long as the loop size of both basic 
blocks fits into the ICQ, then the loop is predicted as shown in below figure. The loop count is kept with 
the second basic block since the target address is matching with the entry point 0 which is one of the 
conditions for loop detection. 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               42 

Entry point 0:i1i2i3Entry point 1:i4I5I6I7Exit point 0/1:Br x3, x0, Entry_point1Entry point 2:i8 - loop count decrementExit point 2:Br x4, x9, Entry_point1First time, no entry in BTBBasic Block 1-LoopBasic Block 0Basic Block 2Cuzco Microarchitecture Specification, v0.2 

 

 

 

Figure 4-9 Combining Multiple Basic Blocks into Loop Prediction 

In this case, the whole loop can be issued and executed in a single clock cycle.  If the basic blocks are not 
predicted as loop in the ICQ loop buffer, then each basic block takes 2 cycles going through the BTB or a 
minimum of 4 cycles for 2 basic blocks, an improvement of 3:1 with loop buffer.  Future consideration: the 
large loop can also be predicted and cycle through the BPQ instead of going through the BTB.  The BTB 
keeps track of the basic block length of the first basic block and added to the second basic block length for 
the loop length.  The loop prediction is kept with the second basic block, since the target of the second 
basic block matches with the start address of the first basic block.  When the second basic block is fetched 
by the BTB, the BTB will send the valid loop prediction to the ICQ with the next basic block where the total 
loop length is calculated.  The loop prediction includes loops with loop count where the loop iteration can 
be counting up or down, unconditional, or any other comparison for exiting the loop.  The unconditional 
loop has another branch inside the loop which is used for exiting the loop.  When the loop count is not 
known, then the loop is set to maximum. 

Implementation note:  if the loop count is maximum, then the option is not to decrement the loop count 
in IFU and BPQ with couple more option: 

•  Do nothing, misprediction happens when the loop is exited in branch execution units 
• 

The loop execution may trigger the predicted loop count to count down when the count is 
MAX+2.  This is hard to be correct as it is depended on the number of instructions in the loop and 
number of branch execution unit 

There are many options for handling of loops in the front end.  Issuing of loop iterations can be from XIQ 
and/or ICQ depending on the number of instructions in the loop.  In the initial implementation, issuing of 
the loop iterations is only from ICQ if the number of instructions in the loop basic block is fewer than or 
equal to the ICQ size.  Thus, there are two types of loops: (1) small – fit into ICQ, and (2) large – more 
instructions in loop than ICQ size.  Increasinig the ICQ size allows the larger loop to be handled by the ICQ.  
The BPU can thus treat small loop prediction as a single not-taken branch basic block.  Note that only 1 
small loop at a time is allowed in the ICQ: 

• 

• 

• 

The predicted small loop is sent to ICQ.  The BPQ processes next basic block after the loop 
 The ICQ is responsible to count down the iterations and send loop instructions to XIQ   
The ICQ accepts instructions of the subsequent basic block and thus continues to send 
instructions after the loop count is zero  

For large loops, the BPQ is responsible for tracking the loop count and sends address requests to IC tag 
array.  Note that the ICQ size is at least equal or greater than the IC cache line size.  A large loop must be 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               43 

Entry point 0:i1Entry point 0:Br x3, x0, Entry_point1i3i4Entry point 1:I5I6I7i8Entry point 2:Br x4, x9, Entry_point0Basic Block 0Basic Block 1Loop: basic block 0 & 1i3 and i4 are not in the loopCuzco Microarchitecture Specification, v0.2 

 

 

at least 2 cache lines.  The BPQ requests cache lines from start address until end address and repeats for 
each loop iteration.  The IFU is not aware of the large loop prediction.  For a large loop with a loop count 
greater than the MAX, the loop execution causes misprediction to exit the loop.   

4.4.4  Global History Table (GHT) and Global History Register (GHR) 

The GHT array is 16K entries, organized as 2 banks of 256x64; 1 bank for each bimodal prediction bit. The 
numbers of configurations for BTB and GHT sizes are limited to allow for verification. GHT[1] is the 
direction (taken) array, and GHT[0] is the strength array. The array is indexed by GHR[9:6] and 
start_address[10:7] and GHR[5:0] ^ start_address[6:1] to select prediction bit 

•  GHT[1] is read on every BTB read access to provide prediction for conditional branch 

GHT[0] is written “1” on every correct conditional prediction as indicated by the BEU 

On misprediction, the GHT banks are updated for only conditional branches: 

•  Misprediction with BTB-miss, write “10” to the GHT 
•  Misprediction with taken prediction, target address miss-matched, write “1” to GHT[0] 
•  On direction misprediction, branch direction prediction is opposite that of the branch execution: 

o 
o 

First cycle, GHT[0] is read 
If the strength bit is “1”, then write “0” to GHT[0], else write “1” on taken misprediction 
and “0” on non-taken misprediction to GHT[1] 

Figure 4-10 GHT Array 

 

The GHR is 10 bits, so the index to the GHT uses the combination of GHR[9:6] and address[10:7].  Only 
conditional branches should enter the GHR; unconditional taken branch does not change the global 
history of the branches.  The basic block branch prediction uses the start address, so all the address bits 
are known at the beginning, so any configuration of the GHT SRAM array should work.  This is different 
than the fetch-block based branch prediction where the BTB entry is based on the branch address in 
which the LSB of the address is not known until the BTB entry is fetched. 

On a misprediction, the GHR must be updated immediately for the mispredicted start address to access 
the GHT in next cycle. 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               44 

Addr[10:7],GHR[9:6] GHR[5:0]Addr[6:1]taken/non-takenstrengthGHT[0] SRAMGHT[1] SRAM64:1646464:1Cuzco Microarchitecture Specification, v0.2 

 

 

4.4.5  Return Stack (RS) 

The most accurate target prediction for a return instruction is the return stack (RS). As a call instruction is 
predicted from the BTB, the next address is pushed into the top-of-stack (TOS) of the RS. As a return 
instruction is predicted from the BTB, the TOS entry is popped and used as the target address.  The RS is 
last-in-first-out, the next address of the last predicted call is the target address of the current predicted 
return. 

The likely depth of nested call instructions determines the number of entries in the RS.  If the number of 
call instructions prior to a return is greater than the size of the RS, then the RS could be empty for a later 
predicted return instruction.  If the RS is empty for a BTB predicted return, then it is treated as BTB miss 
where the BTB becomes idle and waits for misprediction from BEU.  Few benchmarks show significant 
performance drop with respect to the number of RS entries. A 12-entry return-stack minimizes the 
number of RS empty mispredictions.  A recursive subroutine can take many entries in the RS where the 
same next address of the call is pushed into the RS more than the number of entries in the RS.  A counter 
can be used to detect the recursive calls so that a single entry is used.  

Name 

Bits   Description 

Valid 

1 

Entry valid 

Recursive count 

8 

Recursive count to keep track of  recursive subroutine 

Return address 

63 

Next address of  the predicted Call which is return address of  next 
predicted Return 

Table 4-3. Return Stack Bit Fields 

The recursive call is detected when call next_address==tos_address, nothing is pushed into the RS, but the 
recursive count is incremented. For predicted return, if the recursive count=0, then the entry is popped, 
else the recursive count is decremented.  The EEMBC Text01 benchmark has 3 recursive subroutines and 
6-bit recursive counter is sufficient. The 8-bit counter is 256 maximum count value which should be 
sufficient for most cases. 

Figure 4-11 Example of Recursive Calls 

 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               45 

Starti1Call  StartStarti1i3Call  StartStarti1Returni3Call  StartStarti1Returni3Call  StartReturni3ReturnCuzco Microarchitecture Specification, v0.2 

 

 

The BPQ keeps the current states of the RS in order to restore the RS on a misprediction.  The TOS entry is 
kept with each predicted branch in the BPQ which includes the TOS number, recursive count, valid bit, and 
the return address.  

The cases where a return instruction can be mispredicted are: 

1. 

2. 

Jump to subroutine is used instead of Call, the program jumps to the subroutine without a Call 
instruction where no corresponding next address for the return instruction; yielding more 
Returns than Calls. 
Jump to the calling basic block is used instead of a Return. That is the program jumps back to the 
main program without using a Return instruction; yielding more Calls than Returns. 

3.  The link register is modified by software to return to a different location than the next address of 

the original Call instruction; Call-Return pair exists in the return-stack. The target will be 
mispredicted for one Return only. 

In most cases, once there is a mispredicted RETURN, all future return instructions will be mispredicted; 
(optional) flushing of return stack on return misprediction and stalling on subsequent return instructions 
can save power. In the performance simulation, Dhrystone and EEMBC benchmarks do not have any of the 
above cases. 

Restoring the RS includes mispredictions on Call and Return instructions (note that Call and Return are 
unconditional, and misprediction can only be mismatched target address): 

•  Mispredicted call does not need to change the current TOS 
•  Mispredicted return removes the TOS of the RS 

On a misprediction, the RS should be updated immediately because the mispredicted start address could 
be a call or return basic block. 

The below Figure is the example of 4 calls followed by 4 returns.  The Figure illustrates the states of the 
return stack from the reset state where the TOS and the wrptr are the same and no call or return basic 
blocks are in the BTB. 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               46 

Cuzco Microarchitecture Specification, v0.2 

 

 

 

Figure 4-12 Example 1: 4 Calls and 4 Returns with all BTB miss and Branch 
Misprediction 

Note: The RS TOS and write pointer are initialized to N-1 and 0, respectively.  The RS is empty when the 
rs_valid_r[TOS] is invalid and the write pointer is at the bottom (first) entry of the RS.  Both TOS and write 
pointer can be incremented or decremented at the same time.  The data is written based on the write 
pointer and not the TOS.  It is possible for the write pointer to wrap around in which case, the older 
entries in RS are overwritten and the bottom of the RS is no longer at physical location 0.  The 
rs_valid_r[TOS]=0 is RS empty, and write pointer is at the bottom of the RS. 

The second example is the same 4 calls and 4 returns where all the calls and returns are in the BTB and 
predicted correctly.  The BTB provides the correct predictions for the call, and the return stack provides 
the correct return address for all the BTB return predictions. 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               47 

PCInstTargetActionTOSAddrBasic BlockTOSWrptrEntryValAddrEntryValAddrAIA01BTB miss0XN-1000XIA140XBA030CEIA210X20CDCallB1BmissA-->CA, Call0101CA10CBCAIA39BTB miss0CA00CABIB02BTB miss0CAIB1BB020XIB211CB40XCallD2BmissB-->CB, Call1201CA30CECBIB38BTB miss0CA20CDIB410CBReturnCA8BmissCB-->RB, ReturnN-1000CARBDID03BTB miss1CBID130XBD021CDID211CB40XCallE3BmissD-->CD, Call2301CA30CECDID37BTB miss1CB20CDID410CBReturnCB7BmissCD-->RD, Return0101CARDEIE04BTB miss2CD40XIE131CEBE021CDIE211CBCallF4BmissE-->CE, Call3401CA40XCEIE36BTB miss2CD30CEBF120CDIE511CBReturnCD6BmissCE-->RE, Return1201CAREFIF05BTB miss3CEIF140XBF030CEIF221CDIF411CBReturnCE5BmissF-->RF, Return2301CARFBPQReturn StackReturn StackCuzco Microarchitecture Specification, v0.2 

 

 

Figure 4-13 Example 2: 4 Calls and 4 Returns with all BTB hits and Correct Branch 
Predictions 

The next example added branch mispediction for independent jump instructions within the call basic block 
or the return basic block.  Since the BTB and RS are in initialized states, no entry is set up yet, the RS states 
are modified as in Example 1. 

 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               48 

PCInstTargetActionTOSAddrBasic BlockTOSWrptrEntryValAddrEntryValAddrAIA01BTB hit,0XA-->CA, CallN-1000XIA140XBA030CEIA20110X20CDCallB01CA10CBCAIA300CABIB02BTB hit0CAB-->CB, CallIB1BB01220XIB211CB40XCallD01CA30CECBIB38BTB hit0CACB-->RB, ReturnN-1020CDIB410CBReturnCA00CARBDID03BTB hit1CBD-->CD, CallID130XBD02321CDID211CB40XCallE01CA30CECDID37BTB hit1CBCD-->RD, Return0120CDID410CBReturnCB01CARDEIE04BTB hit2CDE-->CE, Call40XIE13431CEBE021CDIE211CBCallF01CA40XCEIE36BTB hit2CDCE-->RE, Return1230CEBF120CDIE511CBReturnCD01CAREFIF05BTB hit3CEF-->RF, Return23IF1BF0IF2IF4ReturnCERFCuzco Microarchitecture Specification, v0.2 

 

 

Figure 4-14 Example 3: 4 Calls and 4 Returns and additional Jump, all BTB misses 
 

The last example is the same as example 2 where all calls and returns are correctly predicted by the BTB 
and mispredicted jumps are inserted into the call and return basic blocks.  Effectively, the jump 
instruction break the call and return basic blocks into 2 basic blocks as in Example of Error! Reference 
source not found. 

 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               49 

PCInstTargetActionTOSAddrBasic BlockTOSWrptrEntryValAddrEntryValAddrAIA01BTB miss0XN-1000XIA140XBA030CEIA210X20CDCallB1BmissA-->CA, Call0101CA10CBCAIA39BTB miss0CA00CABIB02BTB miss0CAIB1BB0X2ABmissB-->B3, jmp20XB3IB22BBTB miss0CA11CB40XCallD2BmissB3(B)-->CB, Call1201CA30CECBIB38BTB miss0CA20CDIB410CBReturnCA8BmissCB-->RB, ReturnN-1000CARBDID03BTB miss1CBID130XBD03ABmiss1D-->D3, jmp21CDD3ID23ABTB miss2CA11CB40XCallE3BmissD3(D)D-->CD, Call2301CA30CECDID37BTB miss1CB20CDID410CBReturnCB7BmissCD-->RD, Return0101CARDEIE04BTB miss2CD40XIE131CEBE021CDIE211CBCallF4BmissE-->CE, Call3401CA40XCEIE36BTB miss2CD30CEBF15ABmissCE-->E7, jmp20CDE7IE55BBTB miss2CD11CBReturnCD6BmissE7(CE)-->RE, Return1201CAREFIF05BTB miss3CEIF140XBF030CEIF221CDIF411CBReturnCE5BmissF-->RF, Return2301CARFBPQReturn StackReturn StackCuzco Microarchitecture Specification, v0.2 

 

 

 

Figure 4-15 Example 4: 4 Calls and 4 Returns with all BTB hits and additional Jumps 
with misprediction 

Note:   

•  The first jump misprediction shows that the call misprediction (Call D) for the same 
return address which is already in the return stack.  In this case, the return address 
must be compared with the return address in the BPQ to write into the next entry as in 
Example 1, but not increment the TOS and write pointer. 

•  On the 2A-Bmiss and 2-Bmiss, the TOS=1 and return address must write to the RS 

again because the BPU makes prediction ahead of the execution of the jump instruction.  
For example, a series of predicted call and return may overwritten the TOS=1: 

o  Predicted Call D pushes CA into TOS=1 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               50 

PCInstTargetActionTOSAddrBasic BlockTOSWrptrEntryValAddrEntryValAddrAIA01BTB hit,0XA-->CA, CallN-1000XIA140XBA030CEIA20110X20CDCallB01CA10CBCAIA300CABIB02BTB hit0CAB-->CB, CallCannot push the second time for the same callIB1replaceBB02ABmissB-->B3, jmp1220XB3IB22BBTB miss1CA11CB40XCallD2BmissB3-->CB, Call01CA30CECBIB38BTB hit0CACB-->RB, ReturnN-1020CDIB410CBReturnCA00CARBDID03BTB hit1CBD-->CD, CallID130XBD02321CDID211CB40XCallE01CA30CECDID37BTB hit1CBCD-->RD, Return0120CDID410CBReturnCB01CARDEIE04BTB hit2CDE-->CE, Call40XIE13431CEBE021CDIE211CBCallF01CA40XCEIE36BTB hit2CDCE-->RE, Return1230CEBF15ABmissCE-->E7, jmpreplace20CDE7IE55BBTB miss2CD11CBReturnCD6ABmissE7-->RE, Return1201CAREFIF05BTB hit3CEF-->RF, Return23IF1BF0IF2IF4ReturnCERFBPQReturn StackReturn StackCuzco Microarchitecture Specification, v0.2 

 

 

 

o  Predicted Return to CA pops TOS=1 and invalidate TOS=1 
o  Predicted Call X pushes XA into TOS = 1, the RS TOS=1 has address XA 
o  2A-Bmiss and 2-Bmiss have TOS=1 and CA which should be used to restore the 

RS to the same state as after 2-BTB-hit 

4.4.6 

Branch Prediction Queue 

Branch Prediction Queue keeps the branch prediction information for every predicted branch from the 
BTB and tracks the branch prediction through all pipeline stages from IF0 to completed execution in BEU. 
The BPQ is the central control for both IFU and BPU and for instruction PC throughout the execution 
pipeline. The number of BPQ entries is configurable, the default capacity is 16 entries 

Name 

Bits   Description 

Start Addr 

Target Addr 

End Addr 

RS Addr 

RS Recursive  

RS TOS pointer 

Misc bits 

GHR 

Same cache line 

63 

63 

63 

63 

8 

4 

4 

10 

2 

Start address of basic block 

Predicted target address 

End address of the basic block 

TOS return address 

RS recursive bits 

Current TOS entry on branch prediction 

BTB hit, BTB miss, predicted taken, exception 

Copy from GHR  

For next address with respect to start address 

Same page 

2x2 

Same page with start address and TLB for target,and next addresses 

BPB hit way 

2x3 

Bway of start, target, and next addresses. On BTB miss, the Bway of start address is updated when 
the start address access the iTLB.  The Bway of the next address is updated when the incremented 
address after the start address cross the page boundary 

BTB hit way 

Branch type 

BTB loop count 

DEC loop count 

EX loop count 

2 

3 

5 

5 

5 

Predicted BTB array hit way 

Predicted branch type 

Predicted loop count, to return to  

Predicted loop count, track by decode to calculate PC, count down every encounter 

Predicted loop count, track by BEU for loop execution, count down every encounter 

RET loop count 
Table 4-4. BPQ Bit Fields 

5 

Predicted loop count, track by ROB to retire each iteration, count down every encounter 

The BPQ accepts the predictions from BTB/GHT/RS, controls every fetched cache line in IFU, validates 
every byte from the IC data SRAM, controls the loop iteration in the tag array, provides prediction 
information to the decode unit to calculate decode PC, provides prediction information for instructions in 
XEQ, and receives the misprediction information from the BEU to update the BTB and GHT, and receives 
the retire information from the ROB.  Each BTB prediction is an entry in the BPQ and each entry controls 
the branch prediction as it moves through all the pipeline stages from IFU through the BEU.  In some 
cases, the same pipeline stages can correspond to multiple branch predictions. For example, in the 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               51 

Cuzco Microarchitecture Specification, v0.2 

 

 

decode/issue stage, 4 issued instructions can be all branch instructions which correspond to 4 predicted 
branches.  The BPQ has several independent pointers to interface with many pipeline stages:  

•  Datptr (1): validate the bytes of the instruction cache line from the IC data SRAM or instruction 

• 

pre-fetched buffer.  The IC data fetch is from the IC tag array.  The IFU data control sends an 
acknowledge signal to the BPQ once all the bytes from the cache line are written to the ICQ. 
Tagptr (1): validate and synchronize with fetched cache lines in ITQ. The tag fetch can be 1 
speculative cache line as the BTB is 2-cycle access.  The tag fetch sends an acknowledge signal to 
the BPQ when reading the cache line from tag SRAM array. 

•  Wrptr (1): write predicted branch from BTB, the BPQ is full when the wrptr entry is valid 
•  Rdptr (1): invalidate the entry once it is retired by the ROB, this is also retire pointer 

Current implementation for reset:  

• 

• 

Start_address is 0 (could be programmed to start at difference address or address 0 contains the 
address of boot code – implementation is different) 
The BPB/BTB/RS/BPQ/GHT valid bits are reset 

•  BPQ: sends the reset address to BTB and IC tag SRAM  
•  BTB: the reset address is BTB miss, BTB/GHT/RS/TLB go to idle mode, clock-gated off 
•  BPQ: BTB-miss causes sequential fetch from IFU until misprediction 
• 

IFU: IFU fetches instructions sequentially  

•  BEU: executes the first taken branch instruction which is branch misprediction which will cause 

the first basic block to be written to BTB along with prediction in GHT 

The ICQ adds the BPQ pointer to all the instructions of a basic block before sending to XIQ.  Any 
instruction (including previously not-taken branch instruction) can reference to the predicted basic block 
in the BPQ.  The BTB-miss basic block is the block with known start address but no end address where all 
instructions from start address has the same BPQ pointer.  The back-end units can get all needed 
prediction information from the BPQ pointer to properly execute the branch instruction. 

4.4.6.1 

BPQ Interfacing with the Front-End Units 

The BPQ generates the start address to all modules in the front-end.  Each new instruction stream is 
either from reset, branch misprediction or any redirection from the execution unit to the front-end.  Five 
cycles are needed to send instructions to the XIQ.  The BPU is 2 cycles while the IFU has single-cycle 
pipelining. 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               52 

Cuzco Microarchitecture Specification, v0.2 

 

 

Figure 4-16 Front-End Pipeline  

 

The BPQ generates the start address for the BPU and IFU and subsequently controls addresses to the IC 
tag and data arrays.  Each BTB prediction is an entry in the BPQ, the BPQ has the option of using the 
branch prediction directly from the BTB to the IC tag and data modules. 

• 

• 

The BTB continues to make a new prediction every 2 cycles and stops when the BPQ is full or on 
a BTB miss 
The ITAG fetches 2 cache lines if IC hits before it receives the prediction information from BTB.  
The IC hit fetches a cache line from IC data array for IDAT.  Four possible cases for prediction 
information from BTB: 

1.  BTB miss, the ITAG continues to fetch sequential cache lines 
2.  BTB hit, the exit point is in the same cache line, the ITAG invalidates the second fetch and 

removes the entry from ITQ 

3.  BTB hit, the exit point is in the next cache line, the ITAG fetches next cache line from the BTB 

next start address 

4.  BTB hit, the exit point is beyond the next cache line, the ITAG fetches next sequential cache 

line 

• 

The IDAT receives cache line and prediction information from BTB to extract the valid bytes for 
the ICQ and sending instructions to XIQ.  The cases are: 

1.  BTB miss, the current cache line is valid from start address (from the BTB) to the end of the 

cache line 

2.  BTB hit, the exit point is in the same cache line, the current cache line is valid from the start 

address to the next address, both addresses are from the BTB 

3.  BTB hit, the exit point is not in the same cache line, the current cache line is valid from the 

start address (from the BTB) to the end of the cache line 

The actual sequences of branch predictions and IC fetches are more complicated when taking into 
account the iTLB miss and IC miss cases.  The pipeline must be stalled until the valid IC tag or external 
instruction fetch data is received.  The most common sequences of instructions are from BTB hit and IC 
hit.  Examples are used to illustrate the flow of instructions and branch predictions.   

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               53 

IF0IF1IF2IF3IF4ActionBEU Misprediction to BPQBPQITLBITLB  - Access ITLB hitStall only ITAG on ITLB missBTBBTB-wrBTBBTB hit/missBPAGEBPAGE-wrBPAGEBPAGE hit/missGHTGHT-wrGHTTaken/non-takenRS-wrRestore RSITAGITAG-seqITAGIcache hit/missWrite to ITQBPAGEBPAGECalculate next addressRSRSPush Call, Pop returnIDAT1IDAT1-seqIDAT1Fetch cache lineIMQMiss is sent to BIUIDAT2IDAT2-seqCache line mux to ICQBypass on empty to XIQSelect sequential or taken to access BPAGECirculate IF2/IF3 until BPQ fullNext address back to BTBValidate IDAT cache lineStart address to ITAG, BTB, and GHTCuzco Microarchitecture Specification, v0.2 

 

 

Figure 4-17 Example 1, Basic Block with more than 2 cache lines  

 

The BTB prediction takes 2 cycles to make the prediction for Addr A and the ITAG goes ahead with 2 cache 
lines, Addr A and Addr A+1.   

• 

• 

Cycle 2: the BTB predicts the next start address is Addr B to cycle back to the BTB.  The BTB 
prediction is sent directly to the ITAG and IDAT, BPQ generates signal “bypass” so that the 
ITAG/IDAT selects branch information from the BTB instead of BPQ.  In this example, the basic 
block is more than 2 cache lines, the cache lines with Addr A and A+1 are good, and the ITAG can 
continue to fetch A+2.  The ITAG acknowledges 2 valid cache lines (assuming IC hit) while IDAT 
acknowledges of 1 valid cache line. 
Cycle 3: the BPQ deasserts “bypass” signals, all subsequent branch predictions are from BPQ.  
The ITAG acknowledges the third valid cache lines while IDAT acknowledges the second valid 
cache line if the second cache line is written to ICQ.  The basic block may end in the third cache 
line in which case the ITAG will get the branch prediction from BTB in cycle 4.  The IDAT will 
continue with the third cache line by sending its acknowledge to the BPQ.  

Figure 4-18 Example 2, Basic Block with 2 cache lines  

 

The basic block of Addr A is exactly 2 cache lines, the “same cache line [1]” signal from the BTB is asserted.   

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               54 

Time01234BPQAddr AEntry 0BTBAddr AAddr BAddr BAddr CReadBTB hitReadBTB HitITAGAddr AAddr A+1Addr A+2Addr A+3ReadReadReadReadIDATAddr AAddr A+1Addr A+2ReadReadReadICQAlignAlignBypassHitHitTime012345BPQAddr AEntry 0Entry 1BTBAddr AAddr BAddr BAddr CAddr CReadBTB hitReadBTB HitReadITAGAddr AAddr A+1Addr BAddr B+1Addr B+2ReadReadReadReadReadIDATAddr AAddr A+1Addr BAddr B+1ReadReadReadReadICQAlignAlignAlignBypassHitHitHitBypassCuzco Microarchitecture Specification, v0.2 

 

 

 

• 

• 

Cycle 2: the BTB predicts the next start address is Addr B to cycle back to the BTB.  The BTB 
prediction is sent directly to the ITAG and IDAT, the BPQ asserts the “bypass” signal so that the 
ITAG/IDAT selects branch information from the BTB instead of BPQ.  In this example, the basic 
block is 2 cache lines, the cache lines with Addr A and A+1 are good, and the ITAG uses Addr B 
from the BTB for the next tag array fetch.  The ITAG signals acknowledges of 2 valid cache lines 
(assuming IC hit) while IDAT signals acknowledge for 1 valid cache line. 
Cycle 3: the Entry 0 of BPQ is done with the ITAG in the last cycle and the next entry in BPQ is not 
valid, so the “bypass” signal continues to be asserted for the ITAG to get the next branch 
prediction from BTB.  The IDAT has 1 more cache line to write to the ICQ, the end address of the 
second cache line is from the BPQ.  If all valid bytes of the second cache line are written to the 
ICQ, then the acknowledge of the second valid cache line is sent to the BPQ.  The Entry 0 of BPQ 
could be done with the IDAT in this cycle.  In the next cycle, Entry 1 of BPQ is not valid, thus the 
“bypass” signal for IDAT is asserted by BPQ to receive data from the BTB.  Basically, if the next 
BPQ entry is not valid, then the “bypass” signal is asserted.  The Tagptr and Datptr are 
independently incremented based on acknowledge of all the cache lines in the basic block in the 
BPQ.  For each cache line in the basic block, the BPQ sends the address of the cache line to the 
ITAG and IDAT.  If the cache line is the last cache line of the basic block, then the “dat_last_line” 
is also sent to IDAT where the end address is used instead of the last valid byte of the cache line. 

 

Figure 4-19 Example 2, Basic Block with single cache line  

The basic block of Addr A is exactly 1 cache line, the “same cache line [0]” signal from the BTB is asserted.   

• 

• 

Cycle 2: in this example, the basic block is 1 cache line, the ITAG invalidates the Addr A+1 in the 
ITQ and uses Addr B from the BTB for the next tag array fetch.  The ITAG acknowledges 1 valid 
cache line (assuming IC hit) while IDAT also acknowledges 1 valid cache line. 
Cycle 3: the Entry 0 of BPQ is done with the ITAG and IDAT in the last cycle and the next entry in 
the BPQ is not valid, so the “bypass” signal continues to be asserted for both ITAG and DTAG to 
get the next branch prediction from BTB.   

For small loop prediction, the ICQ handles all iteration, the BPQ sees the small loop prediction as a not-
taken basic block.  For large loop prediction, the BPQ keeps track of the loop iteration and send the start 
address cache line to the end address cache line to the ITAG and the IDAT. 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               55 

Time012345BPQAddr AEntry 0Entry 1BTBAddr AAddr BAddr BAddr CAddr CReadBTB hitReadBTB HitReadITAGAddr AAddr A+1Addr BAddr B+1Addr B+2ReadReadReadReadReadIDATAddr AAddr BReadReadICQAlignAlignBypassHitHitHitBypassCuzco Microarchitecture Specification, v0.2 

 

 

4.4.6.2 

BPQ Interfacing with the Back-End Units 

The PC is calculated in 3 different pipeline stages: 

• 

• 

• 

In BPQ for branch prediction and IC to fetch instructions 
In Decode (ID2 stage) to attach the PC to all branch instructions for execution 
In ROB for the architecture PC to take into account branch mispredictions, exceptions, and 
interrupts 

The branch execution needs branch prediction information to validate the prediction.  Since the BPQ 
pointer is attached to all instructions of the basic block, the back-end units send the BPQ pointers to BPQ 
to read the branch prediction information.  The BPQ pointers take 1 cycle to route from the back-end 
units to the BPQ: 

• 

• 

• 

The BPQ pointers are sent in the ID1 stage to get the branch prediction information back in the 
ID2 stage 
The BPQ pointers are sent in the EX0 stage to get the branch prediction information back to the 
BEUs in the EX1 stage 
The BPQ provides branch information from the Rdptr and the ROB sends acknowledge to the 
BPQ as the branch is retired 

The start address, next address, target address, branch taken, and branch types are sent to the back-end 
units to validate and increment the PC accordingly.   

Since multiple BPQ pointers can be sent from the independently operating BEUs, the BPQ must indicate 
which BPQ pointer is the earlier branch so that multiple mispredictions can be correctly selected.  Only 
the oldest branch can be mispredicted.  

The BPQ handles loop prediction differently.  The BPU treats the loop in the BTB as a not-taken branch 
where the next address is the address after the last loop instruction.  For the back-end units, each loop 
iteration is a taken branch instruction except for the last iteration of the loop.  If the BPQ pointer from the 
back-end units is part of the loop prediction, then the corresponding loop count is decremented.  Three 
sets of loop counts: decode, execute, and retire are used and independently decremented.  When the 
loop count is zero, then the prediction is not-taken.  Note that more than one BPQ pointer can be 
received for the same loop prediction, the BPQ pointer must be correctly returned with the branch 
prediction information to the back-end units. 

4.5  Summary of Branch Executions and Mispredictions 

In the current implementation, all branch mispredictions are from a BEU for simplicity.  The possible 
misprediction due to “not-a-branch” in the decode unit is not yet implemented.  Mispredictions due to 
unconditional branches are delayed to the BEU.  The ROB is responsible to assert execution redirection 
from many sources: interrupt, LS precise exception, LS data bus error (treated as an interrupt), instruction 
SWI, and IFU precise exceptions which include TLB page fault, instruction break point, ECC/parity error, 
and memory protection error (user/supervisor mode violation). 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               56 

Cuzco Microarchitecture Specification, v0.2 

 

 

Predicted 

Actual 

WB to BPU 

Actions 

NT 

NT 

T 

NT 

NT 

BPQ ex_pointer is incremented, write 1 to GHT[0] 

Not-a-branch 

Not-a-branch 

 

 

The BTB entry remains valid, may consider invalidating, 
but hopefully the PLRU will remove the entry 

Misprediction, he BTB entry is invalidated 

Not Predicted 

NT 

None 

Nothing 

Not Predicted 

T 

target/next 
addresses, T 

Misprediction: BTB is written with new entry, GHT is 
written with 10 

NT 

NT 

T 

T 

T 

Matched Target, T 

T 

Misprediction, GHT[0] is read 
- If 1, write 0 to GHT[0], else write 1 to GHT[1] 

Mismatched Target, T 

target/next 
addresses, T 

Misprediction, BTB is updated, GHT[0] is read 
- If 1, write 0 to GHT[0], else write 1 to GHT[1] 

NT 

NT 

Misprediction, GHT[0] is read 
- If 1, write 0 to GHT[0], else write 0 to GHT[1] 

Matched Target, T 

T 

BPQ pointer is incremented, write 1 GHT[0]  

Mismatched target, T 

Target/next 
addresses, T 

New target is written to BTB, write 1 to GHT[0] 

Table 4-5. Branch Prediction and Execution 

BPQ 

Next 

Next 

Flush 

Nothing 

Flush 

Flush 

Flush 

Flush 

Next 

Flush 

Implementation note: Not-a-branch is not yet implemented.  The “not-a-branch” mispredictions are from 
the stale entry of a prior context switch or the replacement of a BPB entry (in which the BTB still 
references the old entry in the BPB which will jump to an incorrect address on a “not-a-branch” 
instruction). 

The ROB keeps track of cancelled instructions from misprediction or execution redirection.  The cancelled 
branch retires the entry in BPQ without any change in the BPU.  The cancelled branch does not update the 
PC in the ROB. 

4.6  Branch Predictor Exploration 

Here we’ll document other potential branch predictors. For example an, ahead-pipelined zero-bubble 
uBTB backed with a large TAGE direction corrector and indirect branch BTB (iBTB) implemented across 
several cycles that also drives instruction prefetching and L2 cache replacement policy. See  

Figure 4-20: Basic Decoupled BPU-IFU Pipeline 

 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               57 

Cuzco Microarchitecture Specification, v0.2 

 

 

4.6.1 

BPU and IFU Pipeline Structure 

A decoupled BPU and IFU pipeline structure is as follows (shown in Figure 4-21 as closely matching the 
pipe stages in the Figure 4-1): 

•  BP0: the PC address from branch misprediction and redirection (exception, interrupts) from the 

execution pipeline are flopped in BPQ before accessing the uBTB, TAGE/large predictor, iBTB, 
branch page buffer (BPB), and GHT arrays.  The iTLB is accessed in this cycle to avoid address 
aliasing.  The BPB contains only virtual page numbers. 

•  BP1: a prediction is returned by uBTB, the prediction data are returned from the BTB SRAM array 

for a larger BTB.  

o  The baseline BTB prediction is 2 clock cycles. Since branch prediction is per basic block, 

the next address (i.e., the exit point of the basic block) is not known until after the 
prediction, so it is equivalent to a 2-cycle blocking BTB.  The BTB hit/miss is known in 
this cycle while the prediction and target address are calculated in the next cycle.   

o  The iBTB is read, and target computed.  
o  All BTB-hit/miss, target address, start address, next address, branch type, and loop 

prediction are flopped for the next cycle.  

o  The GHT is also accessed in this cycle to generate taken/non-taken prediction; data from 

the GHT can be flopped before determining taken/non-taken in next cycle for timing. 

•  BP2: Multiple events on this cycle are: 

o  Prediction computation by large BTB; prediction comparison with uBTB on previous 

cycle. 

o  Prediction by medium sized BTB (if exists). 
o  Prediction by iBTB for indirect branches. 

•  BP3: Prediction by large BTB which corrects any previous predictions. 

o  A BTB-hit provides a target address prediction. The BTB provided partial target and next 

addresses are translated into full addresses by reverse lookup of the page buffer.  
(Cuzco: TBD) The BTB includes the “same page” bit to skip accessing the page buffer.  

o 
o  The BTB target prediction is based on branch type (1) return, target address is taken 
from the RS (2) taken, target address is taken from the BTB and if indicated, the page 
buffer, (3) non-taken, then the next address is taken from the BTB and page buffer.  The 
predicted target address is written to the BPQ and routed to the IC tag array for next 
instruction address. 

o  Up to two-taken (2T) branches are predicted on each cycle. 
o  Classifications of various branches are used to improve predictions: 

▪  Not-Taken (NT) do not enter the BTB, till they are taken for the first time. 
▪  Always-Taken (AT) do not affect weights of aliases till they fail the first time. 
▪ 

Function calls always use Return-Address-Stack on returns and do not update 
weights. 

▪  A minimum of two branch predictions would be the target: 2T, T-NT, NT-T and 

NT-NT predictions for two consecutive branches are maintained. 

• 

FTQ: (Fetch Target Queue) Branch target addresses on prediction from BP1, BP2, and BP3 are 
sent to FTQ. Target of an executed branch on misprediction also comes directly to an emptied 
FTQ. Each successive corrections in BP2 and BP3 empties predictions from earlier BTBs (i.e., 
those in BP1, BP2 stages). 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               58 

Cuzco Microarchitecture Specification, v0.2 

 

 

• 

• 

• 

• 

 

o  The FTQ controls the next address to the IC tag array and ICQ. 
o  The BPQ controls the next address to the branch prediction pipeline: BP0, BP1, BP2 and 

BP3. 

 IF0: the PC address from branch prediction, misprediction and redirection (exception, interrupts) 
from the execution pipeline are flopped in FTQ before accessing the IC tag array.  The iTLB is 
accessed in this cycle to avoid address aliasing.  (TBD: Revisit: The BPB contains only virtual page 
numbers.) 
IF1: IC tag array data, IC data bank/way and iTLB data are read in this cycle. 

IC tag array data is used to determine the cache hit on the virtual address. 

o 
o  Way prediction is verified, otherwise on a cache hit and way-miss, the correct way is 

re-read through IF0 stage. 

o  On a cache miss, iTLB data is used to prepare the physical address for an iCache miss. 

IF2: A BTB-hit provides a target address prediction. The BTB provided partial target and next 
addresses are translated into full addresses by reverse lookup of the page buffer. The BTB 
includes the “same page” bit to skip accessing the page buffer. The BTB target prediction is based 
on branch type (1) return, target address is taken from the RS (2) taken, target address is taken 
from the BTB and if indicated, the page buffer, (3) non-taken, then the next address is taken from 
the BTB and page buffer.  The predicted target address is written to the BPQ and routed to the IC 
tag array for next instruction address. 
IF3: The BPQ controls the next address to the IC tag array and ICQ. 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               59 

Cuzco Microarchitecture Specification, v0.2 

 

 

Figure 4-21 Decoupled Multi-Predictor BPU-IFU Pipeline 
 
Key components of the multi-predictor BPU are as follows: 

 

• 

FTQ: Fetch Target Queue as described earlier forms the main link between the BPU and IFU. FTQ 
may be implemented as a part of BPQ, as the BPQ carries the same data for longer duration 
(than just the Fetch pipeline for FTQ) for the life of a branch being predicted through its 
execution and the predictor update. 

•  BPQ: A predicted branch is lodged into the BPQ  

o 

o 

to provide fetch address to the Fetch pipeline, and alignment information on ICache 
read,  
to update speculative from the result of branch execution or load-store execution, 
leading to either continuation on the same path or a target address redirect. 

•  Branch Update Queue: Executed branches provide results to train the branch predictor(s) in the 

BPU. When the branch execution result matches the branch prediction, the strength of the 
prediction is increased and the target address is maintained, otherwise the strength is decreased 
and/or the target address is updated. 
Prediction and Address Selection Mux: This mux selects the target prediction and the address for 
next fetch group and next branch prediction: 

• 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               60 

Cuzco Microarchitecture Specification, v0.2 

 

 

o  The next fetch address: the redirect address coming from BEU, the next (predicted) 
fetch address from BPQ when BPU prediction is ahead of the IFU fetch, and the next 
(predited) fetch address when BPU prediction and IFU fetch are in sync. 

o  The next address to predict: the redirect address coming from BEU, the next (predicted) 

target address from the BPU prediction from any of the branch predictors (with BTB 
providing branch target as well as the next sequential address), using the tournament 
selector. 

• 

• 

• 

• 

Tournament Selector: 
nBTB (nanoBTB): 
TAGE and TAGE-L: 
Perceptron: 

•  RS: 
• 

Indirect Predictor: 

4.6.2 

FTQ and BPQ 

BPQ is maintained as the original design in the structure with differences outlined below. 

FTQ is effectively a subset of BPQ such that FTQ provides the current fetch target address to IFU to fetch 
entries from the L1 ICache and pass on through decode to the rest of the core pipeline.  

The BPQ has several independent pointers to interface with many pipeline stages:  

•  Datptr (1): validate the bytes of the instruction cache line from the IC data SRAM or instruction 

o 

pre-fetched buffer.  The IC data fetch is from the IC tag array.  The IFU data control sends an 
acknowledge signal to the BPQ once all the bytes from the cache line are written to the ICQ. 
(new) As the new functionality of Wrptr shows below, the Datptr fetches next fetch 
block from ID tag array followed by L1 ICache using the most accurate prediction 
available from a set of predictors.  
(new) Fetch target address supplied to the fetch pipeline (IFU) can be different in time 
with respect to the address supplied to the branch predictors to allow the branch 
predictor to run ahead to achieve more accurate prediction as well as keep the fetch 
pipeline busy. 

o 

• 

Tagptr (1): validate and synchronize with fetched cache lines in ITQ. The tag fetch can be 1 
speculative cache line as the BTB is 2-cycle access.  The tag fetch sends an acknowledge signal to 
the BPQ when reading the cache line from tag SRAM array. 

•  Wrptr (1): write predicted branch from BTB, the BPQ is full when the wrptr entry is valid 

o 

o 

o 

o 

(new) Wrptr for each prediction is used for filling the BPQ with accurate predictions 
from a set of multiple predictors as mentioned below. 
(new) Wrptr is supplied by each of the branch predictors indicating which BPQ entry is 
predicted by that predictor (due to their inherent deifferent latencies). Also, the 
tournament selector chooses the predictor and its prediction to update the BPQ. 
(new) A long latency branch predictor can update an older predicted branch, there by 
overwriting the older prediction and the target address read earlier using BTB. 
(new) When the long latency branch predictor updates the prediction to a different 
target address that a low latency branch predictor (GHR/GHT etc.), all predictors are 
reset to use the new prediction and the target address. 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               61 

Cuzco Microarchitecture Specification, v0.2 

 

 

o 

(new) When a target address for a BPQ entry is already being fetched by the ITAG and 
L1 ICache, the BPQ entry is no overwritten by the long latency branch predictor. Instead, 
the new prediction is written to the next available entry, and the all predictors move on 
to the new direction and the new target address.  

▪ 

▪ 

In this scenario, on BEU update for the low latency predictor is simply 
invalidated if found incorrect, because the long latency prediction would get 
executed in the next few cycles on the correct path.  
It is possible that long latency prediction is incorrect, which is treated similar to 
a normal branch mispredict. 

•  Rdptr (1): invalidate the entry once it is retired by the ROB, this is also retire pointer 

(Same as original IFU-BPU design) Current implementation for reset:  

• 

• 

Start_address is 0 (could be programmed to start at difference address or address 0 contains the 
address of boot code – implementation is different) 
The BPB/BTB/RS/BPQ/GHT valid bits are reset 

•  BPQ: sends the reset address to BTB and IC tag SRAM  
•  BTB: the reset address is BTB miss, BTB/GHT/RS/TLB go to idle mode, clock-gated off 
•  BPQ: BTB-miss causes sequential fetch from IFU until misprediction 
• 

IFU: IFU fetches instructions sequentially  

•  BEU: executes the first taken branch instruction which is branch misprediction which will cause 

the first basic block to be written to BTB along with prediction in GHT 

 
 

4.6.3 

BPU Interfaces 

This section describes the top-level port of the frontend block (cz_frontend.sv) 

(Unchanged from Frontend MAS) The following is BPU interface with the Branch Execution Unit ( BEU). 
The BEU is part of the execution units. 

I/O 

Source 

Dest. 

Width 

Name 

Description 

Input 

Issue (idu_d2)  

1 

bpu_bpq,  
bpq_btb, 
ifu_dat, 
ifu_tag, 
ifu_imq  

brn_mispredict  Used in bpq, btb, and imq etc. Validates 
the imq_fwd in imq, and clears ifu_dat. 

Input 

Issue(idu_d2_i2)  bpq 

 

brn_taken 

Taken misprediction 

Input 

Input 

Issue 
(idu_d2_i2) 
Issue 

bpq 

bpq 

Input 

Issue (idu, exu) 

bpq 

Input 

Issue 

bpq 

IADDR_RANGE, [63:1] 

brn_next_pc 

For offset calculation 

IADDR_RANGE, [63:1] 

brn_target_pc 

brn_mispredict  and brn_taken ,  

BTB_BTYPE_RANGE, 
[2:0] 
BTB_LP_CNT_RANGE, 
[4:0] 

beu_btype 

beu_lp_cnt 

 

 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               62 

Cuzco Microarchitecture Specification, v0.2 

 

 

Input 

Issue 

bpq 

Input 

Issue 

bpq 

Input 

Issue 

bpq 

BRN_RANGE,  
[3:0] default ISSUE8,  
[1:0] for ISSUE4,  
[0] for ISSUE2 
BRN_RANGE, 
[3:0] default ISSUE8, 
[1:0] for ISSUE4, 
[0] for ISSUE2 
BPQ_PTR_RANGE, [3:0] 

beu_valid_r 

[Thang] some signals are sent from all 
functional units 
 

beu_bmiss 

 

bmiss_num 

It is now bpq_exptr[bmiss_num] 

 Table 6-1 Interface with BEU 

(Unchanged from Frontend MAS) The following is Instruction execution interface from XBEU. It has two 
sets of start address and length, ex and ex1, and relevant information derived by indexing into data using 
bpq_ptr. 

I/O 

Source  Dest.  Width 

Name 

Description 

Input 

Issue 

bpq 

BPQ_PTR_RANGE, 
[3:0] 

Input 

Issue 

bpq 

Output 

bpq 

Issue 

Output 

bpq 

Issue 

Output 

bpq 

Issue 

Output 

bpq 

Issue 

BRN_RANGE,  
[3:0] default ISSUE8,  
[1:0] for ISSUE4,  
[0] for ISSUE2 
BRN_RANGE,  
[3:0] default ISSUE8,  
[1:0] for ISSUE4,  
[0] for ISSUE2 
BRN_RANGE,  
[3:0] default ISSUE8,  
[1:0] for ISSUE4,  
[0] for ISSUE2 
BRN_RANGE,  
[3:0] default ISSUE8,  
[1:0] for ISSUE4,  
[0] for ISSUE2 
IADDR_RANGE, [63:1] 

Output 

bpq 

Issue 

ICQ_IDX1_RANGE, 
[5:1] 

Output 

bpq 

Issue 

IADDR_RANGE, [63:1] 

Output 

bpq 

Issue 

ICQ_IDX1_RANGE, 
[5:1] 

brn_bpqptr [`BRN_RANGE] 
[3:0] default ISSUE8,  
[1:0] for ISSUE4,  
[0] for ISSUE2 
brn_valid 

bpq_ex_valid 

bpq_ex_loop 

bpq_ex_taken 

This pointer is used to index into bpq 
valid, data, and length for execution. 

Used countdown of loop count or to 
take misprediction during retiring by 
ROB or for execution by XBEU 

This information is needed for XBEU 
and is derived by indexing into 
bpq_data by brn_bpqptr.  
The bpq_ex_loop is indicative of 
BTYPE_LPICQ and BTYPE_LPICQ1. 
[Thang] Loop is predicted non-taken 
by BTB, but the BPQ must send 
bpq_ex_taken until the loop count for 
Execute is counted down to zero. 

bpq_ex_target[`BRN_RANGE] 
[3:0] default ISSUE8,  
[1:0] for ISSUE4,  
[0] for ISSUE2 
bpq_ex_length    
[3:0] default ISSUE8,  
[1:0] for ISSUE4,  
[0] for ISSUE2 
bpq_ex_start[`BRN_RANGE] 
[3:0] default ISSUE8,  
[1:0] for ISSUE4,  
[0] for ISSUE2 
bpq_ex1_length[`BRN_RANGE] 
[3:0] default ISSUE8,  

These are bpq length and bpq data 
being indexed by the brn_bpqptr 

These are bpq length and bpq data 
being indexed by the brn_bpqptr -1  

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               63 

Cuzco Microarchitecture Specification, v0.2 

 

 

Output 

bpq 

Issue 

IADDR_RANGE, [63:1] 

[1:0] for ISSUE4,  
[0] for ISSUE2 
bpq_ex1_start[`BRN_RANGE] 
[3:0] default ISSUE8,  
[1:0] for ISSUE4,  
[0] for ISSUE2 

[Thang] the ex1 signal used for loop 
detection when 2 basic blocks are 
combined into a single loop (the 
normal loop is single basic blocks) 

Table 4-2 7BPU Interface to XEQ_BRN (Instruction execution from XBEU) 

(Unchanged from Frontend MAS) The following is BPQ interface to Decode Unit for Instruction decode to 
setup PC in D2 stage 

I/O 

Input 

Input 
Input 

Source 
Issue 

Issue 
Issue 

 
bpq 

bpq 
bpq 

Width 
BPQ_PTR_RANGE, 
[3:0] 
[`INST_RANGE], [7:0] 
[`INST_RANGE], [7:0] 

Name 
dec_bpqptr [`INST_RANGE], 
[7:0] 
dec_brn_valid 
dec_lp_valid 

Output 
Output 

bpq 
bpq 

Issue 
Issue 

Output 

bpq 

Issue 

[`INST_RANGE], [7:0] 
IADDR_RANGE, 
[63:1] 
[`INST_RANGE], [7:0] 

bpq_dec_taken 
bpq_dec_target[`INST_RANGE], 
[7:0] 
bpq_dec_valid 

Table 4-3 8BPQ interface to Decode Unit 

 

Description 
Used to index into bpq lpval and dec lpcnt 
for loop count logic, and to decide 
whether to take misprediction instead of 
counting down loop count. The 
dec_bpqptr is also used to set up PC in D2 
stage. 
[Thang] Loop is predicted non-taken by 
BTB, but the BPQ must send 
bpq_dec_taken until the loop count for 
Decode is counted down to zero. 
The inputs dec_bpqptr and dec_lp_valid 
are used to index into bpq valid and data 
to get these three output signals. The 
taken and target are from the bpq data 
and valid is the indexed valid. 

(Unchanged from Frontend MAS) The following is a request to BIU from the Frontend’s IFU’s Instruction 
Miss Queue (ifu_imq) and is used for IMQ storage. 

I/O 

Output 

Source  Dest. 
ifu 

AXI 

Output 

ifu 

AXI 

Output 
Output 

ifu 
AXI 
ifu_imq  ext 

Width 
BURST_RANGE, 
[3:0] 
ADDR_RANGE, 
[63:0] 
 
 

Input 

 BIU 

mem 
ifu 

 

Input 

 BIU 

ifu 

AXI_ID_RANGE, 
[5:0] 

Name 
ifu_biu_burst 

Description 
when bus width is less than the cache line size 

ifu_biu_raddr 

Request address from the IMQ based on the IMQ rq ptr 0 

ifu_biu_rd_valid 
ifu_rdata_ready 

biu_ifu_rd_ready 

biu_ifu_axi_id 

This is a cache line request. 
rd_valid and rd_ready together indicate that IMQ accept. 
[Thang] ifu_biu_rd_valid remains asserted until 
biu_ifu_rd_ready is high. Request is accepted by external 
memory 
When IMQ is accepting, this is part of IMQ FIFO data 
storage, and the AXI ID must match with an IMQ entry. 
[Thang] Need to covert to output. 

 Table 4-4 9Frontend's IFU request to BIU 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               64 

Cuzco Microarchitecture Specification, v0.2 

 

 

(Unchanged from Frontend MAS) The following interface is between Frontend’s IFU Instruction Miss 
Queue (IMQ) (ifu_imq) and the external memory. The IDB – Instruction Prefetch Buffer forwarding is done 
when the rdptr of ITQ increments. 

I/O 

Input 

Input 

Input 

Source 
ext.me
m  
ext.me
m 

ext.me
m 

Outpu
t 

ifu_imq 

ext 
mem 

Dest. 
ifu_imq 

Width 
AXI_ID_RANGE, [5:0] 

ifu_imq 

BUS_DATA_RANGE, 
[255:0] 

Name 
biu_ifu_rdata_axi_i
d 
biu_ifu_rdata_data 

Description 
The AXI ID must match with an IMQ entry 

Data for prefetch forwarding sent to ICQ and written 
to data array. Cancel IDB entry if itag continues to 
fetch new cache line. 

ifu_imq 

 

 

biu_ifu_rdata_valid  When these two are asserted, write external memory 

ifu_rdata_ready 

Instruction prefetch buffer (IDB). 
[Thang] bitu_ifu_rdata_valid remains asserted until 
ifu_rdata_ready is high. Fetched data is accepted by 
IFU 
 

Table 4-5 Interface from External memory to IFU (IMQ) 

 

(Unchanged from Frontend MAS) The following interface is between Frontend’s IFU ( ifu_dat) and the 
scalar Instruction queue (XIQ). When the XIQ is not full and can receive incoming instruction, the ifu_dat 
can assert the xi_valid and indicate a new instruction being presented on the interface. The information 
presented includes the Branch Prediction bit, Loop Prediction bit, the exception bit, i32/16 bit (xi_i32_val), 
the inst (xi_inst), the BPQ pointer. 

I/O 

Input 

Source  Dest 
Issue 

ifu_dat 

Width 
[`INST_RANGE], [7:0] 

Name 
xiq_full 

Output 
Output 

ifu_dat 
ifu_dat 

Issue 
Issue 
(XIQ) 

[`INST_RANGE], [7:0] 
[`INST_RANGE], [7:0] 

xi_valid 
xi_bp_val 

Output 

ifu_dat 

Issue 

[`INST_RANGE], [7:0] 

xi_lp_val 

Output 

ifu_dat 

Issue 

[`INST_RANGE], [7:0] 

xi_i32_val 

Output 

ifu_dat 

Issue 

[`IDATA_RANGE] 

xi_inst[`INST_RANGE], 
[7:0] 

Output 

ifu_dat 

Issue 

Output 

ifu_dat 

Issue 

BPQ_PTR_RANGE, 
[3:0] 
[`INST_RANGE], [7:0] 

xi_bpqptr[`INST_RANGE], 
[7:0] 
xi_exception 

Description 
When set, it stops the xi_valid from being 
asserted. 
This qualifies the remainder of the interface 
Branch Prediction bit, either from ICQ or cache 
line. It is bit 0 for 16bit inst, or, OR of 1:0 for 
32-bit inst. 
[Thang] indicates the last instruction of the 
basic block 
Loop Prediction bit when loop is valid. It is 1 bit 
for 16b inst, or, OR of 2 bits for 32-bit inst. 
[Thang] indicates the last instruction of the 
basic block when loop is valid in ICQ 
When set, it indicates that it is 32-bit inst. 
otherwise 16. [Thang] only the first bit is 
relevant 
The ICQ Instruction  
[Thang] always 32-bit, but only [15:0] is 
relevant for 16-bit instruction 
The BPQ Pointer [Thang] the instruction is in 
the basic block of the BPQ pointer 
The ICQ exception bit  

 Table 4-6 10Frontend’s IFU_DAT interface to Scalar Instruction Queue(XIQ) 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               65 

Cuzco Microarchitecture Specification, v0.2 

 

 

 

(Unchanged from Frontend MAS) The following interface is for ROB retire branch, when misprediction 
occurs or should be taken when Loop count is all ‘1’s 

 

 

I/O 

Input 

Source 
Issue 

Dest. 
bpu_bpq 

Width 
 

Name 
rob_brn_miss 

Input 

Issue 

bpu_bpq 

IADDR_RANGE, [63:1] 

rob_except_pc 

 

rob_exception  

Description 
For ROB retire branch, take 
misprediction and not countdown if 
loop count is all 1s 
when rob_exception occurs, it’s the 
start address to BTB to read on 
misprediction 
It clears the BPQ, IFU dat and tag 

Input 

Issue 

Output 

bpq 

Output 

bpq 

Output 

bpq 

bpu_bpq, 
ifu_dat, 
ifu_tag 
Issue 
(pipe_ctrl) 

Issue 
(pipe_ctrl) 

Issue 
(ppe_ctrl) 

Input 

Issue 

bpu_bpq 

BRN_RANGE,  
[3:0] default ISSUE8,  
[1:0] for ISSUE4,  
[0] for ISSUE2 
BRN_RANGE,  
[3:0] default ISSUE8,  
[1:0] for ISSUE4,  
[0] for ISSUE2 
IADDR_RANGE, [63:1] 

BRN_RANGE,  
[3:0] default ISSUE8,  
[1:0] for ISSUE4,  
[0] for ISSUE2 

bpq_rob_taken 

When ret_lpcnt_nxt, or nxt_retptr in 
bpq 

bpq_rob_valid 

Retire entry from the ROB, setup 
retire PC 

bpq_ret_pc[`BRN_RANGE] 
[3:0] default ISSUE8,  
[1:0] for ISSUE4,  
[0] for ISSUE2 
rob_ret_ack 

It’s the nxt_retptr from bpq 

Retire entry and the younger 
branches  when the mispredicted 
branch is retired. 
Also, to take misprediction and not 
count down when loop count is all 1s 

Table 4-7 11For ROB retire branch 

I/O 

Source  Width 

Name 

Description 

Input 
Input 
Input 
Input 
Input 
Input 
Input 
Input 
Input 
Input 

CSR 
CSR 
CSR 
CSR 
CSR 
CSR 
CSR 
CSR 
CSR 
CSR 

 
 
 
 
 
 
 
 
 
 

csr_mclk_ctl_bpq 
csr_mclk_ctl_bpu 
csr_mclk_ctl_btb 
csr_mclk_ctl_ght 
csr_mclk_ctl_icd 
csr_mclk_ctl_icm 
csr_mclk_ctl_ict 
csr_mclk_ctl_ifu 
csr_mclk_ctl_itlb 
csr_tlb_disable 

From CSRs, for power management 
From CSRs, for power management 
From CSRs, for power management 
From CSRs, for power management 
From CSRs, for power management 
From CSRs, for power management 
From CSRs, for power management 
From CSRs, for power management 
From CSRs, for power management 
From CSRs, for power management 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               66 

Cuzco Microarchitecture Specification, v0.2 

 

 

 

Table 4-8 12Block mclk control for power management 

I/O 

Output 
Output 
Output 
Output 
output 
output 
output 

Source  Width 
cz_idle 
cz_idle 
cz_idle 
cz_idle 
cz_idle 
cz_idle 
cz_idle 

 
  
 
 
 
 
 

 

I/O 

Input 

Input 
Input 

Source  Width 
System 

 

System 
System 

 
 

Description 
Idle blocks – idle signals 

Name 
imq_idle 
btb_idle 
bpq_idle 
idat_idle 
itag_idle 
itlb_idle 
ght_idle 

Table 4-9 13Block Idle signals 

Name 
clk 

rstn 
test_en 

Description 
Clock 

Reset 
Test 

Table 4-10 Core signals - clk, rstn, test_en 

4.6.4 

Frontend block’s BPU/IFU/SRAMS-TLB-Caches interfaces  

(Internal interfaces unchanged so far from Frontend MAS ) 

BPU 
I/O 

Width 

Name 

Description 

BPU/bpu_btb/bpu_page 
interface with idle. All 
the input signals to the 
FE/BPU/.are tied off in 
the cz_idle.sv 

Input 

BPAGE_INDEX_RANGE, [4:0] 

bpage_cm_addr 

Input 

 

bpage_cm_valid 

Input 

BPAGE_WADDR_RANGE, [1:0] 

bpage_cm_way 

Input 

BPAGE_ADDR_RANGE, [63:15] 

bpage_cm_wdata 

Input 

Input 

 

 

bpage_cm_write 

bpage_cm_wval 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               67 

Cuzco Microarchitecture Specification, v0.2 

 

 

Output  BPAGE_ADDR_RANGE, [63:15] 

bpage_cm_rdata 

Output 

 

bpage_cm_rval 

Input 

IADDR_RANGE, [63:1] 

btb_cm_addr 

Input 

Input 

 

 

btb_cm_valid 

btb_cm_valin 

Input 

BTB_WADDR_RANGE, [1:0] 

btb_cm_way 

Input 

BTB_DATA_RANGE, [47:0] 

btb_cm_wdata 

Input 

 

btb_cm_write 

Output  BTB_DATA_RANGE, [47:0] 

btb_cm_rdata 

Output 

 

btb_cm_valout 

BPU/bpu_btb interface 
with idle. All the input 
signals to the 
FE/BPU/bpu_btb are 
tied off in the cz_idle.sv 

Input 

BTB_DATA_RANGE, [47:0] 

btb_data_rdata BTB_WAY_RANGE, [3:0] 

BTB - SRAM 

output 

[`BTB_INDEX_RANGE], [8:0] 

btb_data_addr 

output 

 

btb_data_bit_we 

output  BTB_WAY_RANGE, [3:0] 

btb_data_cs 

output  BTB_DATA_RANGE, [47:0] 

btb_data_wdata 

Input 

[`GHT_ADDR_RANGE], [7:0] 

ght_cm_addr 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               68 

Cuzco Microarchitecture Specification, v0.2 

 

 

Input 

Input 

 

 

ght_cm_bank 

ght_cm_valid 

BPU/bpu_ght interface 
with idle. All the input 
signals to the 
FE/BPU/bpu_ght are 
tied off in the cz_idle.sv 

Input 

[`GHT_DATA_RANGE], [63:0] 

ght_cm_wdata 

Input 

 

ght_cm_write 

Output 

[`GHT_DATA_RANGE], [63:0] 

ght_cm_rdata 

input 

[`GHT_DATA_RANGE], [63:0] 

ght_tag_rdata [`GHT_BANK_RANGE], 
[1:0] 

GHT – SRAM array 
interface. 

Output 

[`GHT_INDEX_RANGE], [5:0] 

ght_tag_addr[`GHT_BANK_RANGE], [1:0] 

The ght_bank_sel is 
active next cycle 

Output 

[`GHT_CS_RANGE], [7:0] 

ght_tag_cs 

output 

[`GHT_SBK_PTR_RANGE],  [1:0] 

ght_bank_sel[`GHT_BANK_RANGE], [1:0] 

Output 

[`GHT_DATA_RANGE], [63:0] 

ght_tag_wdata[`GHT_BANK_RANGE], 
[1:0] 

Output 

[`GHT_DATA_RANGE], [63:0] 

ght_tag_bit_we[`GHT_BANK_RANGE], 
[1:0] 

Input 

 

idat_cm_valid 

Input 

[`IWAY_ADDR_RANGE], [1:0] 

idat_cm_way 

Input 

[`DATA_RANGE], [63:0] 

idat_cm_wdata 

Input 

 

idat_cm_write 

IFU/ifu_dat interface 
with cz_idle where the 
signals are tied off. 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               69 

Cuzco Microarchitecture Specification, v0.2 

 

 

output 

[`ADDR_ITAG_RANGE], [63:13] 

itag_cm_raddr 

Input 

IADDR_RANGE, [63:1] 

itag_cm_addr 

Input 

 

itag_cm_valid 

Input 

[`IWAY_ADDR_RANGE], [1:0] 

itag_cm_way 

Input 

Input 

 

 

itag_cm_wdata 

itag_cm_write 

IFU/ifu_tag interface 
with cz_idle where the 
signals are tied offf 

Input 

[`ITAG_DATA_RANGE], [50:0] 

itag_tag_rdata [`IC_WAY_RANGE], [3:0] 

ICache tag I/F 

output 

[`ITAG_ADDR_RANGE], [7:0] 

itag_tag_addr 

Output to the IC data 
SRAM array 

output 

[`IC_WAY_RANGE], [3:0] 

itag_tag_cs 

output 

 

itag_tag_we 

output 

[`ITAG_DATA_RANGE], [50:0] 

itag_wdata 

Input 

IADDR_RANGE, [63:1] 

itlb_cm_addr 

 

Input 

 

itlb_cm_valid 

Input 

[`ITLB_PTR_RANGE], [3:0] 

itlb_cm_way 

Input 

[`ITLB_DATA_RANGE], [56:0] 

itlb_cm_wdata 

Input 

 

itlb_cm_write 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               70 

Cuzco Microarchitecture Specification, v0.2 

 

 

Input 

[`TLB_PAGE_RANGE], [51:0] 

itlb_cm_wtag 

Input 

 

itlb_cm_wval 

Output 

[`ITLB_DATA_RANGE], [56:0] 

itlb_cm_rdata 

Output 

[`TLB_PAGE_RANGE], [51:0] 

itlb_cm_rtag 

Input 

[`IC_LINE_RANGE], [255:0] 

idat_data_rdata 

I cache data 

Output 

[`IDAT_ADDR_RANGE], [7:0] 

idat_data_addr 

Interface to Inst. Cache 
Data. O/P of ifu_dat.  

Output 

[`IC_WAY_RANGE], [3:0] 

idat_data_cs 

Output 

[`IC_LINE_RANGE], [255:0] 

idat_data_wdata 

Output 

[`IC_BANK_RANGE], [3:0] 

idat_data_we 

Output 

[`IWAY_ADDR_RANGE], [1:0] 

idat_way_sel 

active next cycle 

input 

 

l2tlb_hit 

To IFU/tlb_inst from 
tlb_l2 

input 

[`ITLB_DATA_RANGE], [56:0] 

l2tlb_idata 

 

input 

Input 

 

 

l2tlb_itlb_valid 

l2tlb_page_fault 

output 

IADDR_RANGE, [63:1] 

itlb_addr 

From IFU/tlb_inst  to 
the tlb_l2 

output 

 

itlb_miss 

 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               71 

Cuzco Microarchitecture Specification, v0.2 

 

 

 

Output 

 

ic_cm_valid 

Output 

[`DATA_RANGE], [63:0] 

icache_cm_rdata 

 

 

(Not in Frontend MAS) Interfaces for the mux generating the next instruction fetch address, branch target 
address and branch direction: 

Mux I/O 

Width 

Name 

Description 

BPU/bpu_btb/bpu_page 
interface with idle. All 
the input signals to the 
FE/BPU/.are tied off in 
the cz_idle.sv 

Input 

[`GHT_ADDR_RANGE], [7:0] 

ght_cm_addr 

Input 

Input 

 

 

ght_cm_bank 

ght_cm_valid 

Input 

[`GHT_DATA_RANGE], [63:0] 

ght_cm_wdata 

Input 

 

ght_cm_write 

Output 

[`GHT_DATA_RANGE], [63:0] 

ght_cm_rdata 

input 

[`GHT_DATA_RANGE], [63:0] 

ght_tag_rdata [`GHT_BANK_RANGE], [1:0] 

Output 

[`GHT_INDEX_RANGE], [5:0] 

ght_tag_addr[`GHT_BANK_RANGE], [1:0] 

Output 

[`GHT_CS_RANGE], [7:0] 

ght_tag_cs 

output 

[`GHT_SBK_PTR_RANGE],  
[1:0] 

ght_bank_sel[`GHT_BANK_RANGE], [1:0] 

Output 

[`GHT_DATA_RANGE], [63:0] 

ght_tag_wdata[`GHT_BANK_RANGE], [1:0] 

BPU/bpu_btb interface 
with idle. All the input 
signals to the 
FE/BPU/bpu_btb are 
tied off in the cz_idle.sv 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               72 

Cuzco Microarchitecture Specification, v0.2 

 

 

Output 

[`GHT_DATA_RANGE], [63:0] 

ght_tag_bit_we[`GHT_BANK_RANGE], [1:0] 

Input 

BTB_DATA_RANGE, [47:0] 

btb_data_wdata 

Input 

 

btb_cm_write 

Output 

BTB_DATA_RANGE, [47:0] 

btb_cm_rdata 

Output 

 

btb_cm_valout 

Input 

BTB_DATA_RANGE, [47:0] 

btb_data_rdata BTB_WAY_RANGE, [3:0] 

BTB - SRAM 

output 

[`BTB_INDEX_RANGE], [8:0] 

btb_data_addr 

output 

 

btb_data_bit_we 

output 

BTB_WAY_RANGE, [3:0] 

btb_data_cs 

output 

BTB_DATA_RANGE, [47:0] 

btb_data_wdata 

Input 

[`GHT_ADDR_RANGE], [7:0] 

ght_cm_addr 

Input 

Input 

 

 

ght_cm_bank 

ght_cm_valid 

Input 

[`GHT_DATA_RANGE], [63:0] 

ght_cm_wdata 

Input 

 

ght_cm_write 

Output 

[`GHT_DATA_RANGE], [63:0] 

ght_cm_rdata 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               73 

BPU/bpu_ght interface 
with idle. All the input 
signals to the 
FE/BPU/bpu_ght are 
tied off in the cz_idle.sv 

Cuzco Microarchitecture Specification, v0.2 

 

 

input 

[`GHT_DATA_RANGE], [63:0] 

ght_tag_rdata [`GHT_BANK_RANGE], [1:0] 

Output 

[`GHT_INDEX_RANGE], [5:0] 

ght_tag_addr[`GHT_BANK_RANGE], [1:0] 

GHT – SRAM array 
interface. 

The ght_bank_sel is 
active next cycle 

Output 

[`GHT_CS_RANGE], [7:0] 

ght_tag_cs 

output 

[`GHT_SBK_PTR_RANGE],  
[1:0] 

ght_bank_sel[`GHT_BANK_RANGE], [1:0] 

Output 

[`GHT_DATA_RANGE], [63:0] 

ght_tag_wdata[`GHT_BANK_RANGE], [1:0] 

Output 

[`GHT_DATA_RANGE], [63:0] 

ght_tag_bit_we[`GHT_BANK_RANGE], [1:0] 

IFU/ifu_dat interface 
with cz_idle where the 
signals are tied off. 

IFU/ifu_tag interface 
with cz_idle where the 
signals are tied offf 

Input 

 

idat_cm_valid 

Input 

[`IWAY_ADDR_RANGE], [1:0] 

idat_cm_way 

Input 

[`DATA_RANGE], [63:0] 

idat_cm_wdata 

Input 

 

idat_cm_write 

output 

[`ADDR_ITAG_RANGE], 
[63:13] 

itag_cm_raddr 

Input 

IADDR_RANGE, [63:1] 

itag_cm_addr 

Input 

 

itag_cm_valid 

Input 

[`IWAY_ADDR_RANGE], [1:0] 

itag_cm_way 

Input 

 

itag_cm_wdata 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               74 

Cuzco Microarchitecture Specification, v0.2 

 

 

Input 

 

itag_cm_write 

Input 

[`ITAG_DATA_RANGE], 
[50:0] 

itag_tag_rdata [`IC_WAY_RANGE], [3:0] 

ICache tag I/F 

output 

[`ITAG_ADDR_RANGE], [7:0] 

itag_tag_addr 

Output to the IC data 
SRAM array 

output 

[`IC_WAY_RANGE], [3:0] 

itag_tag_cs 

output 

 

itag_tag_we 

output 

[`ITAG_DATA_RANGE], 
[50:0] 

itag_wdata 

Input 

IADDR_RANGE, [63:1] 

itlb_cm_addr 

 

Input 

 

itlb_cm_valid 

Input 

[`ITLB_PTR_RANGE], [3:0] 

itlb_cm_way 

Input 

[`ITLB_DATA_RANGE], [56:0] 

itlb_cm_wdata 

Input 

 

itlb_cm_write 

Input 

[`TLB_PAGE_RANGE], [51:0] 

itlb_cm_wtag 

Input 

 

itlb_cm_wval 

Output 

[`ITLB_DATA_RANGE], [56:0] 

itlb_cm_rdata 

Output 

[`TLB_PAGE_RANGE], [51:0] 

itlb_cm_rtag 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               75 

Cuzco Microarchitecture Specification, v0.2 

 

 

 

Input 

[`IC_LINE_RANGE], [255:0] 

idat_data_rdata 

I cache data 

Output 

[`IDAT_ADDR_RANGE], [7:0] 

idat_data_addr 

Interface to Inst. Cache 
Data. O/P of ifu_dat.  

Output 

[`IC_WAY_RANGE], [3:0] 

idat_data_cs 

Output 

[`IC_LINE_RANGE], [255:0] 

idat_data_wdata 

Output 

[`IC_BANK_RANGE], [3:0] 

idat_data_we 

Output 

[`IWAY_ADDR_RANGE], [1:0] 

idat_way_sel 

active next cycle 

input 

 

l2tlb_hit 

To IFU/tlb_inst from 
tlb_l2 

input 

[`ITLB_DATA_RANGE], [56:0] 

l2tlb_idata 

 

input 

Input 

 

 

l2tlb_itlb_valid 

l2tlb_page_fault 

output 

IADDR_RANGE, [63:1] 

itlb_addr 

From IFU/tlb_inst  to 
the tlb_l2 

output 

Output 

 

 

itlb_miss 

ic_cm_valid 

Output 

[`DATA_RANGE], [63:0] 

icache_cm_rdata 

 

 

 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               76 

Cuzco Microarchitecture Specification, v0.2 

 

 

5 

Instruction Fetch Unit 

The IFU provides a continuous stream of instructions to the decode unit. The goal is no bubble in the 
instruction stream to the execution pipeline. The IC serializes the tag and data array accesses for simplicity 
and power reduction. The misprediction penalty is increased by 1 clock cycle but with better branch 
prediction mechanism, the performance impact should be minimal. Considerations for serializing the 
instruction fetch: 

• 

• 

Power: the hit way fetches a cache line from the data array which is ¼ the power of concurrent 
fetching of both tag and data arrays in 4-way associative IC 
Performance: since the tag array operates independently of the data array, the tag array 
accesses several cache lines ahead of the data array and a cache-miss can start fetching from 
external memory at an earlier time. This is can more than offset the performance lost by 
increasing the branch misprediction penalty 

•  Modularization: the tag array can be designed independently.  The tag and data accesses are 

independently controlled by the BPQ. Furthermore, it is simpler for the instruction miss queue 
(IMQ) to interface with the instruction tag queue (ITQ) to validate an external fetch 

Other features of the IFU are: 

•  Out-of-Order external memory fetch 
• 

Predecode of 16/32-bit instruction format 

Functionality of IFU pipeline stages: 

• 

• 

• 

• 

IF0: the start address from the BPQ is sent to the iTLB and the iTLB sends the physical address to 
the ITAG where the request is sent to the tag SRAM array 
IF1: the tag SRAM array returns data and the address is compared for tag hit or miss 
determination. The tag-miss is sent to the IMQ.  The tag-hit and hit way are sent to the data 
SRAM array; bypass-on-empty is implemented. Tag data are written into an entry in the ITQ.  The 
next sequential address is calculated and accesses the iTLB if it crosses the 4KB page boundary, 
the physical address is sent to the tag SRAM array.  The next sequential address can be replaced 
by the next start address from the BPU (last_line is received from BPQ).  
IF2: the data SRAM array returns cache line data to the IDAT.  The whole cache line is flopped 
before writing into the ICQ in the next cycle. The “bypass” signal from BPQ is used to select  data 
from a BTB supplied address instead of one from the BPQ.  The start and end addresses and 
branch prediction information are flopped.  The BPQ pointer and 16/32-bit instruction decoded 
bits are attached to all instructions of the cache line.  The last valid instruction of the basic block 
is also marked in the ICQ. 
IF3: The cache line must be validated by the BTB or BPQ before writing valid bytes into the ICQ 
and sending those to the decode unit; bypass-on-empty is implemented.  The masks are 
generated to write only the valid bytes of the cache lines into the ICQ and are forwarded to the 
XIQ for bypass-on-empty.  The instructions sent to XIQ are either from the ICQ or the fetched 
cache line.  

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               77 

Cuzco Microarchitecture Specification, v0.2 

 

 

5.1  Tag SRAM Array and Instruction Tag Queue (ITQ) 

An address width of 64-bits is used in this specification but could be 32-bits. All the tag/BTB/TLB arrays 
use the same 64-bit address configuration. The tag SRAM array is 1K cache lines organized as 4 banks of 
256x20; 1 bank per way of associativity. The number of SRAM columns is changed for different IC capacity 
configurations. Studies are necessary to optimize the number of banks for power, timing, and area: 

Notes:  

• 

The organization of instruction cache (IC) may be the same as data cache (DC) for design 
simplicity 

•  An extra mode bit may be added for user/supervisor mode – e.g., for a security purpose 

The valid array is implemented using register cells, 4 sets of 256 valid bits. The valid array is reset in 8 
cycles, 32 bits per set are reset in each cycle. This implementation is used for all the SRAM arrays: IC, DC, 
BTB, L2TLB and is not discussed in the other SRAM arrays. If the valid bits are implemented in the tag 
SRAM array, then 256 cycles (number of rows of SRAM banks) are needed to reset valid bits in SRAM 
arrays. 

The current assumption is that the address to the IC is the physical address where translation was done in 
the IF0 cycle, thus there is no aliasing. The tag SRAM data is returned in the IF1 stage: 

• 

• 

Tag hit: bypass-on-empty (ITQ empty) accesses the data SRAM array as long as the BPQ indicates 
valid branch prediction  
Tag miss: immediately sends to the IMQ as long as the IMQ is not full. The IMQ has the same 
number of entries as the ITQ, but the ITQ can be flushed by branch misprediction while the IMQ 
entries must remain to receive data which will be later invalidated 

Note that the tag array continues to process next incremented cache lines when the BTB takes 2 cycles to 
make its prediction. The BTB sends same_cache_line[0] to invalidate the next incremented cache line.  All 
cache lines are kept in the ITQ.  The ITQ is tightly coupled to the IDAT, the IMQ, and the BPQ.  

Coupling to the IDAT: the entry in the ITQ is removed in-order once acknowledge by the IDAT 

Coupling to the BPQ/BTB: 

• 

If the bypass signal is asserted by the BPQ, then the start address is validated unless it is 
invalidated by another branch mispredition or execution redirection 

o  The next incremented address is validated by the BTB in absence of the 

same_cache_line[0] signal.  This is the only time the ITAG fetches a speculative address. 

•  All other addresses are initiated by BPQ valid entries 
• 

Taking into account of iTLB miss, IC miss, and delay of IDAT to acknowledge the next cache line, 
the state machine as shown in the Figure below is used to indicate when the ITAG should read 
the tag arrays 

Coupling to the IMQ: 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               78 

Cuzco Microarchitecture Specification, v0.2 

 

 

• 

• 

The IMQ sends the tag-miss request to external memory immediately with a valid/accept 
protocol. The tag-miss entries in the ITQ can be invalidated by the BPQ or branch misprediction 
The external fetched data can return out-of-order: 

o 

o 

o 

If the requested cache line is no longer in the ITQ, then the externally fetched data are 
ignored and the IMQ entry is invalidated without writing to the IC, else the cache line is 
written into the tag and data SRAM arrays with highest priority 
If the requested cache line is the first entry in the ITQ, then the externally fetched data 
are sent to IDAT and the fetched cache line remains valid until it is acknowledged 
If the requested cache line is not the first entry in the ITQ, then the ITQ entry is modified 
as cache hit and the hit way are set to the written way. This can happen because the 
external requests are out-of-order and the misprediction can loop back to an earlier 
address. 

 

Figure 5-1 State Machine for Reading of the Tag Arrays 

The valid/accept protocol is used with the data SRAM array for sending valid addresses. The ITQ has 4 
entries each containing: 

Name 

Valid 

Address[63:5] 

Tag hit/miss 

Size  

Description  

1 

59 

1 

Valid entry, flush on branch misprediction or execution redirection, invalidate when 
acknowledge by the IDAT, for cache miss the IMQ send data to IDAT for acknowledge 

Cache line address 

Tag hit/miss 

Tag hit way 
Table 5-1. ITQ Entry Bit Fields 

2 

For 4-way associative 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               79 

single cyclebpq_btb_valid & itlb_hitbpq_btb_valid & ~itlb_hit    bpq_tag_bypass bpq_tag_bypass    bpq_btb_valid & btb_validitlb_hitwait0& btb_line_same[0]itag0assert until ack& itlb_hitbpq_tag_bypass & btb_valid     bpq_tag_valid & btb_line_same[0] & ~itlb_hit(~bpq_tag_bypass & ~bpq_tag_valid)bpq_tag_bypass(~bpq_tag_bypass& itlb_hit& btb_valid| bpq_tag_valid)& btb_line_same& ~itlb_hititq_full& itlb_hitbpq_tag_bypass & btb_valid itag1wait1& btb_line_same & ~itlb_hit~bpq_tag_bypass & ~bpq_tag_valid) & itlb_hit(bpq_tag_valid | (bpq_tag_bypass & ~(| btb_line_same))) & itlb_hitbpq_tag_valid & ~itlb_hitbpq_tag_valid & itlb_hititq_fullitag2bpq_tag_valid & itlb_hitwait2bpq_tag_valid& itlb_hitbpq_tag_valid & ~itlb_hit~bpq_tag_valid | itq_fullIdlebpq_tag_valid & ~itlb_hitCuzco Microarchitecture Specification, v0.2 

 

 

The first entry in the ITQ is read and remains until acknowledged by the IDAT.  For a miss request, the IMQ 
compares the address to this first entry address to send data to the IDAT where the acknowledge 
invalidates the ITQ entry and increments the read pointer. 

5.2  Instruction Miss Queue (IMQ) and Pre-fetch Buffer 

For simplicity, the instruction bus width is 32B (cache line size) and the pre-fetch buffer is a single cache 
line. The cache line has the highest priority to write to the IC, so that the next fetch data can be accepted 
without much delay. 

The IMQ keeps track of all external instruction requests. IMQ consists of 4 entries with the following bit 
fields: 

Name 

Valid 

AXI Valid 

AXI ID 

Size  

Description  

1 

1 

Valid entry 

Set once the request is accepted with valid AXI ID 

4-5 

AXI ID for out-of-order requests 

Address[64:5] 
Table 5-2. IMQ Bit Fields 

59 

Cache line address 

The IMQ has 3 pointers: 

•  Write pointer: to write tag-miss request from the ITQ 
•  Request pointer: to send request valid to external memory, which must be accepted before the 

pointer can be incremented 

•  Read pointer: increments when external data is received, written to IC, and acknowledged by 

IDAT if the IMQ address matches with the first entry address of ITQ 

The pre-fetch buffer has a single entry. When external data is received: 

• 

• 

• 

Cycle 1: the AXI ID is compared to all IMQ entries and the address from the matching entry is 
read the  for comparison to the first entry address of the ITQ  
Cycle 2: request IC valid array and LRU arrays to select a way to write into the IC, if it is first entry 
in the ITQ, then the fetched data is also sent to the ICQ with a valid/acknowledge protocol 
Cycle 3: valid, tag, and data are written into the IC. The entry is invalidated if the fetched data is 
acknowledged by the ICQ 

Implementation note: all prefetched data is written into the cache regardless of whether it is invalidated 
in the ITQ. Study is needed to see if it should instead be invalidated and not written into the IC. 

5.2.1  Alternate Implementation: Instruction Miss Queue (IMQ) and 

Prefetch Buffer (PFB) 

Following is an alternate implementation of IMQ and PFB. 

• 

IMQ entry description is as above in Table 5-2, with an additional bit indicating “demand” or PF. 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               80 

Cuzco Microarchitecture Specification, v0.2 

 

 

 

•  Prefetch Buffer entry has the same description as the IMQ entry. A prefetch hit turns the 

“demand” bit to be set. 

•  The number of combined IMQ/PFB entries is 4 (default), subject to performance studies. 
•  An incoming demand supplies data (instructions) to the waiting IFetch pipe, While writing into 

ICache. 

•  A prefetch hit gets written into ICache on  

o 
o 
o 

a branch mispredict, or  
another ICache miss, or 
a BTB miss (filled by L2BTB or branch execution) 

5.2.2 

BPB (Branch Page Buffer) 

Branch Page Buffer has extra entry for physical page, with following purpose: 

•  BPB read providing physical address avoids the need for ITLB (effectively becomes ITLB), thus 

an ICache Miss is sent down the memory hierarchy at the earliest. 

•  While the cost is an area increase of BPB, the impact is performance increase to send the ICache 

miss earlier. 

5.3  Data SRAM Array and Instruction Cache Queue (ICQ) 

The data SRAM array is 1K cache lines of 32B organized as 16 banks of 256x64; 4 banks per each 
associativity way. The number of SRAM banks is changed for IC size configuration.  For simplicity, the 
whole cache line is fetched at one time. Note that it is optional that the data SRAM array can be blocking 
2-cycle fetched. Low-power and high-density SRAM cells can be used for the data SRAM array. The 
valid/accept protocol to the ITQ would include bank conflict detection and resolution for multi-cycle 
SRAM access.  

The ICQ receives the cache line from either the IC data SRAM array or the pre-fetch buffer. The 
assumption is that the data from SRAM data array or pre-fetch buffer remain valid until accepted by the 
ICQ. The ICQ size is 1 cache line, (256-bits) organized as 16 entries of 16-bits with write and read pointers. 
Writing into the ICQ is 256-bits write data with a mask. The write pointer is incremented based on the 
start and end address from the BTB or BPQ.  Reading is always 128-bit or 8 entries of 16-bits for 4 
instructions. Four instructions are sent to the XIQ every clock cycle unless the XIQ is full and accepts fewer 
instructions.   

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               81 

Cuzco Microarchitecture Specification, v0.2 

 

 

Figure 5-2 Writing Cache Line Data into ICQ 

Two LSBs of every 16-bits of the fetched cache line are predecoded for 16/32-bit instruction format 
determination. The predecoded bits are aligned and shifted in the same way as the mask and data to 
write to the ICQ. A cache line has 16 predecoded bits, the timing for writing into the ICQ should be faster.  
In addition, the BPQ pointer and the last 16-bits of a basic block must be marked.  Each entry of the ICQ 
consists of: 

 

Name 

Valid 

Branch valid 

16/32-bit inst 

Size  

Description  

1 

1 

1 

Valid entry 

Set by the end address of the predicted basic block 

0: 16-bit instruction, 1: 32-bit instruction, predecode in IF3 stage 

BPQ pointer 

4-5 

Every entry of the cache line has the same BPQ pointer 

Data 
Table 5-3. ICQ Bit Fields 

16 

16-bit instruction, 32-bit instruction takes 2 consecutive entries 

The read operation is simpler where the ICQ is rotated right by the read pointer to read 8 entries for 4 
instructions. All the other bit fields are extracted along with the instructions to send to XIQ 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               82 

1514131211109876543210IC Registernext_addrstart_addr0000001111111000mask1514131211109876543210data0000001111111mask1514131211109876543data 1111110000000000write mask1514131211109876543210ICQwrptrwrptr=wrptr+(next_addr-start_addr)ICQ write logicShift (rotate) left by write pointer for writing to ICQIC Data ArrayInst Prefetcth Buffermask is generated from start and end addressesmuxCuzco Microarchitecture Specification, v0.2 

 

 

Figure 5-3 Reading of 4 Instructions (Up to 8 Entries) from ICQ 

The loop prediction is more natural with basic block branch prediction where the target address is the 
same as the start address. Fetching of a small loop is implemented in the ICQ. A small loop is defined one 
in which the number of instructions in the loop is equal to or less than the ICQ size.  Procedures to track 
and issue instructions from the loop in ICQ are illustrated in the Figure below: 

 

•  With a predicted loop from BTB/BPQ: the start address sets the loop-start pointer and the end 

address sets the loop-end pointer.  The loop count is also captured. 

•  When reading of instructions from the ICQ crosses the loop-start pointer, then the loop valid 

signal is set. 

•  A mask of loop valid bits is created from the loop-start pointer and loop-end pointers.  The ICQ 

entry data is modified to be the concatenation of 2 masks of loop valid bits.   

•  When the read pointer goes past the loop-end pointer, it goes to the second set of loop valid 

• 

bits. The read pointer is then converted to point back to an entry between the loop-start and end 
pointer.  
Every time the read pointer goes past the loop-end pointer, the loop count is decremented by 1. 
The implementation is limited to only 2 iterations of the loop instructions, the read pointer is 
pointed to the loop-start pointer.  The read pointer goes past 2 loop-end pointers in this case. 

•  After writing the loop basic block into the ICQ, the next basic block is written into the ICQ when 
the entries after the loop-end pointer are invalid.  The next basic block is written until the loop-
start pointer in the ICQ.  The loop instructions in the ICQ remain valid until the loop count is zero 
at which time the loop valid signal is also reset. 

•  When the loop valid is reset, the ICQ entries go back to normal behavior where the instructions 
of the last iteration are normal instructions in the ICQ and read pointer can issue the next basic 
block instructions with the loop instructions. 

 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               83 

1514131211109876543210ICQ321031302928rdptr=rdptr+(ival0*ilen0+ival1*ilen1+ival2*ilen2+ival3*ilen3)Inst to XIQICQ Read LogicShift (rotate) right by read pointer to read 8 entries from ICQCuzco Microarchitecture Specification, v0.2 

 

 

 

Figure 5-4 ICQ Loop Implementation 

5.4  L2 TLB  

Note: Need to add table walk caching to support high performance hypervisor operation. This would 
imply that the L2TLB entries will be of 4 types: 4KB, 2MB, or 1GB pages, and hypervisor walk. 

Note: Initially the L2TLB can be a SW managed cache with the table walk done by supervisor and 
hypervisor software. Need to check if RISCV Linux has SW table walk support. Multiple concurrent 
hardware tablewalks are a goal, so both the table walker function and the amount of outstanding 
tablewalks need to be determined. 

The L2 TLB SRAM array is 1K entries organized as 4 banks of 256x99; 1 bank per way of associativity. Each 
entry consists of a 44-bit virtual tag, 3-bits of memory attributes, and a 52-bit physical address. The 
number of SRAM columns is changed for different L2 TLB configuration sizes.  

The valid array is implemented using register cells, 4 sets of 256 valid bits. The valid array is reset in 8 
cycles, 32 bits per set are reset in each cycle (see 5.1). The L2 TLB is looked up only when there is L1 TLB 
miss from iTLB or dTLB: 

•  Virtual address[63:12] for translation to physical address[63:12] 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               84 

Vali32BPBPQInst000xx000xx1I01004I01I01004I01I11004I11I11004I11I21004I21I21004I21I31004I31I31004I31I41004I41I41014I41I01005I51I01005I51I11005I61I11005I61I21I21I3Loop count51I31I41I4Start pointerEnd pointerread pointerICQView by issuing instructionsread pointerread pointer32-bitinstructionIncrementdecrementif the BP is validin the issue instructionsrdptr= tmp_rdptr -endptr + startptrCuzco Microarchitecture Specification, v0.2 

 

 

Figure 5-5 TLB Block Diagram 

5.4.1  RISC-V Architecture Notes 

 

•  Memory addresses are circular. Addresses 0xFFFF… and 0x0000… are adjacent. 
• 

• 

There are 32 Physical Memory Attributes (PMA). 
Per-VM page specification of both weakly and strongly ordered uncacheable attributes are under 
development for use instead of the hardware PMA. 
The most common offsets are +/-11bits from a GPR or +/- 21bits from PC. 

• 
•  Based on page table walking, 2MB megapage & 1 GB gigapage TLB entries should cacheable in the 

L2TLB. The L1 TLBs need not translate pages or subpages of these larger pages larger than 4KB. 

5.4.1.1 

Page-Table & Address Translation 

• 

• 

Page tables are implemented as trees of naturally aligned 4KB pages. The intermediate-level PTE and 
leaf PTE formats are identical and thus a table walk may terminate at an intermediate level mapping a 
corresponding to 2MB “mega” VM page or 1GB “giga” VM page. The default (leaf) VM page size is 
4KB. The top level of a hypervisor page table is four contiguous 4KB page naturally aligned to a 16KB 
enabling the hypervisor to map 4 times more physical address space than a supervisor. 
Page table walks and TLB caches thereof may be speculative and may write the PTE Accessed bit (A), 
but must not write the PTE Dirty (D) bit.  

•  RV32 has one translation mode which uses a 2-level page table (PT) with 32bit page table leaf entries 

(PTE) to map 32bit virtual addresses (VA) into 34bit physical addresses (PA). 

•  RV64 has 3 translation modes which use 3, 4, or 5 level page tables of 64bit PTEs to translate either 

39, 48, or 57 VA bits respectively all into 56 PA bits. An RV64 VA must always be sign-extended to 
64bits. 
Page table updates and TLB shootdowns are accomplished with the SFENCE.VMA supervisor 
instruction. 

• 

5.5  Instruction TLB 

The instruction TLB has 16 entries which are fully associative, implemented with register cells for timing 
and translates virtual address[63:12] to physical address[63:12]. 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               85 

256254.Address[19:12]..10Address[63:20]Hit way is used for BPBVirtual AddressVirtual_tag[63:20]L2 TLBWay 3Valid BitsInstruction/DataL2 TLBWay 0Virtual AddressVirtual_tag[63:20]Valid BitsInstruction/DataVirtual AddressVirtual_tag[63:20]L2 TLBWay 1Valid BitsInstruction/DataVirtual AddressVirtual_tag[63:20]L2 TLB = 1024-entry, 4-way associativeL2 TLBWay 0Physical AddressPhysical_tag[63:12]L2 TLBWay 1L2 TLBWay 2L2 TLBWay 3Physical AddressPhysical_tag[63:12]Physical AddressPhysical_tag[63:12]Physical AddressPhysical_tag[63:12]Phyiscal Address[63:12]L2 TLBWay 2Valid BitsInstruction/Data(for Icache)=?Hit=?=?=?Cuzco Microarchitecture Specification, v0.2 

 

 

Figure 5-6 Instruction TLB Block Diagram 

Each instruction TLB entry consists 3-bit for valid and memory attributes, the 52-bit physical address, and 
the L2TLB way for use by the BPB. The iTLB accesses are similar to the L2 TLB: 

 

The iTLB is accessed in the IFU only if the PC crosses the page boundary: 

•  BPU does not access iTLB on BTB miss 
•  BTB has a bit to indicate if the target or next address crosses a page boundary 
•  RS has a bit to indicate if the return address crosses a page boundary 

Special consideration for the L1 TLB: 

Aliasing complexity of L1 cache: the index to the physical-address cache data/tag array is not known until 
after translation. For example, with 32KB and 4-way associative, the index to the cache arrays is [12:5] 
where bit 12 of the address is from TLB translation with the TLB page = 4KB (bits [11:0]). 

To solve this problem, the L1 TLB can be accessed in the cycle before tag array access.  For the data cache, 
the AGU is in the same cycle with the dTLB.  If the TLB is small enough, then the dTLB can be accessed 
after the address calculation in the same cycle.  For RISC-V ISA, the immediate field for the load/store 
instruction is 12-bit, the procedure to speed up the DTLB access is: 

• 

• 

For 16-entry L1 TLB and fully associative: the index address to access the TLB array is 4-bits 
[15:12] 
For 64-entry L1 TLB and 4-way associative: the index address to access the TLB array is 4 bits 
[15:12].  For larger TLB sizes, the way associativity can be increased to keep the same index 
address 

•  Virtual address [15:12] and address+1[15:12] are used to access the TLB.  The address+1[63:16] is 

calculated to compare the TLB virtual tag for hit/miss 

The carry-out of calculated address [11:0] can be used to select the TLB physical address 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               86 

Inst_Addr[63:12](for Icache)iTLB = 16-entry iTLB, fully-associativePhysical Address[63:12]Virtual_tag[63:12]Physical_tag[63:12]iTLBl2TLB_wayBway[1:0]Bway[1:0](for BPB)iTLBPhysical AddressiTLBVirtual AddressValid Bits=?=?=?=?=?=?=?HitCuzco Microarchitecture Specification, v0.2 

 

 

6 

Instruction Decode/Issue Unit (IDU) 

The decode and issue unit covers 3 pipeline stages in the Cuzco microprocessor.  Most of the novelties of 
the Cuzco microprocessor are in this chapter where the mechanisms are described to “plan ahead” 
instruction issue and execution by various functional units based on the instruction latency and 
throughput times.  A time count is continuously incremented every clock cycle.  For a time count of 64, 
the size of the time count is 6 bits. With 4-wide instruction issue, the maximum number of instructions in 
the pipeline is 256 (i.e., 64*4).  All instructions are issued based on this time count.  The time and 
conditions for execution of instructions are described in Error! Reference source not found.. The times 
are recorded in the RSB, TRM, read/write port control, and execution queues.  The time count provides 
the view of all previous instructions which have been scheduled for execution.   

Definitions of the various times used in this specification: 

• 

• 

Latency time – number of cycles for the functional unit to produce result data, e.g., 1 cycle for an 
ALU instruction 
Throughput time (xput time) – the number of cycles the functional unit is blocked from allowing 
the next instruction from entering the functional unit. Most instructions are pipelined with 1 
cycle throughput time with exception of divide instruction which can keep the divide functional 
unit busy for many cycles  

•  Read time – the earliest time that the source operands can be read from the PRF or forwarded 

• 

from the functional units that produce them 
Execute time – 1 cycle after the read time, that is when an instruction starts execution in its 
functional unit(s) 

•  Write time – when the result data is ready for writing back to the PRF from functional unit(s).  
The write time is the addition of the latency time to the read time. The result data are written 
back to the PRF immediately to avoid an extra set of registers in the functional unit(s) to buffer 
result data 

The RSB records the write time of the renamed destination register of an instruction.  For example, if an 
instruction writes to a destination register, R5, at time 27, then time 27 is set for R5 in the RSB.  If the 
source register R5 is used by a subsequent instruction, then the subsequent instruction reads the RSB and 
the earliest time to read R5 is 27.  The execution and write times are calculated from the read time and 
are used to access the TRM for any conflict.  If there is no conflict, then the subsequent dependant 
instruction is scheduled to be issued from its execution queue at time 27.  In order to reduce the chance 
of a conflict, the read time+1 may be used.  If there is a conflict at read time 27, then the instruction may 
be scheduled to be issued from the execution queue at time 28. 

Operations in each of the decode pipeline stages: 

1. 

ID0: instructions are decoded in 2 different stages, the first decode in this stage is for 
source/destination operands, latency and throughput times, and instruction type.  The excel 
decode table is used to define and configure instruction decoding.  The latency, throughput, and 
instruction type are localparameter in the define.vh file which can be modified without 
recompilation of instruction decode from the excel table.   

• 

Source register, x0, is converted into 0 immediate data and destination register, x0, is 
converted to no write back 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               87 

Cuzco Microarchitecture Specification, v0.2 

 

 

• 

Four or more instructions are issued in a clock cycle, so the RAW dependencies (intra 
dependencies) between the issued instructions are detected, used in the register 
renaming, and sent to the next stage 

•  Destination registers are renamed from the register free list (RFL) 

• 

• 

Source registers are renamed from the register alias table (RAT) or muxed from the RFL 
if there is a RAW dependency on a destination register within the decode group 

The compressed 16-bit instructions (RISC-V C extension) are decoded separately from 
the 32-bit instructions so that it can be easily removed 

2. 

ID1: The instructions are written into the ROB in this cycle. The ROB has no timing information, 
its functions are strictly to receive the instruction completion information, write to ART/RFL, and 
restore the RAT on branch misprediction or exception.  The time count module is started in this 
cycle.  The register scoreboard is based on the PRF.  The RSB is accessed to calculate the read, 
execute, and write times for the decode instructions.  A second set of read, execute, and write 
times based on read_time+1 is also calculated to allow flexibility in resolving resource conflicts.  
Instruction intra-dependencies are taken into account on top of the read times from the RSB. 

•  Note that the RSB is not updated with the new write time until the next stage.  The 
write times are forwarded to the read times for instructions in this stage.  The read 
register addresses are compared to the destination register addresses in the ID2 stage 
and bypassed to provide the read times in this cycle.  

3. 

ID2: the issue decisions are in this stage. The time-resource matrix is used to resolve resource 
conflictsincluding read ports, write ports, and functional units. The TRM indicates if resources are 
available when required by the read, execute, and write times of the instructions. The TRM 
assigns specific resources to each instruction and blocks future instructions from using the 
resources at the allotted times.  The instructions are further decoded for control bits for 
functional units in this stage. The excel decode table is again used along with localparameter in 
the define.vh file to configure this stage.  The write times of valid destination registers are 
written to the RSB.  Issued instructions are written to a selected XEQ.   

Since the number of entries in the pipeline queue is the same as the number of issue instructions, having 
many pipeline stages increases the problem of stalling and back pressure if one instruction cannot be 
issued.  The scalar issue queue (XIQ) which has 2X the number of entries as the number of instructions 
issuable per cycle decouples the IDU from the IFU. Bypassable queues between the decode stages serve a 
similar purpose.    

Figure 6-1 IDU Pipeline 

 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               88 

IDOID1ID2EX0Time CountPhysical Register FileRead Control UnitRegister ScoreboardArchitectural Register TableRegister Free ListRegister Alias TableRe-Order BufferIssue LogicResource MatrixXIQICQD0QD1QXEQCuzco Microarchitecture Specification, v0.2 

 

 

Issued instructions are “fire-and-forget” in that they are preset for execution at certain time in the future.  
The load/store instruction latency time is based on the L1 data cache hit latency which may not be when 
the data actually ready due to a cache or TLB miss or other reasons.  Two mechanism for replaying 
load/store instructions are implemented: (1) reissue the load/store instruction from the XEQ and (2) the 
tag array access is already completed, but the load instruction is waiting for valid data.  In both cases, the 
RSB is updated with a new destination register write time. 

The load/store instruction can be replayed from start: 

• 

TLB miss: the TLB is updated by reading the L2 TLB. If there is a L2 TLB hit the L2 TLB latency is 
used to calculate thenew write time. If there is an L2 TLB miss a HW table walk is performed. 

•  DC tag ECC 1-bit error: the tag array is corrected 
• 

Partial hit with previous store (narrow to wide): the load instruction will be replayed at the retire 
stage by ROB stage (not yet implemented) 

The load/store instruction is only waiting for valid data: 

•  DC miss: the L2 cache latency is speculatively used to calculate the new write time 
•  DC data bank conflict: the bank delay time is used to calculate the new write time 
•  DC data ECC 1-bit error: the data array is updated, and the new write time is 1 cycle more than 

the original 

Modular design: the critical timing path is in the TRM logic where the selection of read/execution/write 
times are accumulative from first instruction to the next instruction to the next instruction.  Breaking up 
the issue instructions into single or dual instruction slices simplifies the issue logic.  The simplification 
derives from reducing the routing of read and write buses for 1 or 2 instructions at the expense of having 
more functional units. This tradeoff should match nanometer scale technology where the routing is 
expensive and transistors are cheap.  The worst-case timing is the issue signal from the first instruction to 
the last instruction because instructions are issued in-order. 

 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               89 

Cuzco Microarchitecture Specification, v0.2 

 

 

 

Figure 6-2 Modular Design of Single and Dual Instruction Modules 

6.1  Instruction Serialization 

Moves between the PRF and a CSR 

• 

• 

The write to CSR is a serialized event because the processor state (i.e., write to PMA/PMP, 
rounding mode) can be modified. All previous instructions must be completed prior to the CSR 
write. 
The read from CSR is a serialized event as previous instructions can update the status (i.e., sticky 
bit error, timer) 

•  ROB empty is indication of completion of all previous instructions for serialization 

SWI and Frontend exception handling 

• 

• 

SWI includes SCALL, SBREAK, ECALL, EBREAK, ERET 
Front-end exceptions include all instruction fetch errors and illegal instruction 

•  ROB empty is indication of completion of all previous instructions for serialization  

FENCE instruction handling 

• 

FENCE.I is synchronization of instruction and data 

FENCE.I is a serialized instruction as with move to/from CSR 

o 
o  All prior operations must be completed as with move to/from CSR 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               90 

Read Port CtrlWrite Port Ctrlrs1TRM ReadXEQ - ALU (8)Read LogicALUrs2TRM ReadwrTRM WriteXEQ - MUL (2)Read LogicMULTRM Funit (ALU)FunitTRM Funit (BRN)XEQ - DIV (2)Read LogicDIVTRM Funit (MUL)TRM Funit (DIV)XEQ - BRN (4)Read LogicBRNTRM Funit (LS)XEQ - LS PORT (8)Read LogicLS-PORTALUrs1TRM ReadXEQ - ALU (16)Read LogicALUrs2TRM Readrs1TRM ReadXEQ - MUL (4)Read LogicMULrs2TRM ReadwrTRM WriteXEQ - DIV (4)Read LogicDIVwrTRM WriteTRM Funit (ALU)XEQ - BRN (8)Read LogicBRNTRM Funit (ALU)FunitTRM Funit (BRN)XEQ - LS PORT (16)Read LogicLS-PORTFunitTRM Funit (MUL)TRM Funit (DIV)TRM Funit (LS)Physical REGFIssue Logic (dual insts)Issue Logic (single inst)Twoload/store portsreplayreplay2single instruction -use for odd issued instructionsdual instructions -use for every 2 issued instructionsCuzco Microarchitecture Specification, v0.2 

 

 

o  Execution of the FENCE.I instruction flushes all instructions after the FENCE.I and 

fetching is restarted afterward 

• 

FENCE is data synchronization 

o 

o 

FENCE instruction is stalled in decode until both the LS scoreboard and the load miss 
queue are empty 
It is possible to issue the FENCE instruction to the TEQ which will cause TEQ full to be 
asserted. The load-miss queue must be empty for TEQ full to be deasserted 

• 

Treated as load/store DC miss 

Handling of the Atomic load/store instructions 

• 

Treated as load/store DC miss 

Uncacheable loads or stores are treated as DC misses. 

Handling of Custom instructions with 2 micro-ops: 

• 

• 

The store instruction is considered 2 micro-ops, the address calculation uop and the store data 
uop to write into the DC through the STB. Arithmetic atomic instructions are issued to both the 
load/store XEQ and arithmetic XEQ.  Decode identifies both which resource will fetch data and 
where to write the result data back.  If 2 destinations are needed, then the next instruction is 
invalid to allow the instruction to use 2 registers from RFL and RAT.  Similarly, the RSB would 
require 2 write times to for a single instruction.  Both micro-ops are issued from the same 
instruction position to the XEQs 
The CSR instruction has many different operations – in initial implementation, no swap operation 
is implemented and the operation is serialized: 

o  Nop: RS1=0, and RD=0 
o  Read from CSR: RS1=0, the CSR is read and its contents are written to RD, a move 

instruction 

o  Write to CSR: RD=0, RS1 is read and written to the CSR, a move instruction 
o 

Set bits to CSR (CSRRS):  RD=0, RS1 is read, CSR is read, a logical OR (RS1 | CSR) 
instruction 

o  Clear bits to CSR (CSRRS):  RD=0, RS1 is read, CSR is read, a logical AND, (!RS1 & CSR) 

o 

o 

instruction 
Swap operation: in the last 3 cases, if RD is not zero, then a swap operation is required.  
The second ALU is used to move data from the CSR to the RD.  The second ALU is thus 
always a move instruction.  The source operand CSR is read for both ALUs, the read bus 
selection should be the same.   
For CSRRWI, CSRRSI, and CSRRCI, immediate data (zero extended) is used instead of RS1.  
The imm[4:0]=0 is treated the same as RS1=0 where the instruction is Read from CSR. 

o  The read/write port control unit has an added bit to read and write to the CSR instead of 

the PRF 

o  The number of CSRs is limited to the number of physical registers, for example, if the 

PRF is 128 registers, then the maximum number of CSRs is also 128. 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               91 

Cuzco Microarchitecture Specification, v0.2 

 

 

6.2  Physical Register File (PRF) 

The PRF includes all the architectural and rename registers.  The PRF may include multiple architectural 
register files (ARF) as long as the width of the ARF is equal or smaller than the XRF.   For example, the XRF 
is 64-bit, the FRF can be single-precision (32-bit), the floating-point instructions use the lower half of the 
register files.  

The number of read and write ports is dependent on the number of issued instructions. In general, the 
PRF reads and forwards to the data paths of the functional units as shown in Figure 5 3.  From ID2, all 
register references are based on PRF and not individual ARF. 

 

Figure 6-3 Read and Write Data Paths from PRF 

The register read and result forwarding are scheduled ahead of time, so all the executions are at preset 
times in the future.  The issue logic checks for availability of the read and write ports at specific times in 
the future to read source operand data from the PRF and to write result data back to the PRF, 
respectively.  If the data is not yet projected to be in the in the PRF, then read port control forwards result 
data.  The scheduled instruction in the XEQ is synchronized with the read and write port control units 
where source operand data are provided and result data are selected to write back to the PRF at 
precalculated times. 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               92 

Forwardimmimmrs1_selectrs1_selectPRFrs1 sourceExecution Unit 1Execution Unit 0rs1 sourcewr2-3wr2-3rd0-3Read-port controlXEQwr0-1Write-port controlrd4-7wr0-1XEQCuzco Microarchitecture Specification, v0.2 

 

 

Unfortunately, some instructions do not have fixed latency times such as load and store instructions. The 
load instruction is very important for performance. The processor is often stalled waiting for load data 
especially due to a  load DC miss. The load/store instructions are issued with the expected data cache hit 
time (a known latency which on average is about 80% of dynamic loads) using the same issue algorithm as 
used for ALU instructions.  The RSB is updated with a new write time for the DC miss load destination 
register.  All instructions read the RSB again to confirm that the write times they depend on have not 
been changed.  If the time is changed, then the read time and read time +1 goes through the TRM to 
confirm no conflict.  The instruction remains in the XEQ with the new read time.  In hopefully rare cases, if 
there is conflict, then the “unknown time” bit is set in the XEQ which will be executed in-order by the 
ROB. 

6.3  Register Renaming and Re-Order Buffer (ROB) 

Register renaming is necessary for OOO execution and especially when speculating beyond branch 
instructions. 

•  Allowing branches to be executed out-of-order 
•  WAW and WAR architectural register name hazards are removed. Since all destination registers 

are renamed, a renamed destination register can be written with wrong data as long as the 
completion of subsequent dependent instructions are delayed until valid data is written to the 
destination register. 

Register renaming is implemented with several basic structures: register free list (RFL), register alias table 
(RAT), the architecture register table (ART), the physical register file (PRF), and re-order buffer (ROB).  One 
important function of the ROB is to restore the RFL and RAT for canceled instructions whenever a branch 
is mispredicted.  

Issued instructions have “fire-and-forget” execution.  Each issued instruction will be completed regardless 
of prior branch misprediction, contemporaneous exception, or interrupt.  The ROB is responsible to cancel 
all the instructions in the wrong code execution path.  It is possible to take an interrupt without waiting 
for completion of earlier instructions, and instructions may continue to execute concurrent with interrupt 
subroutine instructions.   

On a branch misprediction, all instructions in the ID0 stage and earlier are invalidated.  The instructions in 
ID1 stage are still written to the ROB.  The ROB pointer is attached to all branch instructions where the 
BEU sends the ROB pointer to the ROB in EX0 stage.  The ROB blocks cancelled branch instructions from 
asserting branch misprediction by sending a “canceled” signal to the BEU in the EX1 stage.   

Interrupt procedure: 

• 

Flushed all instructions from ID0 stage 

•  Wait for completion of load/store miss in order to ensure that there is no imprecise exception 
•  Wait for all branches to be executed (i.e., XEQ(s) empty) to avoid cancelation of the interrupt 

routine in the ROB in the case of a branch misprediction 

•  When the interrupt routine is fetched, all earlier instructions in ROB are to retire as normal, 

without cancellation 

Instruction exception handling depends on their type: 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               93 

Cuzco Microarchitecture Specification, v0.2 

 

 

•  An instruction exception should not issue any additional instructions, thus there is nothing in 

ROB to cancel 

•  A data exception is handled like a branch misprediction, the ID0 stage should be flushed while all 

instructions after the exception are cancelled 

•  An imprecise exception is treated like an interrupt where all instructions from the ID0 stage are 

flushed and handling should wait for completion of branches and pending external load and 
store requests 

Note: RISC-V has a major departure from the X86 and ARM ISA in external data memory fetch.  RISC-V can 
retire the load or store entry and treats an external memory error as an imprecise exception where the 
other ISAs retain the ROB entries to implement their precise exception model. 

When executing c.addi instructions, to optimnize for Dhrystone inner loop, there’s a special carveout for 
duplicate c.addi instructions in the same execution bucket. The logic inside the register renaming module 
checks if inside the same bucket there are two equal c.addi instructions AND there are no other 
instructions between them updating the same target register. When this happens, only the latter c.addi 
instruction gets executed with twice the immediate, thus freeing an ALU slot. 

6.3.1  Re-Order Buffer (ROB) 

The ROB keeps the speculated program order of instructions from decode and retires the instructions in-
order when their execution is completed.  The ROB is implemented as a FIFO where the write pointer 
writes entries for decoded instructions in the ID1 stage and the read pointer retires instructions from 
ROB.  The number of instructions retireable per clock cycle should be the same as the number of 
instructions that can issue per cycle.  Each ROB entry includes: 

Size  

Description  

Old PRF register 
Table 6-1. ROB Bit Fields 

7 

If the valid and execution-done bits are set, then the instruction can be retired and ROB read pointer 
incremented; 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               94 

Name 

Valid 

Except 

Branch miss 

Execution Done 

Restore 

16/32 bit inst 

Branch predict 

Valid write back 

Inst type 

Arch register 

PRF register 

1 

1 

1 

1 

1 

1 

1 

1 

3 

5 

7 

Valid entry, sets by write pointer and clears by read pointer 

Exception, i.e., instruction error, TLB fault, Memory error, DC tag/data ECC, … 

Branch miss prediction 

Set when instruction completes execution 

This instruction is canceled by branch misprediction, need to restore RAT.  The restore bits 
are clears by the read pointer 

16 or 32-bit instruction, needed for PC calculation 

Last instruction of the predicted basic block 

Valid write back to PRF 

000: ALU, 001: BRN, 011: CSR, 100: MUL, 101: DIV, 110: Load, 111: store 

Destination register; pointer to an entry in the ART 

Renamed destination PRF register 

Old renamed destination PRF register 

Cuzco Microarchitecture Specification, v0.2 

 

 

• 

• 

• 

• 

The PRF register is written to the Architected register of the ART 
The old-PRF register is written to the RFL and the RFL valid bit is set using the RFL write pointer 
The retire PC is incremented using the 16/32 instruction bit.  If the branch prediction valid bit is 
set, then the prediction information is used to update the PC.  The branch target from BPQ is 
used for PC if the predicted branch is taken 
If the entry is a mispredicted branch, then the mispredicted target PC is kept to update the retire 
PC when the mispredicted entry is retired 

If the retire instruction has the restore bit set, then the instruction is cancelled 

• 

• 

The ART is not updated 
The PRF register is written to the RFL and the RFL valid bit is set using the RFL write pointer.  The 
new PRF register which was assigned to the cancelled instruction, was not used, thus it is written 
back to the RFL 

•  No update to the retire PC  

The retire PC is accumulation of the number of retired instructions in a clock cycle.  The taken branch as 
indicated by the BPQ sets the retire PC to the BPQ target PC and restarts the accumulation of retired 
instructions in the next retire entry. 

For each branch misprediction, all subsequent instructions must be set to cancelled status in a single clock 
cycle in order to send cancelled signal to BEU in the next cycle.  The cancel mask is generated by using the 
mispredicted branch ROB pointer and the write pointer.  Note that a mask is used to clear the restore bits 
from the read pointer, the clear mask is used in combination with the cancel mask to write to the restore 
bits.   

Interfacing with branch execution unit: the ROB entry is sent with all instructions.  Multiple branches can 
be executed in the same cycle but only 1 can sent misprediction.  In EX0 stage, the branch execution 
queues sent the ROB entries which will be executed in the next clock cycle.  The ROB sorts the order of the 
branches and provide the order for the branches in case of multiple branch mispredition: 

• 

• 

The branch ROB entry subtracts the ROB read pointer for the relative position, the smaller 
number is the earlier branch 
The sort algorithm is used for ordering of the branch position.  The result is flopped to be sent to 
the BEU in the next cycle to select the earliest mispredicted branch 

Cancelling of ROB entries on branch misprediction: if there is branch misprediction in the first cycle, then 
a mask is generated to cancel all the later instructions which including the subsequent branches.  When 
the branch ROB entry is received by the ROB in EX0 stage, is must be checked against the cancel mask to 
not allow the BEU to assert branch misprediction from the cancel branches.  The BEU should sent only 
valid branch mispredition to the BPU and ROB. 

Restoring the RAT which is no longer correct as the architecture registers in the RAT are renamed to the 
cancelled registers.  The canceled entries in the ROB must restore the RAT starting from the ROB write 
pointer to the mispredicted branch pointer.  The new stream of instructions must be stalled in the ID0 
stage until the RAT is restored to the condition prior to the mispredicted branch.  The number of 
instructions to be restored in a clock cycle should be equal to or more than the number of instructions 
issuable per cycle.  

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               95 

Cuzco Microarchitecture Specification, v0.2 

 

 

• 

• 

• 

From the ROB write pointer, N number of ROB entries are read: the valid write back, the Arch 
register, and the old PRF registers 
The RAT writes the old-PRF registers to the Arch register entries 
Keeping track of misprediction 

Reset RSB entries – the physical registers may be released back to the RFL at later time, but forwarding 
and writing back to PRF are no longer necessary.  To save power, the write-back valid bits are cleared.  
The WPQ is responsible for writing back to the PRF, if the write-back valid bits in the RSB are cleared, then 
the write back can be gated off.  The RPQ can read the data from PRF but no forwarding is possible.  The 
restore ROB pointers can be compared and the valid bits reset of the entries containing wrong path 
instructions.  

Cancel STB entries – Cancel signals for every wrong path store instruction in the ROB are sent to the STB 
to cancel their corresponding entries.  The STB is also in-order. The stores are cancelled from the write 
pointer backward.  Corner case: it is possible that the ROB retires the cancelled store entries before the 
restore ROB pointers are sent to the STB.  

Cancel MRQ entries – The cancelled load instruction ROB pointers are similarly sent to the MRQ to cancel 
their corresponding entries.  The cancelled entries remain in the MRQ because its entries must wait for 
cache miss data to correctly maintain cache status.  The corresponding DMQ entry is deleted if the DMQ 
entry is associated only with cancelled entries in MRQ 

• 

• 

If there is a valid store written to a DMQ entry, and only a ROB retired store can write to DMQ 
entries, then valid DMQ entries write to the DC.  If there is no store, and the MRQ entry is 
cancelled, then the DMQ entry is cancelled to avoid a security issue with writing speculatively 
fetched cache lines into the DC 
Corner case: the MRQ entries are being cancelled at the time it is sent from MRQ to DMQ, in this 
case it must be  ensured that the MRQ-DMQ cancel signal is forced, invalidating entry allocation 
into the DMQ 

Cancel the XEQ entries – the XEQ entries are compared to the restore ROB pointers and matching XEQ 
entries are invalidated (i.e., “partial flush”).  The RPQ still reads data from PRF to XEQ but they are not 
used. The WPQ is responsible to write data back to the PRF and to reset the RSB entries which are no 
longer necessary.  The busy bits in TRM are not reset. 

• 

• 

• 

The XEQ valid signals to functional units should also be cancelled 
The XEQ replay instructions in TRM are allowed to complete, but the XEQ entries are invalidated 
LS XEQ instructions can be in the LSU pipelines.  The LS XEQ tracks and attaches the ROB pointers 
to the instructions in the LSU pipeline for cancellation of instructions, no write back, no cache 
miss, no load data replay 

Note: the restore ROB entries’ instructions are retired as normal, N instruction per cycle, but without the 
execution done bits. 

Other functions of the ROB: 

• 

Load and store instructions are considered completed execution when their addresses access the 
DC tag array without error.  A cache miss instruction can take hundreds of cycles to fetch the 
cache line.  The ROB could be full while waiting for this external memory fetch.  The load/store 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               96 

Cuzco Microarchitecture Specification, v0.2 

 

 

• 

ROB entries are retired in this case.  The RSB handles the long latency of the cache miss load data 
by setting the “unknown time” status for the load destination register and blocking any RAW 
instructions from execution.  Any pending store data can be kept in the MRQ to merge with the 
cache line data at later time.  Any external bus error is an imprecise exception. 
Store instructions can only update memory in-order.  As long as the store address is translated 
and the tag array is accessed without any error, the store entry can be retired in-order by the 
ROB, and writing store data to the DC can be done at a later time.  Any store data error is an 
imprecise exception.  The store instructions are kept in-order in the store buffer. The ROB 
indicates if the store should be retired.  The number of stores retireable per cycle is the same as 
the number of ROB instructions that can be retired per cycle.  The store buffer maintains a retire 
pointer to update the entry status. 

6.3.2  Register Free List (RFL) 

RFL is a list of all available PRF registers for renaming. Instructions are stalled in the ID0 stage if the RFL is 
empty.  This should be a rare case as the PRF size is set by benchmarks and performance model.  The RFL 
is implemented as a FIFO: 

•  Read pointer: assign free registers to new instructions (with valid write back) in the ID0 stage, the 

valid bits are cleared as the free registers are assigned to new instructions.  Valid destination 
registers of instructions in the ID0 stage are renamed with registers taken from the RFL 

•  Write pointer: reclaim PRF registers that were mapped to old architectural registers as indicated 

by the ROB, the valid bits are set as the old architectural registers are written 

The RFL retires written back registers from ROB as normal. If an instruction is cancelled, the new PRF 
register is written instead of the old PRF register.  Implementation note: writing of the new PRF register 
can be skipped as it is the current register in the RFL. 

The RFL can be used for integer/FP registers from the RISCV instruction where the number of registers in 
ART are defined by the ISA.  In addition, the RFL can be used in general when the registers are assigned 
and released out-of-order.  For example, the XEQ entries are assigned as the instructions are issued from 
the IDU with read times.  The read times can be random in which case the instructions are executed and 
retired out-of-order, the retired XEQ entries should be reclaimed and assigned to new instructions out-of-
order.  When the RFL is used for XEQ, then there is no ART, and all free registers are initialized from 0 to N-
1 (size of the RFL). 

The RFL must initialize all free registers.  The RFL is initialized from ART to PRF, for example 32 to 127 
integer registers, or 0 to 31 for XEQ.  The free_val_r is set to valid as the registers are initialized. 

 

6.3.3  Register Alias Table (RAT) 

The RAT tracks the latest PRF entry (speculatively) renamed for every architectural registers. The RAT has 
the same number of entries as the ART.  Both the ART and RAT are initialized to be identical for XRF 
registers.  The most recent valid state of the RAT can be recovered by copying the ART to the RAT. 

•  Valid source registers access the RAT in the ID0 stage for the latest renamed register for the 

architectural registers in the instruction. 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               97 

Cuzco Microarchitecture Specification, v0.2 

 

 

•  RAW intra-instruction fetch group dependencies cause the renamed register from the RFL to be 

used for the source operand of the later instructions with RAW  

•  Valid destination registers with assigned physical registers from RFL updates the RAT by the end 

of ID0 stage 

•  WAW intra-dependencies of instructions in a fetch group cause the latest instruction to write to 

the RAT 

The RAT is restored on branch misprediction as commanded by the ROB, starting from the most recent t 
instruction (i.e., ROB write pointer-1) to the earlier instruction (i.e., the ROB branch miss pointer). 

The RAT can be restored to its most recent correct non-speculative state by waiting for the mispredicted 
branch to retire and copying the ART to RAT. 

Best with an example: 

when r5 (arf_reg_r) is renamed to r54, and then renamed to r87.  

In the RAT, r5 = r54, then = r87 

In ARf, r5 = r5, then when r5=r54 is retired, then: 

•  R5 old_paddr is released back to the RFL (arf_reg_r[r5] = r5.  arf_reg_r[r5] = r54 

When r5=r87 is retired, then 

•  R5 old_paddr is released back to the RFL (arf_reg_r[r5] = r54, arf_reg_r[5] = r87 

Exactly the same in the RAT.  

rat_reg_r[r5] = r5 (initially)  

• 
•  when r5 is renamed to r54, then then old renamed register (d0_ren_paddr) is rat_reg_r[r5]=r5. 

before the rat_reg_r[r5] = r54 

•  when r5 is renamed to r87, then then old renamed register (d0_ren_paddr) is rat_reg_r[r5]=r54. 

before the rat_reg_r[r5] = r87 

The old reamed registers are release back to RFL when the instructions are retired. The old renamed 
register is no longer used any where in the execution pipeline. 

The d0_ren_paddr is the future old_paddr for register r5.  If the instruction is cancelled, i.e.m r5=r87 is 
cancalled. Then there is no update in the ARF, ARF is still ocrrect, but the RAT rat_reg_r[r5]=r87 should 
reverse back to r54.  Thus the r87 is released back to RFL and rat_reg_r[r5] = r54. 

 

6.3.4  Architecture Register Table (ART) 

The ART keeps track of which PRF register contains the most recent non-speculative correct contents of a  
renamed architectural register name as instructions are retired by the ROB. 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               98 

Cuzco Microarchitecture Specification, v0.2 

 

 

• 

• 

The new renamed register is written to the ART as the instruction is retired by the ROB 
The ROB can retire 2 instructions with the same architecture register name (WAW), in which case 
the second (i.e., the most recent) renamed PRF register entry is written to the ART 

6.3.5 

Example of Retiring and Restoring of Register Renaming 

Register renaming removes the WAW and WAR of the instruction and is necessary for out-of-order 
microprocessor.  To simplify the retirement and restoring of the destination registers, the architectural 
register, the old renamed architectural register, and the renamed register are kept in the ROB for each 
instruction with valid write back.  The first example show 5 registers has been renamed in the ROB, and 3 
instructions with valid write back are retiring: 

• 

• 

• 

• 

• 

The “arch” register 4 has been renamed to new register 8 where the previous register is 4 
The “arch” register 4 is used to select an entry in ART to update with new register 8 
The “old” register 4 is released back to the RFL by writing into the write-pointer entry, and 
increment the write pointer for the next write. 
The same procedure is used for second entry of ROB to retire architectural register 5 with new 
register 9 and to release register 5 back to the RFL. 
The same procedure is used for third entry of ROB to retire architectural register 1 with new 
register 10 and to release register 1 back to the RFL. 

•  After retiring of the ROB third entry to the ART and RFL, the RFL write pointer is incremented by 

3. 

Figure 6-4 Example of Register Renaming  

 

The same example is used for restoring of renamed registers upon branch misprediction.  All instructions 
younger than the branch misprediction instructions are used to restore the RAT and RFL.  The RFL retains 
the registers that were sent to the RAT and ROB, so the restoring of the RFL is simple by just decrementing 
the read pointer by the number of restored registers.  The RAT needs to restore to the previous registers, 
note that restoring of the RAT is in the reverse order in comparison to retiring: 

• 

• 

The “arch” register 2 has been renamed to new register 12 where the previous register is 2 
The “arch” register 2 is used to select an entry in RAT to update with old register 2 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               99 

vRFLARTRATarcholdnewwrptr18 --> 40044819 --> 51 --> 1010559Retire110 --> 12121110wrptr0113377110124 --> 882212rdptr1135 --> 9911466115711Write 3 entries in RFL by reclaiming 3 old registers in ARTROBselectentry 5write9 to set ART to new registerretireCuzco Microarchitecture Specification, v0.2 

 

 

• 

• 

The same procedure is used for second entry (from the bottom) of ROB to restore architectural 
register 7 in the RAT with old register 2. 
The same procedure is used for third entry (from the bottom) of ROB to restore architectural 
register 1 in the RAT with old register 1. 

 

Figure 6-5 Example of Restoring the RTL and RAT 

 
• 

 

6.4  Register Scoreboard (RSB) 

Each PRF register has a corresponding register in the RSB, the main function of the RSB is recording the 
write times for the PRF registers: 

Name 

Valid 

Overflown 

Unknown 

Write time 

ROB pointer 

Bits  

Description 

1 

1 

1 

N 

R 

Set if there is a pending write back. Reset if PRF entry contents are valid  

Set if the write time is greater than the max time counter 

Set if the load data is not returned at the expected time from external memory.  The MRQ sends a 
request to the XEQ to write back load data when it becomes valid.  Other dependent instructions 
are executed in-order as activated by the ROB 

N is the amount of time count bits. Write time for result data of instruction execution 

R is the amount of ROB entries log 2. The ROB pointer for the write back register.  When the 
register is written with result data (i.e., instruction completion), then the RSB entry is read to 
update the execution done bit in the ROB 

Functional unit 

5 

32 possible functional units 

Table 6-2. Register Scoreboard Bit Fields 

Where: 

•  Valid: (1) the instruction is pending in execution (0) no pending write back, no RAW, read data 

from register file. The write port control is responsible to clear the valid bit as data is written 
back to the register file. 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               100 

vRFLARTRATarcholdnewwrptr080044809110 --> 1559rdptr010212 -->21110011337711012482212rdptr1135911466115711 --> 7Restore 3 entries in RFL by adjusting the rdptr and setting valid bitsROBBmissselectentry 7write7 to restore register 7restoreCuzco Microarchitecture Specification, v0.2 

 

 

•  Overflown: The write time could be greater than the maximum time count in which case the 

Overflown bit is set. If the overflown bit is set, then the subsequent dependent instruction will be 
stalled in issue unit until the overflown bit is clear. The TRM has the number of time-count 
entries, and shows the write port as busy twice. If the overflown bit is set, then the write port 
control does not clear the valid bit. In another N time counter cycle, the valid bit will be reset as 
data is written back to the register file 

•  Unknown: the write back time can be unknown due to a L2 cache miss in which case the MRQ 

will trigger the corresponding entry in a XEQ to write data back to PRF and clear the valid. (Later 
implementation) the issued instruction can have RAW with a register with unknown time in 
which there are 2 options: 

o  The issue instruction is stalled in ID1 stage until the unknown bit is reset. 
o 

(Default option) The issued instruction is issued to the XEQ with unknown time is a 
replay instruction. The rationale is that doing so avoids stalling in the decode stages and 
subsequent instructions may not have a RAW data dependency with the unknown 
registers. This option is the same as replay instruction with unknown RAW data 
dependency. Benchmark performance modeling is needed. 

•  Write time: the write-back time of the issued instruction for which this PRF register is the 

destination  

o  This is the scheduled write time of the instruction from the preset execution time. The 
write time of a register is the read time of any RAW-dependent instruction(s) when the 
result data are forwarded. 
For any new instruction with 2 operands, then the worst-case (i.e., farthest in the 
future) write time is used to preset the read time, the other earlier available operand 
may be read from the register file.  

o 

• 

Functional-unit: the encoding for the functional units are defined as localparameter in the 
define.vh file. The functional units are used for forwarding and writing back to PRF: 

o  The write port control reads the RSB to know from which functional unit to grab result 

data to be written back to the PRF. 

o  The read port control reads the RSB to confirm that the write time has not changed:  

▪ 
▪ 

▪ 

▪ 

If the valid bit is 0, then the operand is read from the PRF. 
If the entry is valid and write time is equal to the time count, then the data is 
forwarded from the producing functional unit to the read port used for the 
dependent instruction(s). 
If the write time is greater than the time count, then the instruction is replayed 
where the new write time is used for the new read time for the replay 
instruction.   
If the write time is being modified at the same time as it is being read, then it 
may be a timing path.  If the write time is being modified, then updated write 
time is certainly greater than the time count.  The replay signal is asserted and 
the new write time bypasses to the new read time. 

Special cases: 

• 

(Later implementation – may not be necessary) For instructions with critical timing paths, result 
forwarding is not allowed. The forwarding is qualified with the functional unit types. From the 
architecture point-of-view, these instructions can add an additional cycle to the write time. The 
forwarding can be restricted between the functional units. For example, forwarding from a load 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               101 

Cuzco Microarchitecture Specification, v0.2 

 

 

instruction to an ALU or branch instruction is normal but it is less common to other functional 
unit types. When a functional unit reads from the RSB as a load and the issue instruction is a 
multiply, then 1 is added to the read time to read from the PRF. The read port control reads the 
RSB for forwarding information, if the forward data is from the load and the recipient is not an 
ALU, then the instruction is replayed as forwarding is not allowed. 

In the ID1 stage, the source operands read the RSB to set the read time (i.e., worst case RAW write time): 

•  Read_time = RSB_write_time (result data is forwarded from the RSB functional unit field) 
• 

Execution_time = RSB_write_time+1 

•  Write_time = RSB_write_time + instruction_latency_time 

If the write times for both operands are invalid (i.e., the PRF has the latest data for the source operands), 
then: 

•  Read_time = time_count + 1 
• 

Execution_time = time_count + 2 

•  Write_time = time_count + 1 + instruction_latency_time 

The above times are used in next stage, ID2, to access the TRM for resource availabilities to preset 
instruction issue. The read time is used to check for availability of the PRF read ports, the execution time 
is used to check for availability of the functional unit, and the write time is used to check for availability of 
the PRF write port. In order to minimize the chance of stalling an instruction in decode, the read_time+1, 
execution_time+1, and write_time+1 are also used to access the TRM to detect any conflict with the 
alternative time.  The basic concept for issuing an instruction is:  

1.  No RAW data dependency 
2.  Available PRF read ports at the read time 
3.  Available functional unit at the execute time, and  
4.  Available PRF write port at the write time 

The issued instructions are executed out-of-order as long as the above conditions are satisfied. 

Four (or more, if a wider microarchitecture is configured,) instructions can be in the issue stage at the 
same time. If there is no intra-instruction data dependency, then the instruction read times are calculated 
independently.  If there is intra-instruction RAW data dependency, then the later instruction should use 
the write time of the first instruction as the read time.  For example, if the second instruction has a RAW 
dependency with the earlier older first instruction, and the write times of first instruction are 6 and 7, 
then then read times of the second instruction are 6 and 7.  If the first instruction is issued with the later 
time, then second instruction can only be issued with the later time. 

If an instruction cannot be issued in ID2 stage, then the instruction must be recirculated.  The pending 
recirculated instruction increases its read time by 2 (i.e., read_time+2 and read_time+3) for the next 
cycle.   

The issued instructions write to the RSB in ID2 stage while the current instruction(s) reads the RSB in ID1 
stage.  The current instructions in the ID1 stage must compare their source operands with the destination 
operands of the issued instruction in the ID2 stage.  If any RAW data dependency is detected, then the 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               102 

Cuzco Microarchitecture Specification, v0.2 

 

 

write times of the issued instructions in ID2 stages are used instead of the write times from RSB.  The 
priority for a single source operand from highest priority to lowest are: 

• 

Intra-instruction decode group RAW data dependency between current instructions in ID1 stage 

•  RAW data dependency with issued instructions in the ID2 stage 
•  RAW data dependency with pending instructions in the RSB 
•  No data dependency in either currently decoding or issued instructions or the RSB 

Since an instruction can have 2 source operands, the instruction selects the worst-case time among its 
source operands. 

Issuing of store instructions:  the store address is from source register 1 while the store data is from 
source register 2.  The store address is sent to the AGU and tag array, while the store data are sent to the 
STB.  The store data are written to the DC data array only when the store instruction is retired in-order by 
the ROB.  For store instructions, source register 1 and source register 2 availability are 2 independent 
events where the store read time is based solely on source register 1 and the read time for the data in 
source register 2 can be at any time.  Issuing of the store instruction must be written to STB in ID2 stage 
for proper ordering of load and store instructions where the store data can be forwarded to a later (i.e., 
younger) load instruction with a matching or overlapping address. 

Special case: The RSB is the only unit where the magnitudes (i.e., greater or less) of instruction times are 
compared.  The time count is wrapped around, for example a 7 bit time counter wraps from 7F to 0 as 
..7C, 7D, 7E, 7F, 0, 1, 2, ... .When this occurs the g magnitude comparisons are no longer correct.  It is 
actually complicated to compare 2 time numbers.  The time count is assumed to be the earliest time and 
all other times are relative to the time count for comparison.  In a way, this is similar to the ROB where the 
position of the branch instructions are relative to the ROB read pointer.  The below table shows the case 
statement for greater than or equal comparison.  The inverted less than comparison is used for more 
efficient implementation.  

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               103 

Cuzco Microarchitecture Specification, v0.2 

 

 

Table 6-3. Greater-Than Comparator for Time A and B 

 

Implementation details: 

•  When the LDB is implemented, the current forcing of timing for ordering of load/store is no 

• 

longer needed. The LDB will flush the load instruction if the ordering is violated for valid older 
store address matching with completed load address.  Majority of the timing is eliminated. 
Setting the read times of the instruction is based on carry-select concept.  The RAW data 
dependency with previous instructions in the same ID1 stage is the highest priority, and next the 
RAW data dependency with previous instructions in the previous cycle ID2, and then no data 
dependency which uses the read time from the RSB 

o  The basic concept is concurrently calculated the read time based on the possible 

combination of RAW data dependency and select the one of the possible combinations. 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               104 

TABA>=TB>=TA>=BA>B111111111111001110011111000011110?111101?0000111100011000000011000000000010?100001?0011??11011??00100??11100??00010?1?1010?0?00011??00010??1101?1?1101?0?01101??01100??1MSBT is time_count, A is time_A, and B is time_BCuzco Microarchitecture Specification, v0.2 

 

 

 

 

o  The first instruction has 3 possible read times, the second instruction has 6 possible read 

times where 2:1 mux is used to select the read time, the third instruction is 3:1 and 3:1 
muxes, and the last instruction has 8:1 and 3:1 muxes.   

o  The latency time is accumulative latency time.  The worst case is the full RAW data 

dependency time from the first to the eighth instruction.  The accumulative latency time 
should be calculated in ID0 stage so that each module of 3 possible times have the same 
timing. 

o  Note that there are 2 sets of times for rd_time and rd_times, they are independently 

calculated. 

Dependency 

 

 

 

Inst 

ID1 

ID2 

RSB 

rd_time 

Mux 

Comments 

0 

1 

2 

x 

x 

x 

1 

1 

1 

0 

0 

0 

1 

1 

1 

1 

1 

1 

0 

0 

0 

1 

1 

0 

x 

x 

x 

1 

1 

0 

x 

x 

x 

x 

x 

x 

1 

1 

0 

x 

x 

1 

x 

x 

x 

x 

x 

1 

x 

x 

x 

x 

x 

x 

x 

x 

1 

d2_wr_time_r 

d2_wr1_time_r 

rsb_rd_time 

d2_wr_time[0]+latency_time 

d2_wr1_time[0]+latency_time 

rsb_rd_time[0]+latency_time 

d2_wr_time_r 

d2_wr1_time_r 

rsb_rd_time 

d2_wr_time[0]+latency_time[1:0] 

d2_wr1_time[0]+latency_time[1:0] 

rsb_rd_time[0]+latency_time[1:0] 

d2_wr_time_r[1]+latency_time 

d2_wr_time_r[1]+latency_time 

rsb_rd_time[1]+latency_time 

d2_wr_time_r 

d2_wr1_time_r 

rsb_rd_time 

Table 6-4. Read times calculation for 3 instructions 

 

3:1 

raw d2_wr_time from flops 

3:1 
and 
2:1 

This is a module with 3 basic 

times + latency time 

The same module with  latency 

time=0 

The same module with  
accumulative latency time 

3:1 
and 
3:1 

The same module with  latency 

time of inst 1 

The same module with  latency 

time=0 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               105 

Cuzco Microarchitecture Specification, v0.2 

 

 

 

6.5  Time-Resource Matrix (TRM) and Plan-ahead Scheduler 

The time-resource matrix consists of statuses of the PRF read and write ports and functional units for all 
time counts.  A bit is used to indicate availability of each resource at any time. The TRM has the same 
number of rows as the configured amount of time count values. If the read time of an instruction is 8 
cycles from the current time count of 17, then the instruction accesses time 25 of the TRM to determine 
the availabilities of PRF read ports at that time. 

 

Figure 6-6 Time-Resource Matrix for Availabilities of Resource for Issuing a Single 
Instruction 

In the above Figure, 1 PRF read port is busy at the instruction’s read time of 25, no write ports are busy at 
the instruction’s projected write time of 27, and 1 load/store port is busy at the instruction’s execution 
time of 26 and no other functional unit is busy. The resources are always assigned to instruction in-order.  
In the above figure, if the current instruction needs 1 read port at read time, then the second read port is 
also reserved for the issued instruction at read time 25. Thus the instruction busies both read ports at 
time 25.  The resource numbers are reset to 0 when the time count is incremented.  For example, in the 
above Figure, when the time count increments from 16 to 17, all busy bits at time 16 are reset and in next 
cycle, all busy bits at time 17 will be reset.  Five functional units: ALU, MUL, DIV, BRN, and LS port are 
available for a single instruction which may be wasteful, but simplifies the issue logic. 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               106 

TimeRd ports (2)Wr ports (1)ALU (1)LSMULDIVBEU6362.2702601000251.1711111000160000000.0Time-Resource Matrix - Single InstructiontimecountexecutetimereadtimewritetimeCuzco Microarchitecture Specification, v0.2 

 

 

 

Figure 6-7 Time-Resource Matrix for Availabilities of Resource for Issuing Dual 
Instruction 

The read ports, write ports, and ALUs are doubled for a 2 instruction slice but the other functional units 
remain single.  Sharing of resources for 2 instructions (as in the case of 2 ALUs and 2 write ports) may 
enable more flexible conflict resolution but it is more difficult to implement.  Overall, 6 functional units 
are available for 2 instructions.  In an optimization for timing, the PRF read ports are not shared.  
Implementation note: the option of not sharing the ALUs and write ports may also be necessary if there is 
timing issue. On the other hand, sharing of read ports may be necessary for performance. 

If the instruction cannot be issued, then the read time is modified to read_time+2 and its execution is 
reinitiated in the next cycle. 

The worst-case timing path is the serial decision from the first issued instruction to the last issued 
instruction because the instructions are issued in-order.  All issued instruction information is written into 
the XEQ while the TRM read times and RSB valid bits are the last to be set.   

The TRM is updated every time an instruction is issued.  The resource assignments and read time are sent 
with each instruction to the execution queue (XEQ).  The read port assignment selects the read port and 
writes the PRF source register at the read time to the read port control unit as discussed in the next 
section.  The write port assignment selects the write port and writes the PRF destination register at the 
instruction write time to the PRF write port control unit, also discussed in the next section.  Note that the 
XEQ and the read port have the same read time for synchronization.  

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               107 

TimeRd ports (4)Wr ports (2)ALU (2)LSMULDIVBEU6362.2712611000251.174221001160000000.0Time-Resource Matrix - Dual InstructiontimecountexecutetimereadtimewritetimeCuzco Microarchitecture Specification, v0.2 

 

 

 

Figure 6-8 Instruction Slices for 8-Issue Microprocessor 

The number of instructions issued is fixed (configurable) per cycle per slice.  The instructions must be 
issued from an XEQ in order, i.e., if instruction 3 cannot be issued, then instructions 4-7 will not be issued.  
Instructions 3-7 are shifted into XEQ positions 0-4, and new instructions from the ID1 stage may be shifted 
into XEQ positions 5-7.  In a four-slice configuration, slices 0 and 1 are more utilized than the slices 2-3.  To 
compensate for the underutilization of slices 2-3, the data miss queue (DMQ) in the LSU can write data 
back to the PRF using any slice’s write buses.  The MSB write buses are selected before the LSB write 
buses.  

An issue with the shifting of “not-issued” instructions to LSB positions is the assignment of the divide 
instruction to a particular slice is not known until issue time.  The later section will discuss handling of 
divide instructions. 

Implementation details (similar to the RSB read times calculation): 

• 

•  Note: if only partial number of instructions is issued, then the instructions are shifted up, the 
RAW and issued position of an instruction is no longer the same.  Thus, the RAW and issued 
position of the instruction must be calculated again. 
The issue logic is independently calculated for all instructions 
The mux select logic is independently calculated to select a possible combination of read times 
and read1 times.  The mux select logic signals are also used in ID1 stage 
Setting the read times of the instruction is based on carry-select concept.   

• 

• 

o  The first instruction has 2 possibilities of being issued. 
o  The RAW data dependency with previous instructions are assumed to determine if the 
instruction can be issued.  The second instruction has 4 possibilities of being issued, or 
2:1 muxes followed by 2:1 mux. 

o  The last instruction has 2:1 muxes followed by 8:1 mux for possibility of issue, issue1, or 

none (this is same as issue logic). 

6.5.1  Replay Due to L1 Cache Miss 

An instruction can be replayed if the write time of a register in the RSB has been modified.  Since the 4 
read ports can be for 4 issued instructions from XEQ (each instruction uses only 1 read port), the TRM 
slice is potentially accessed by a total of 6 instructions, 2 issuing and 4 replaying.  The replay instructions 
with their new read time and read time +1 must check for the availability of their resources to update 
their read times in the XEQ.  The TRM priorities are: 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               108 

Time CountTag Array LS portInstruction DecodeRegister RenameRegister ScoreboardResource MatrixInstruction Issue UnitExecution QueuesFunctional UnitsInstruction CacheRegister FileBranch Prediction UnitBus Interface UnitTag Array LS portInstruction DecodeRegister RenameRegister ScoreboardResource MatrixInstruction Issue UnitExecution QueuesFunctional UnitsTag Array LS portInstruction DecodeRegister RenameRegister ScoreboardResource MatrixInstruction Issue UnitExecution QueuesFunctional UnitsTag Array LS portData CacheInstruction DecodeRegister RenameInstruction Fetch UnitRegister ScoreboardResource MatrixInstruction Issue UnitExecution QueuesFunctional Units+12Front-endBack-end222Cuzco Microarchitecture Specification, v0.2 

 

 

• 

• 

• 

Issuing instructions have highest priority.  To simplify the issue logic, if the read time of the 
replay instruction matches with any read time of an issuing instruction, then the replay 
instruction at its read time has a conflict regardless of the number of valid operands for the 
issuing instruction 
For further simplicity, the replay instruction(s) may have fixed read buses.  For example, the 
replay instruction rs1 operand uses PRF read bus 1 and its rs2 operand uses PRF read bus 0.  Note 
that rs1 is used more often than rs2, during issue, the rs1 operand uses read bus 0 and the rs2 
operand uses read bus 1.  Replaying might use the opposite read buses assignment to minimize 
conflicts. 
For further simplicity, the PRF write buses and ALU assignments are also fixed.  Replaying of an 
instruction that was in issue position 0 can only use PRF write bus 0 and ALU 0.  Replaying of an 
instruction that was in issue position 1 can only use PRF write bus 1 and ALU 1. 

Replaying of load-miss instructions: The load data needs only the PRF write port to write back data to the 
PRF in the next cycle. The write TRM provides the busy signal for the PRF write ports at time_count+1, 
where the XEQ can use the available write port to write data back to the PRF without accessing the TRM.  
Note that the MSB slices are underutilized, thus the write buses selection is made with the reversed 
order. 

Replaying of instructions with unknown read time:  The instructions that depend on prior instructions 
with unknown write times want to start execution in the next cycle, the TRM provides the busy 
information for the next couple cycles from the time count for dynamic execution of such instructions in 
the XEQ.  

•  Read ports: busy is signaled for PRF read ports at time_count+1. There is no need to write to the 
TRM.  Note: the read port busy signal is also used by the STB entry with “unknown time” to read 
data. 
Functional units: busy is signaled for all functional units at time_count+2. There is no need to 
write to the TRM. 

• 

•  Write ports: busy is signaled for PRF write ports at time_count+1+Funit_latency_time.  There is a 
specific write port time for each type of functional unit. The TRM write port is updated with write 
register address and the RSB is updated with a new write time for any write time > 
time_count+2. 
For further simplicity, the resource selection is fixed for ALUs to use only PRF read buses 2 or 3, 
ALU 1, and PRF write bus 1 while all other units use read buses 0 or 1, and PRF write bus 0. 

• 

An XEQ entry with an unknown time can check the RSB to see if the valid bit is reset in which case the 
instruction can be issued in the next cycle if the resources are available.  The RSB and write port TRM 
must be updated in such cases. 

Issuing of store instructions:  A store instruction has 2 PRF read times for source registers RS1 (store 
address) and RS2 (store data).  The store instruction is based solely on the RS1 read time for issuing to the 
L1 DC.  The second read port in needed to read RS2 data into the STB which can be at any time regardless 
of the RS1 read time.  The RS2 PRF read port does not stall the store instruction in the ID2 stage, at worst 
case, the store instruction can be issued to the STB with an “unknown” time.  

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               109 

Cuzco Microarchitecture Specification, v0.2 

 

 

6.5.2  Handling of Divide Instructions 

All non-divide instructions have a throughput time of 1; i.e., multiply instruction can be dispatched to the 
same multiply unit one per cycle.  Non-fixed throughput instructions with a TRM, is problematic: 

• 

• 

The throughputs of 32-bit and 64-bit divide instructions can be as much as 32 and 64 cycles.  A 
second divide instruction in the same slice can be stalled for 16 or 32 cycles, respectively.   
If the divide instruction is replayed, then the divide TRM is unnecessarily blocked for many cycles 
and disallows divide instructions to be replayed when the load data is returned from the L2 cache 
or beyond. 

A modified scheme for divide instructions is: 

• 

Issue divide instructions the same as other instruction types; assuming single cycle throughput.  
Using the same mechanism used with other instruction types, without blocking multiple divide 
instructions in issue. 

•  When the instruction becomes valid in XEQ and is ready to issue to a divide unit: 

o 

o 

o 

o 

Flops are used for a valid bit and divide throughput time (for the addition of a latency 
time count and a throughput time of 32bit or 64-bit divide instructions) to indicate that 
the divide unit will be busy. 
If the divide unit is busy, then a subsequent divide instruction is replayed with a new 
read time set to the divide throughput time.  A valid divide operation is not immediately 
sent to the divide unit.  The instruction will go through TRM to replay the divide 
instruction with its new PRF read time and read time+1.  Note: multiple divide 
instructions can be replayed because of divide busy where the TRM causes the divide 
instructions to have different PRF read times even if the instruction issuance is based on 
the divide throughput time.  It is possible that a divide instruction is replayed because of 
both the divide busy bit and a delayed source operand in the RSB.  The divide busy bit 
takes priority. The divide instruction will check the RSB again for availability of the 
source operand.  Implementation note: (1) a bit can be used to cause a second replayed 
divide instruction to have its read time incremented by 1, (2) the RSB has the overflown 
bit for any instruction with a write time greater than the time_count; for the replayed 
instruction, if the PRF write time is greater than the maximum write time, then the PRF 
read time can be half of the pending throughput time and then replay again because the 
divide busy bit will still be set. 
If the divide busy bit is not set and the instruction is replayed due to a modified source 
operand (from RSB) write time, then no valid divide operation is sent to the divide unit.  
The new PRF read time from the RSB is used to replay the instruction, i.e., the same 
mechanism used with any other instruction type.  
If the instruction cannot be issued to the divide unit, then the divide throughput time is 
not set, allowing the divide instruction to be replayed when its source operands are 
valid. 

o  Advantages of the new scheme include: (1) flops of valid bit and throughput time are 

per slice (2) only 1 possible divide instruction can be issued to a divide unit at a time; 2 
divide instructions are possible per slice and a divide instruction can move from 1 slice 
to another, and (3) the overflown bit may not be necessary as in the above 
implementation note. 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               110 

Cuzco Microarchitecture Specification, v0.2 

 

 

Divide instructions are rarely used in code. Multiple contemporaneous divide instructions are even more 
rare.  The replay mechanism is already incorporated into the design for other instructions. 

6.5.3  Read-Port Control Unit 

The read port queue (RPQ) is implemented as shown in the Figure below where the number of entries is 
the same as the time count and each entry consists of a valid bit and a register to read data from the PRF. 
Each issued instruction records its source operand register at the read time.  This read time is the same as 
the read time in the XEQ.  The read port number is also recorded in the XEQ to select the operand data.  

At the designated time count, the register is read from the RPQ as shown in the Figure below. For timing, 
the RPQ can be read 1 cycle ahead of designated time count, in which case bypass logic is needed for the 
issued instruction with the same read time as time count.  The procedure to read instruction source 
operand data is: 

• 

• 

The register is sent to the PRF to read data 
The register is sent to the RSB to read register status, its write time, and the functional unit that 
may be writing it: 

If RSB valid bit is 0, then the register data from PRF is valid 

o 
o  Else, if write_time=time_count, then the functional unit from RSB is used for forwarding 

data. The forwarding functional unit is used to grab data from the specified functional 
unit to put data on the read port as shown in Figure 6-3. 

o  Else, if write_time>time_count, then the instruction is replayed. The write time sets the 
read time for replay. The write time of an instruction can be modified because of a data 
cache (DC) miss or bank conflict. The replaying instruction in the XEQ is set with new 
read time after rechecking resource availability from the TRM.  The RPQ is again written 
with a new read time.  

o  Note: The write time can be modified (e.g., due to instruction replay) in the same cycle 
in the RSB.  If the register to be written (from WRQ) is the same as a register to be read 
(in the RPQ), then the instruction will be replayed.  The bypassing write time is used for 
the replay read time, but it is not necessary to compare to time count 

The valid bits are reset by the time count similar to the TRM 

• 

 

• 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               111 

Cuzco Microarchitecture Specification, v0.2 

 

 

Figure 6-9 One Read Port Queue (RPQ) 

 

The RPQ (read ports to PRF) can be dedicated to different types of operations.  The procedure to read the 
PRF registers from issuing or replaying instructions is the same: accessing the RSB, forwarding from 
functional unit, or replay.  The replay algorithm is discussed in the XEQ section. 

6.5.4  Write-Port Control Unit 

The write port queue (WPQ) is implemented as shown in the Figure below where the number of entries is 
the same as the time count and each entry consists of a valid bit and a register to write data to the PRF.  
An issued instruction records the destination register at its write time.  The write port from the TRM is 
also recorded in the XEQ for replaying.  The WPQ is synchronized with the XEQ to grab the result data at a 
write time for writing to the PRF.  

At the time count, the register name is read from the WPQ as shown in the Figure below. The procedure 
to write result data from a functional unit to PRF is: 

• 

• 

The register name is sent to the PRF to write data, the result data can be written regardless of 
validity 
The register name is sent to the RSB to read register status: 

o  The functional unit id is read for selecting the result data of the correct functional unit 

o 

to the PRF write port 
If the overflown bit is set, then the overflown bit is reset. The actual write time is 64 
cycles later in the example shown in the Figure below 

o  Else, if write_time=time_count, then the valid bit in the RSB will be reset 
o  Note that RSB write times can be modified in the same cycle, so it can be a timing path 
in blocking writing back to the PRF (not a functional issue). It is only important for RAW 
data dependency. 

The valid bits are reset by the time count similar as is done in the TRM. 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               112 

Timevalidxreg630x22620x35.1x27.0x66271x35260x27251x94.0x1220x510x1700x22A Read Port ControltimecountCuzco Microarchitecture Specification, v0.2 

 

 

Figure 6-10 One Write Port Queue (WPQ) 

 

The WPQ (write ports to PRF) can be dedicated to different types of operations.  The procedure to write 
the PRF registers from issued or replayed instructions are the same: accessing the RSB, selecting a 
functional unit per write port.  The replay algorithm is discussed in the XEQ section. 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               113 

Timevalidxreg630x22620x35.1x27.0x66281x35270x27261x94.0x1220x510x1700x22A Write Port ControltimecountCuzco Microarchitecture Specification, v0.2 

 

 

7  Branch Execution and PC Calculation 

The PC is calculated in 3 different pipeline stages: 

• 

• 

Speculative PC in the BPU and IFU to fetch instructions. 
Speculative PC in the decode unit that tracks the current PC for attachment to the branch 
instructions. 

•  Architecture PC in the retire stage, the ROB calculates the sequential PC and updates the 

architecture PC accordingly. 

The BPQ pointer and predicted-branch valid bit are attached to all instructions.  In ID1 stage, the BPQ 
pointers are sent to the BPQ to read the respective entries.  PC calculation in this stage is similar to PC 
calculation in the ROB.   

• 

• 

The decode PC is incremented using the 16/32 instruction bit.  If the branch prediction valid bit is 
set, then the prediction information is used to update the speculative PC.  The branch target 
from the BPQ is used for PC if the predicted branch is taken. 
For a branch misprediction, the mispredicted target address or next address is used to update 
the decode speculative PC and decode waits for the new stream of instructions. 

If some instructions are not issued in the ID2 stage, then the BPQ prediction information must be recycled 
while receiving and incorporating new prediction information for the new instruction from the ID1 stage. 

Implementation note: the unconditional branch and branch target address could be calculated in this 
module and detect branch misprediction earlier than branch execution.  This is added complication and 
probably not necessary since the BTB is very good at predicting both unconditional branches and PC 
relative branches.  

Later implementation note: Not-a-branch should be implemented in this module. The branch prediction 
valid bit is set but this is not a branch instruction. 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               114 

Cuzco Microarchitecture Specification, v0.2 

 

 

8 

Integer Execution Queue (XEQ) Including Replay 

The issued instructions are written to an appropriate XEQ.  Within each slice, each type of functional unit 
has its own execution queue; i.e., there is a single shared queue for the multiple ALUs.  The number of 
entries for each XEQ in each instruction slice is shown in Figure 6-2.  Five XEQs are used with each 
instruction slice: 

•  ALU XEQ:  all ALU instructions, as shown in the below Figure 
•  MUL XEQ: all multiply instructions   
•  DIV XEQ: all divide instructions 
•  BRN XEQ: branch instructions, the PC and ROB pointers are included in the ex_ctrl field 
• 

LS XEQ: all load and store instructions, only rs1 is needed for tag address 
STB: the store instructions, only rs2 is needed for store data to be written to data cache in-order.  
The STB includes the ROB pointer for in-order execution of the store instructions.  

• 

The XEQ is discussed in the section while the STB is discussed in the LSU chapter.  The XEQ for arithmetic 
and branch instructions are generic, the data are passed to the XEQ module on a variable width.  The LS 
XEQ is much more complicated in comparison and is implemented as a independent module type.  

 

Figure 8-1 The ALU XEQ with OOO Execution 

When an instruction’s read time matches the time count, the XEQ can issue an instruction to a selected 
functional unit for execution, the read port control is in synchronized to read source operand data from 
the PRF or forward data from a functional unit. The rs1 and rs2 fields are used to select the source 
operand data from preset read buses.  Along with muxing for the source operand data, the replay status 
from the RSB is also selected by the same rs1 and rs2 select fields.   The read port control unit accesses 
the RSB to read the functional unit for forwarding and the write time to ensure that it has not been 
modified. If it has been modified the instruction is setup to be replayed.  A replayed instruction checks the 
TRM resources for availability in the next cycle to set the new read time.  Replay instructions remain in 
the XEQ with their modified read times, but the same originally preset read port, write port, and 
functional unit are used for replay.  It is much simpler (and timing path) for the TRM to check for 
availability of a single fixed read port, write port, and functional unit.  In addition, the select bits for the 
RS1, RS2, WR, and functional units are not modified.  The replay instruction has the read_time and 
read_time+1 as with any issued instruction.  With known read and write times, the read and write port 
control units keep track of the registers.  For unknown read and write times, the XEQ includes the source 
and destination PRF register addresses.  In the case of replaying, the XEQ should update the read port, 
write port, and RSB with the new times and register addresses as needed. The bit fields in XEQs are 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               115 

vex ctrlimm datars1 selrs2 selFunit selreplay ctrlRd time0ctrl datadata000ctrl bits01ctrl datadata12ALU1ctrl bits270ctrl datadata01ALU0ctrl bits211ctrl datadata01ALU0ctrl bits25hitread port 0read port 1Execution Queueselect inst to issue=?=?=?=?timecountCuzco Microarchitecture Specification, v0.2 

 

 

customized to instruction type, for example, a branch prediction XEQ includes branch prediction 
information while the ALU XEQ includes PC, LUI, and AUIPC. 

The number of instructions that can be issued per clock cycle is based on the number of functional units.  

Arithmetic XEQ: The XEQ is set up so that the functional units are generic with respect to execution 
instruction type, with execution opcode, 2 source operand data, and a signed/unsigned bit as necessary.  
Immediate data are signed/unsigned extended and muxed with source operand data.  The immediate 
operands of special instructions like LUI and AUIPC are shifted and muxed in the XEQ before they are sent 
to the functional unit.  The bit fields of an arithmetic XEQ entry include: 

Name 

Valid  

Time 

Ready 

Data 

Bits  

Description 

1 

N 

1 

K 

Entry valid bit 

Same bit-size as the time count, when read_time==time_count, then the instruction is issued to 
functional unit.  The read time is set when the instruction is issued or replayed 

For timing, the (read_time-1)==time_count, then the bit is set for issuing instruction in next 
cycle.  Note that incoming instruction should be compared to time_count to set the ready bit 

The bits in XEQ that are not modified by the XEQ, this field is customized for the instruction type 

Immediate data 

64 

Immediate data which is also used to capture the source operand data if the instruction is 
replayed.  In some cases, the destination data may be kept in this field 

RS1 select 

RS2 select 

WR select 

Functional unit 
select 

Replay valid 

RS1 replay valid 

RS2 replay valid 

ROB unknown 
 

Destination 
unknown 

2 

2 

1 

1 

1 

1 

1 

1 

1 

To select the read port for source operand data for RS1 

To select the read port for source operand data for RS2 

To select the write port for destination operand. For replaying, the same write is selected to read 
the same TRM write port 

To select the functional unit to issue the instruction; 2 ALU instructions can be issued in the same 
cycle.  For replaying, the same functional unit is selected to read the same TRM functional unit 

The current instruction is replayed 

For replay, the valid RS1 source operand data is kept in the Immediate data field 

For replay, the valid RS2 source operand data is kept in the Immediate data field 

The read time is unknown 

The write time is unknown for destination operand.  It is optional that the unknown instruction is 
executed in 2 steps (1) read source operand data and execution instruction and store the result 
data in the Immediate data field (2) write data to PRF 

Table 8-1. XEQ Bit Fields 
 

 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               116 

Cuzco Microarchitecture Specification, v0.2 

 

 

XEQ data bit fields: The data bit fields are modified per instruction type, including both the common data 
and optional data for different instruction types.  These data do not modify in the XEQ, but instead simply 
pass through the XEQ to the functional units.  The arrangement of the bit fields in the data prioritizes ease 
of extraction.  The customized fields are indicated in the table below.  The LS XEQ entries have the same 
bits with the exception of the RS2 related bit fields. 

Name 

IPC valid 

PC 

Predicted BRN 

Rs2 is X0 

BPQ Pointer 

16-32 bit Inst 

Unconditional 

Sign operation 

64-bit Inst 

Rs1 is X0 

Immediate valid 

Write valid 

Execute opcode 

PRF RS1 register address 

PRF RS2 register address 

PRF WR register address 

Bits  

Description 

1 

64 

1 

1 

4 

1 

1 

1 

1 

1 

1 

1 

5 

7 

7 

7 

(ALU) This instruction is AUIPC, PC is RS1 source operand 

(ALU and BRN) Instruction PC 

(BRN) predicted branch instruction 

(BRN) Use 0 for first source operand 

(BRN) BPQ entry for predicted branch instruction 

(BRN) the size of the branch instruction to calculate the next address from the PC 

(BRN) this branch instruction is unconditional 

For functional unit, from instruction decoding in ID2 pipeline stag 

64-bit operation (current implementation) – not set for 32-bit operation 

Use 0 for first source operand 

Immediate data is valid 

Valid write back to PRF 

The opcode for instruction execution in the functional unit, individual execution opcode is 
defined in define.vh file 

N-bit address for RS1 register to read from PRF (for replay instruction with unknown time) 

N-bit address for RS2 register to read from PRF (for replay instruction with unknown time) 

N-bit address for WR register to read from PRF (for replay instruction with unknown time) 

ROB pointer 
Table 8-2. Data Bit Fields for Arithmetic and Branch XEQ 

N-bit pointer to the ROB (for replay instruction with unknown time) 

7 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               117 

Cuzco Microarchitecture Specification, v0.2 

 

 

Figure 8-2 Replay of Dependent Instruction 

The basic replaying mechanism for a dependent instruction with a modified write time in the RSB is 
shown in the above Figure: 

 

• 

EX0 stage:  

o  The RPQ sends the register name to read the RSB, if the write_time>time_count, then 

the instruction is replayed. the  replay signal is asserted andthe RSB write time is the 
used to set the read time for replaying. 

o  XEQ entry: if the read time matches the time count, then the instruction is dispatched to 

the selected functional unit.  The rs1/rs2 select field from the XEQ selects the source 
operand data, the replay signal, and the write time from the selected read port.  If the 
replay signal is asserted, then the instruction is replayed. The XEQ entry remains valid 
and sets the replay bit fields. 

o  The RSB write time is used to calculate replay read, execute, and write times. The 

read_time+1 is also calculated. 

o  The RSB may have the unknown bit set. The replay instruction sets the unknown bit. 
o  The XEQ entry is invalidated if the instruction is not replayed. 

• 

EX1 stage: 

o  The TRM is accessed for resource availabilities. This procedure is the same as in issuing 

instructions. 

o  The RSB is updated with the new write time and the RPQ/WPQ, and XEQ are updated 

with the new read times. Note that the new write time is forwarded to RPQ for reading 
the RSB.  Note that the XEQ provides the destination PRF address to write the WPQ 
(replay write port) and RSB.  The new write time is written into the destination register 
of the RSB. 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               118 

EX0:RPQ access PRF & RSBRSB -valid=0, reads from PRFRSB - wrtime=time_count, forwardRSB - wrtime>time_count, replayXEQ - select rd ports for dataReplay - RSB wrtime -> rd timeEX1 - replay:TRM - resource available?RPQ - new read timeWPQ - new write timeXEQ - new read time        - valid rs1/rs2 data?TRMRegister FileRead Port ControlFunctional UnitRegister ScoreboardReserved LS portmuxmuxXEQrs1datareplaywritetimeCalculaterd/wr/ex timesCuzco Microarchitecture Specification, v0.2 

 

 

o 

If there are conflicts for both read_time and read_time+1, then the replay instruction 
sets the unknown bit to execute the replay instruction in ROB retire order. 

The above procedure includes the LS XEQ if the RS1 operand (i.e., tag address) is delayed.  Unlike the 
arithmetic XEQ, the LS XEQ entry can be replayed at later stages.  The LS XEQ entry remains valid until the 
load data are written  into the PRF.  The LS XEQ entries are pipelined as the LS instruction moves through 
the LSU pipeline.  The LSU pipeline can directly read data from its LS XEQ entry.  

Load/Store XEQ:  These have the same bit fields as the arithmetic XEQs with the exception of RS2 related 
bit fields and many more bits for the complication of load/store executions.  All the additional bit fields 
are related to replaying of load/store instructions. 

Name 

Bits  

Description 

Address valid 

XCL 

XDB 

Rd1 unknown 

Rd2 unknown 

DC miss 

1 

1 

1 

1 

1 

1 

For replay, the valid physical address is kept in the Immediate data field 

Cross cache line, unaligned access, the load/store data straddle 2 cache lines 

Cross data bank, unaligned access, the load/store data straddle 2 data banks (64-bit) 

For unaligned access, the first half timing is unknown 

For unaligned access, the second half timing is unknown 

Data cache miss 

Load data 

64 

Load data storage is cannot write to PRF, needed for crossing data banks or cache lines, partial 
data arrives at different time, and write port conflict 

Bank 1 valid 

Bank 2 valid 

Replay valid 

1 

1 

1 

For unaligned access, indicate that the first bank data is valid in the Load data field 

For unaligned access, indicate that the second bank data is valid in the Load data field 

The current instruction is replayed 

STB pointer 
 
Table 8-3. Load/Store XEQ with Additional Bit Fields 

For store instruction 

5 

Different types of replaying instructions: 

1.  Cache miss: use the L2 cache hit latency to update the write time.  The instruction will be 

replayed and the new write time is written to the corresponding entry in both the RSB and LS 
XEQ.  When the data is valid, the DMQ will assert the request to the LS XEQ to complete the load 
operation.   

o  The LS XEQ checks for write port availability to write back data to the PRF and to reset 

the write time in the RSB. 

o 

o 

If a write port is not available, then the load data is written into the load data field of 
the LS XEQ entry.  The LS XEQ modifies the write time in the RSB entry to “unknown”.  

If both Rd1 and Rd2 valid bits are set, then the LS XEQ checks for write port availability 
to write back data to th ePRF and to reset the write time in RSB.  This same process is 
repeated until the load data can be written to PRF. 

o 

Independent with cache miss valid data, at the write time, if the load data is not written 
back to the PRF, then the LS XEQ sets the write time in the RSB entry to “unknown”. 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               119 

Cuzco Microarchitecture Specification, v0.2 

 

 

2.  Bank conflict: A data bank queue (DBQ) is implemented for each DC data bank. Bank conflict is 

only a hazard for cache-hit load data. The address in the LS2 stage is used to select a data bank. If 
the DBQ is empty, then there is no bank conflict and the load can be accessed as normal.  If the 
DBQ is not empty, then the load enters the DBQ and the position in the DBQ is the additional 
delay cycles for the load instruction (time_count + delay_time). The instruction will be replayed 
and a new write time is written to the corresponding entries in both the RSB and LS XEQ.  Similar 
to cache miss, the DBQ will assert the request to the LS XEQ to write data to the PRF. 

3.  Unaligned access:  Merging of data from unaligned accesses, cache miss data, and the data bank 

delays are handled in the LSU data control unit (lsu_dat).  The data can be from the data SRAM 
arrays, the DMQ queue, forwarding from STB/DMQ/DEQ, and the LS XEQ (load data field).  Both 
data banks can read data at the same time for the LSU data control unit to align data and write 
back to the PRF.  If the data are at different times, then the LS XEQ keeps the raw data (unaligned 
first or second half data) until the other half is triggered by the DMQ or data bank queues.  
Basically, if any valid load data is available and cannot be written to PRF, then the data is written 
into the LS XEQ entry and the corresponding valid bits are set. 

4.  ECC 1-bit error (later implementation): 1 additional clock cycle is needed to correct erroneous 

data.  The ECC data is kept and sent to the load data logic to write to the PRF.  The ECC logic finds 
an idle cycle of the data bank to write cleaned data to the data bank (similar to STB).  The process 
is similar to a data bank conflict with a new write time of time_count +1.  (Optional) The ECC 1-
bit error may be detected late in the LS4 stage, in this case, the load instruction is treated as 
exception where the pipeline is flushed and the load instruction is re-fetched from the IFU.  
(Optional) If the cache line is clean, then the cache line can be invalidated and the instruction is 
re-fetched from the IFU for ECC 1-bit or 2-bit error. 

5.  Special cases of delays to replay the load/store instruction: 

o 

o 

(Later implementation) Partial hit in the STB: For example, 2 store bytes following by the 
load word of the same word address. The store data cannot forward in this case.  In this 
case, the load instruction will be replayed in-order of retiring by the ROB.  The unknown 
status bit is set in the XEQ entry. 

(Later implementation) ECC 1-bit error from the tag array.  Note that for a 4-way 
associate tag array, if there is any hit, then the ECC 1-bit error is ignored.  The ECC 1-bit 
error is valid only if there is no tag hit.  The corrected tag is used when the load/store 
instruction is replayed which is 1 cycle later. The XEQ keeps the translated address and 
starts from the AGU stage, but the XEQ sends a translated address to bypass this stage 
and compares to the ECC corrected tag in the tag access stage.  (Optional) The 
load/store instruction is treated as an exception where the pipeline is flushed and the 
load instruction is re-fetched from the IFU.  

(Later implementation) dTLB miss, the L2 TLB access the time is known, the new read 
time is set in LS XEQ for re-execution of the instruction.  If L2 TLB misses, then a TLB 
exception is taken to re-fetch the instruction.  The L2 TLB does a table walk and updates 
the dTLB.  For re-fetching, if the L2 TLB is busy, then the instructions are stalled in the 
ID0 stage.  (Optional) The load/store instruction is treated as an exception where the 
pipeline is flushed and the load instruction is re-fetched from the IFU. 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               120 

Cuzco Microarchitecture Specification, v0.2 

 

 

8.1  Unknown Replay Protocol 

The ROB buffer knows the instruction issued position and sends replay signals to the specific 
slices.  The STB receives all ROB signals and can send the replays based on the issued position.   

•  The XEQ ensures that the ROB pointer matches with the retired signals from ROB before 

send active replay signal to the XEQ unknown module 

•  The source operands are sent to the RSB to validate that the data is available (the 

rsb_valid bit for the register is 0) and the RSB sends ready signal for the source operand.  
The RSB latches the requested source operands until the RSB valid bits are reset before 
sending to the XEQ unknown module 

•  When the source operands are ready, the XEQ unknown checks TRM for: 

o  The read buses are available in next cycle 

o  The functional units are available in 2 cycles 

o  The write bus is available in next cyle + latency time of the instruction 

o  The read ports (RPQ) and write port (WPQ) are set up for issuing the instruction 

in next cycle 

•  The instruction is issued in the next cycle, same as issuing instructions in ID2 stage for 

starting execution in EX0 stage 

 

 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               121 

Cuzco Microarchitecture Specification, v0.2 

 

 

9  Floating-Point Execution Queue 

An initial idea is all decoded floating-point instructions are sent to a possibly modified form of the floating 
point issue and execution pipeline used in the Andes 45-series RISCV processor IP. 

32 floating-point registers are defined by the architecture as well as the fcsr CSR, of which the 8 LSBs are 
defined to hold accrued exceptions (fcsr.fflags) and the current rounding mode (fcsr.frm). Dirty register 
tracking is allowed to enable minimization of function call FPR saves and restores. The length of the 
floating-point registers is both set and readable in FLEN. 

Single-precision floating-point values that are stored in machines with FLEN=64 (i.e., double-precision 
physical registers) are “NaN-boxed” with all upper bits set to 0x1. 

If the machine implements the vector ISA, then FP16 scalar operations must be supported in the FPRs. 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               122 

Cuzco Microarchitecture Specification, v0.2 

 

 

10 Load Store Unit (LSU) 

The LSU is one of the most complicated units in the microprocessor with many possible precise and 
imprecise exceptions, various data sizes, memory RAW, WAR, WAW address dependencies, memory 
address alignment, data bank conflicts, and various possible latency times for both loads and stores.  The 
Cuzco microprocessor is organized and optimized for fixed latency execution times so the latency of both 
load and store instructions are initially set to have 4 cycle execution, based on the cache hit latency. Both 
aligned and unaligned cache hit accesses are 4-cycle latency.  For low power and simplicity, access of the 
data SRAM arrays is serialized after the tag SRAM array(s) are accessed. The memory pipeline stages are: 

1.  LS0 (XEQ): The source operand for the address of the load/store instruction is fetched from the 

PRF.  The read port control, in synchronization, reads the data from PRF for execution. 

2.  LS1 (AGU): The address is calculated by adding the base address register to immediate. The dTLB 

is small, so it is also accessed in this cycle. The physical address is used to access the tag array 
avoiding any virtual-physical address aliasing problems. For a dTLB miss, the instruction is 
replayed based on the L2 TLB hit access time. If the address misses in the L2 TLB a table walk is 
initiated and the load or store instruction is set to “unknown read time” in the LS XEQ. 

3.  LS2 (Tag): The tag addresses are fetched from the tag array and compared to the physical 

address for cache hit/miss determination and hit way selection to access a specific data bank. 
The data bank(s) is selected in this cycle from which to fetch data.  The tag addresses are also 
sent to the STB/DMQ/DEQ for possible matching.  Note that both tag miss and bank conflict are 
known in this cycle but the STB/DMQ/DEQ forwarding can nullify these statuses.  Tag ECC 
detection is also performed. For a tag ECC error, the ROB takes an exception where the 
instruction is re-fetched from the IFU for a 1-bit error and the exception routine is fetched for 
any >1-bit error.   

4.  LS3 (Data Fetch): The load data are fetched from data array.  The STB/DMQ/DEQ are accessed to 

read data for forwarding.  Implementation note: The forwarded data is received by the data 
control unit in this cycle but muxing with the selected data SRAM bank in done the next cycle. It 
may be possible to mux data in this cycle. 

5.  LS4 (Data Align) Load data is aligned if necessary and written to the PRF.  Partial data from an 
unaligned load data is written to the LS XEQ and the appropriate bits are set for replaying the 
load instruction. 

Store data is written to store buffer which will subsequently be written into the DC upon retiring the store 
from the ROB.  All precise exceptions should set the exception bit in the ROB. 

Inst Issue 

LS ports 

Store Buffer 

Load Buffer 

DBQ  MRQ 

DMQ 

DEQ 

LS XEQ 

1 

2 

3 

4 

5 

1 

1 

2 

2 

3 

16 

16 

16 

32 

32 

16 

16 

16 

32 

32 

2 

2 

4 

4 

4 

6 

3 

4 
Table 10-1. LSU Configuration Parameters 

32 

32 

16 

16 

16 

32 

32 

32 

2 

4 

4 

8 

8 

8 

1 

1 

2 

2 

4 

4 

8 

16 

8/16 

16/16 

8/16/16 

16/16/16 

 
 

 

 

 

 

 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               123 

Cuzco Microarchitecture Specification, v0.2 

 

 

 

Figure 10-1 LSU Data Path – Including Replay 

The overall view of the LSU is shown above with all data paths and control signals for replay and cache 
miss. The LS XEQ is an important block for dispatching and replaying load/store instructions, and also for 
combining data from multiple banks and cache-miss at different times, and temporary holding of data 
before it is written back to the PRF. The STB/DMQ/DEQ/MRQ work together to ensure proper merging of 
stores to misses and evicted cache lines. All sections in this chapter are reference to this Figure. 

FENCE.I is synchronization of instruction and data: 

FENCE.I is a serialized instruction as with CSR. 

• 
•  All earlier operations must be completed. 
•  Upon execution of the FENCE.I instruction,all instructions after the FENCE.I are flushed and 

refetched.  

FENCE is data synchronization: 

• 

Each FENCE instruction is stalled in decode until the LS XEQ, MRQ, and DMEQ queues are empty. 

•  A FENCE instruction is treated as load/store miss. 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               124 

PipelineSTBDMQDEQMRQRegister FileTag arraysdTLBAddressTagECC1-bitbypass-on-emptyData BankAGUTLBmissDBQ16Xdata banksSTBforwardTagmissBIUBIUDMQ forwardDEQ forwardBIUmergemergeDMQ writeEvict dataEvict tagDBQnotemptyMiss orBank-conflictNo forwardReplayLSXEQBanksdataDelayDataWriteLoad-miss dataReadstoretoDMQStorematchDEQLS0LS1LS2LS3LS4Store dataCuzco Microarchitecture Specification, v0.2 

 

 

Atomic load/store instructions: 

• 

Treated as load/store miss. 

•  Uncacheable load/store instructions are treated as cache miss. 

10.1 Tag SRAM Array and LS XEQ 

The address width of 64-bits is used. There are two sets of the tag SRAM array(s), each set is 1K cache 
lines organized as 4 banks of 256x52; 1 bank per way associativity.  

The valid array is implemented using register cells, 4 sets of 256 valid bits. The valid array is reset in 8 
cycles, 32 bits per set are reset in each cycle. The tag operation is decoupled from the data operation.  
The tag control unit is the first 2 LSU stages.  Physical Memory Attribute (PMA) and Physical Memory 
Protection (PMP) are not yet implemented. 

Possible tag exceptions are: tag ECC error, dTLB page fault, data watch point, protection violation in dTLB, 
PMA, or PMP. 

The DMEQ and STB monitor the busy status of the tag arrays and sends requests to access the tag arrays 
directly.  

An unaligned address access, where the address LSBs are not zero or a cross cache line access when the 
address+data_size has carry-out at bit 4, are handled as follows: 

1.  Unaligned address in the same cache line:  

o 

Single tag array access 

o 

If the load data are from multiple banks, then two data banks are accessed 

o 

If there is no bank conflict, then the data can be aligned without any additional cycles 

2.  Unaligned address for 2 cache lines:  

o  The address is sent from the tag control unit to the LS XEQ in the LS2 stage, where the 

address is stored in the Immediate data field of a LS XEQ entry.  When the second half is 
replayed, then two source operands are the immediate data and a constant 1. 

o 

For a load instruction the LSU executes the first cache line as normal where the read 
time is modified if first cache line is cache-miss.  When the first cache line is completed, 
the load data is stored in the LS XEQ entry at which time, the second cache line is 
accessed with a replay when a LS port is available (TRM LS port is not busy at 
time_count +2)  using the already valid address in the XEQ entry immediate data field.  
The second cache line data is merged with the data from the first cache line in the LS 
XEQ.  The corresponding entry in LSB is set to unknown. 

o 

For a store instruction the same procedure as above for a load is used to process 2 
addresses.  The STB has 2 address fields which are updated accordingly.  When the store 
instruction is retired, the STB requests to write twice to the DC.  For DC misses, each 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               125 

Cuzco Microarchitecture Specification, v0.2 

 

 

cache line miss independently takes an entry in MRQ where partial store data is written 
into the DMQ when external data is returned from external memory.   

3.  Unaligned address for 2 TLBs: the address crosses a 4K virtual memory page boundary:  

o  Unless there is TLB page fault (e.g., there would be no fault if the page used is >4KB), the 

access is normal. 

o 

(Later implementation) If there is a TLB page fault, the store data are stalled in the MRQ 
until cleared by the second cache line access. 

10.2 Data SRAM Array 

The data SRAM array consists of 1K cache lines of 32B each and is organized as 16 banks of 256x64; 4 
banks per each way of a 4-way associative cache. The number of SRAM columns is changed for DC size 
configuration.  

The data SRAM access is in the LS3 stage.  In most loads, 64-bit data from a single bank are flopped before 
writing back to the PRF in the LS4 stage.  For unaligned load accesses, 2 sets of 64-bit data are flopped and 
aligned in the LS4 stage before writing back to the PRF.  ECC checking is also done in LS4 stage which may 
take an extra cycle to write corrected 1-bit errored data back to the DC data SRAM. 

The DC data SRAM array uses the tag hit way from the tag operation to access only the required data 
bank(s).  Sixteen DBQs are implemented, one for each data bank.  Data bank conflict enters the bank 
queue and the queue position is used to update the new write time for the RSB and LS XEQ.  The data 
array accesses are much more complicated in comparison to tag array accesses because of bank conflicts 
and mixed read and write operations.  

•  Replay of a load instruction due to delays: 

o  Data cache miss: fetch data from external memory 

o  Bank conflict: fetch data again based on the position in the DBQ.  Note: 2 loads with the 
same address going to the same data bank will share data from the same bank which is 
not bank conflict 

• 

Exceptions – For ECC errors, flush the pipeline, similar  as done for a branch misprediction.  An 
ECC 1-bit error is handled by re-fetching of the load instruction while an ECC 2-bit error is 
handled by fetching of the exception routine. 

•  Data forwarding: 

o 

If the load hits in a STBentry and the store data are valid, aligned, and the size are 
matched, then the store data can be forwarded to the load. The store data size can be 
the same or larger than the load data size to forward data.  The addresses for both load 
and store must be aligned in order to forward data. There may multiple entries in the 
STB with the same address, the STB is designed so that the hit is only with the most 
recent or youngest  store entry.  A partial hit in the STB causes the load instruction to be 
replayed in order from the ROB, the LS XEQ entry is marked with “unknown write time”. 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               126 

Cuzco Microarchitecture Specification, v0.2 

 

 

 

o 

o 

o 

If the load hits in the DMQ with no pending entries in the MRQ, then the data can be 
forwarded to the load without asserting cache miss request.  If there are pending 
entries in MRQ, then the load is treated as a miss and takes an entry in MRQ to be 
serviced in order.  Implementation note: the new write time in RSB and LS XEQ for the 
load can be more than the actual write time.  The load data can be written before the 
write time (the DMQ triggers the LS XEQ to start write back), and the corresponding 
valid bit in RSB is clear for RAW dependent instructions.  It impacts performance but not 
functionality. 

If the store hits in the DMQ, then the “tag miss” status in STB is modified as hit with the 
hit tag way read  from the DMQ used for writing to the DC. 

If the load hits in the DEQ, then the data can be forwarded to the load without initiating 
a cache miss request.  Note that the load can hit in both the STB and DEQ, in which case 
forwarding from STB has higher priority.  The rule for a partial hit in the STB is applied. 

o  Random replacement (change to PLRU at later time) is currently used for the DC. The 

selection of the victim  way must be exclude the locked cache ways.  The evicted cache 
line address is compared with the STB for possible matching.  For simplification, all 
evicted “retire” entries in STB must write to the DEQ before the DEQ can be written to 
external memory.  If the evicted STB entry is not yet retired, then the DEQ must wait for 
the ROB to retire the evicted entries. Since the DEQ has multiple entries, the “Last” 
entry in the STB indicates that eviction from the STB is done. Any new store entry with 
the same DEQ address (i.e., a cache miss from the tag array) is treated as cache-miss 
where the same cache line is re-fetched from external memory.  It is difficult to convert 
the STB entry from a hit to a miss because cache miss requests to the MRQ are all from 
the Tag array.  Sending a miss request from the STB to the MRQ is very complicated as 
there can be multiple STB miss entries with the same address and in current 
implementation all miss entries have to be recorded in the MRQ. 

o 

o 

(Later implementation) The cache line can be replaced (not evicted) by the DMQ. If the 
data bank index and replacement way match with a “cache-hit” STB entry, then the 
replaced cache line is evicted instead of just invalidating it.  The evicted cache line 
follows the same procedure as discussed in previous bullet. 

For a branch misprediction or exception, the ROB canceled the store entries starting 
from the STB pointer.  The STB write pointer is decremented in this case.  The related 
MRQ entry and DEQ hit should also be cancelled.  

o  Note: Check for dead-lock since cache miss data is returned OOO and the DEQ is stalled 

for STB eviction data. 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               127 

Cuzco Microarchitecture Specification, v0.2 

 

 

 

Figure 10-2 Load Data Alignment Module 

The above load data modules are duplicated for each write port to the PRF. 

The cache data arrays are configured with 16 banks (4 ways each with 4 banks per 32B cache line size). 
Each data bank has a 4-entry data bank queue (DBQ). The load-hit accesses the data bank with these 3 
cases: 

1.  DBQ is empty: The DBQ is implemented with bypass-on-empty where the bank access is directly 

to the data bank without writing to the DBQ. 

2.  DBQ is partially full (not empty): The LS XEQ entry remains valid and is converted to a replay 

entry for write back data. 
• 

The position in DBQ is added to time_count for the new write time written to the read time 
field of the LS XEQ and write time in the RSB. 
The DBQ triggers the LS XEQ to search for an available write port to write back to the PRF.  
Merging of data from many different sources are discussed in XEQ section. 

• 

3.  DBQ is full: The LS XEQ entry remains valid and is converted to a replay entry with “unknown 

time” and starts from address valid (going through 4 pipeline stages in LSU).  The replay will be 
initiated by the retire in-order of the ROB. 

Load instruction scenarios order of priority: 

• 

• 

STB hit, store data forwarding to load 
STB partial hit, the entry remains in the LS XEQ with an “unknown” read time, and is replayed 
with src1 = address, and src2 = 0 

•  DEQ hit, evicted data forwarding to load 
•  DMQ hit, prefetched data forwarding to load 
• 

Cache hit: 

o  Data bank conflict, time_count + DBQ_position is the new write time in RSB and LS XEQ. 
DBQ trigger LS XEQ to write load data, LS XEQ is responsible to search for a write port to 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               128 

XEQ-LS64-bit64-bitDMEQ - Miss Cache LineDcache Data BanksTo PRFIf no write port --> write to XEQmuxmuxAlign ShiftCuzco Microarchitecture Specification, v0.2 

 

 

write data to PRF.  If no write port is available, then the data are written to LS XEQ.  The 
write time is incremented by 2, LS XEQ continues to search for an available write port 

o  No conflict, including unaligned banks: data is written back to PRF as scheduled 
o  Unaligned load with bank conflict(s), the LS XEQ and RSB are set with the later time.  The 
DBQ triggers the LS XEQ to write load data to the PRF or LS XEQ while awaiting the other 
half of the load data 

• 

Cache miss: 

o 

Load miss uses the L2 cache hit latency time to update the RSB and LS XEQ.  The load 
miss takes an entry in the MRQ, and the MRQ sends the request to the L2 cache.  There 
can be multiple load/store misses to the same address, a link-list is used to process the 
load and store misses in order.  

o  The DMQ data must process all the entries in the MRQ to the same address before they 

are written to the DC. The DMQ triggers the LS XEQ to start the load. 

Data alignment takes data from the STB, DMQ, DEQ, data SRAM arrays, and LS XEQ to produce load data.  
The data control unit can produce: 2 load data in the normal path, 1 load data from the DMQ, and 1 load 
data from the DBQ. 

10.3 Store Buffer (STB) 

The store data are 64-bit. The STB is operated as a FIFO.  Store data can only write to the DC when the 
store instruction is retired from the ROB.  The ROB can retire up to 4 store instructions in one clock cycle. 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               129 

Cuzco Microarchitecture Specification, v0.2 

 

 

Name 

Valid  

Data Valid 

RS2 select 

1 

1 

3 

Bits  

Description 

Entry valid, written when the store instruction is issued 

The store data valid bit is set when source data is written into the STB 

One of the read buses is selected to read store data. The read-port select can be modified to receive 
store data from the replay read port if the write time in RSB is modified. The MSB is to select between 
issue or replay read port 

N is the time count bits. When read_time=time_count, the store data is captured from one of the read 
buses. The write time in the RSB can be modified, the STB read time is written with new write time 
from RSB.  The STB does not check TRM read port nor setting the RPQ.  The STB will check the read 
port busy signal to force reading of the PRF (or forwarding) data.  The read port select is modified for 
the newly select read port 

For timing, the (read_time-1)==time_count, then the bit is set for issuing instruction in next cycle.  
Note that incoming instruction should be compared to time_count to set the ready bit 

At read time, if the write time in RSB is modified, then the replay bit is set for STB to read the data 
again by checking for read port busy signal 

If the read time is unknown, the STB monitors the RSB to see when the data is available 

Data cache miss/hit 

The tag hit way for writing into data bank 

Store data is written from the read ports 

The physical address from the tag array and written along with tag hit/miss and tag way 

The physical address for crossing cache line 

Read time 

N 

Ready 

Replay Valid 

Unknown 

Cache Miss 

Tag hit way 

Store data 

Address 

Address 1 

1 

1 

1 

1 

2 

64 

64 

59 

LS Control bits 

5 

Control bits written when the valid bit is set. Include: execution control bits, RS2 PRF register; where 
Ex_ctrl = (3) load/store, (2) reserved, (1:0) store size 

Sent 

Last 

Retire 

Evict 

XCL 

DEQid 

MRQid 

1 

1 

1 

1 

1 

1 

4 

When the data is sent to MRQ 

Multiple entries in the STB can have the same address, last entry is used for forwarding 

The entry has been retired by the ROB, up to M entries can be retired in a clock cycle by ROB where M 
is the number of issue instructions. 

The eviction bit is set when the “cache-hit” entry matches with data bank index and replaced way, in 
which case, the replaced cache way is evicted regardless of the dirty bit. All retire evicted entries must 
be written to the DEQ before the DEQ entry can write to external memory.  This is valid bit for DEQid 

The store data is unaligned, straddling 2 cache lines, valid bit for Address 1 

1 bit for DEQ with 2 entries. Which DEQ for STB to write retire store data 

Corresponding entry in MRQ for store cache miss 

Exception 
Table 10-2. STB Bit Fields 

1 

Exception – set exception in ROB 

Each store instruction is issued to both the LS XEQ and the STB. The store data is not a condition to stall 
the instruction in IDU. The store data can be written to the STB at any time after the store instruction is 
issued. If the read time is unknown because the source operand has a RAW data dependency with an 
unknown entry in the RSB. The retire status in the STB must be set (in-order) from the ROB. There are 
several pointers in the STB: 

•  Write pointer: to write a new store instruction from the IDU 
•  Retire pointer: to set the retire bit when rob_store signals are asserted 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               130 

Cuzco Microarchitecture Specification, v0.2 

 

 

•  Read pointer: increments when a retired store entry is written to data cache, DEQ, or DMQ. The 

valid bit is reset when the read pointer increases. 

By the retire pointer, the cache-hit store can be written to a DC data bank, conditions to assert the write 
to a data bank: 

•  DBQ is empty 
•  No load access to the same data bank from LS ports which should be known based on tag-hit way 

and address[4:3].  If tag-hit way is a timing path then only address[4:3] is used for conflict 
detection 

•  No DMQ request write to the same STB tag-hit way 
•  No DEQ request write to the same STB tag-hit way 

(Later implementation) For ECC, the partial write to the data bank will be done with a RMW. Thus the STB 
will have 2 accesses to the data bank. The STB goes through the above procedure twice, using 3 cycles to 
write to a data bank: 

•  Read the data bank and flopped data (including ECC error detection) into the LSU, sending data 

to the  STB in next cycle 

•  Merging with store data from STB and writing back to the STB entry 
•  Write to the data bank 
•  Note: if there ECC error correction on the read data is needed, then that data takes an extra cycle 

to be sent to the STB for merging with the store data 

•  Note: if there is an ECC error, an exception is sent to the ROB 
• 

64-bit accesses do not require a RMW sequence and thus arenot an issue with ECC 
Parity is per byte, so no read-modify-write is not needed for parity protected arrays 

• 

Procedure to read  source data from the PRF: 

• 

• 

• 

The store instruction is issued with a preset time to read its data from the PRF.  The RPQ is 
synchronized to the STB to read data from the PRF.  The same procedure with reading source 
data to the XEQ, the read data can be replayed if the write time in the RSB is modified.   
If replay bit is set, then when read_time-1==time_count, the STB checks the read port busy signal 
to read data from the PRF using the selected read port.   The STB entry PRF register name is sent 
to the RPQ to read the PRF in next cycle.  If the read port is not available, then the read time in 
the STB and the write time in the RSB are incremented by 2 and the same procedure is used 
again. 
If the read time is unknown due to an unknown time in the RSB from issue or replay, then the 
unknown bit is set in the STB.  The STB entry waits for the ROB to retire the entry and 
continuously reads the register in RSB until it become invalid in order to read data from the PRF.  
The STB then searches for an available read port from which to read data and clears the 
unknown bit.  If the store is retired from the ROB, then there should be no RAW dependency and 
the store instruction does not need to check the RSB.  

The store can be unaligned and write to 2 data cache banks (and 2 cache lines). The STB knows the status 
of the DC data banks (DBQ) and load operation(s) in the pipeline to write the STB retired entry to the data 
banks. Because of the possibility of an ECC error, only 1 store entry can write to a data bank at a time and 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               131 

Cuzco Microarchitecture Specification, v0.2 

 

 

the ROB entry must remain valid until the store_write_done signal or store_exception signal is asserted.  
In the current implementation both data banks are written at the same time.  Unaligned store data to 2 
different cache lines writes both the Address and Address 1 fields. 

Handling of writing of store data to the DC:   

• 

• 

• 

• 

• 

• 

For a cache hit, when the store instruction is retired, the STB checks for busy signal(s) for specific 
data bank(s) to write data. The DMQ has higher priority to write data to data cache since the 
DMQ has fewer entries.  The store request is continuously asserted until the data bank(s) is 
available.  Multiple banks can be written for an unaligned access. 
The store data with the same address is merged before writing to the same data bank. 
For a cache miss, when the store instruction is retired, the data is moved from the STB to the 
MRQ to be written to the prefetch data cache line in the DMQ. 
For cache misses of both cache lines of an unaligned store, then the store takes 2 entries in MRQ 
and store data are written into both entries when the store instruction is retired by ROB. 
For cache hit and miss of 2 cache lines of the unaligned store, the STB can write data to the data 
array as well as MRQ when the store instruction is retired by ROB. 
The MRQ uses a link-list for all miss addresses to the same cache line and processes the loads 
and stores in-order before writing the cache line to the DC.  If the store instruction is not yet 
retired, then the DMQ waits for the store to retire before writing to data cache. 

The store entries in the STB can have the same address as the replaced and evicted cache lines. Since the 
replaced and evicted cache lines are cache-hit, only the cache line index and tag way are needed for 
comparison: 

• 

(Later implementation) The DMQ cache line index and selected way from the valid/dirty array 
are used to compare to the STB entries. 

o  A match causes the replaced cache line to become an eviction (i.e., requiring write back 

to the L2) regardless of the dirty bit. 

o  All matched entries in the STB must be written to the DEQ before the DEQ can write the 

cache line to external memory.  If the STB entry is not yet retired, then the DEQ must 
wait until the entry is retired before writing to external memory.  The DEQid fields are 
compared to the ensure that all evicted STB entries are written to the DEQ. 
(Later implementation) Subsequent load instruction with the same evicted address:  

• 

o 

o 

o 

If the load fully matches with a STB entry, then the data is forwarded from the STB. 
If the load address matches a DEQ entry without any STB hit, then the data is forwarded 
to the load. 
If the load address partially matches with a STB entry, then it is load miss, taking an 
entry in MRQ, but the request is stalled until the DEQ writes to external memory.  
(Optional) the DEQ writes to a DMQ entry and an fake AXI-ID is used for the miss entry. 

• 

(Later implementation) Subsequent store instruction with the same evicted address:  

o  This is a store miss, the cache miss entry is allocated in the MRQ.  The cache miss must 

wait for the DEQ to write to external memory before sending the request to read the 
same cache line back.  (Optional) the DEQ writes to a DMQ entry and an fake AXI-ID is 
used for the miss entry. 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               132 

Cuzco Microarchitecture Specification, v0.2 

 

 

The STB entry is invalidated and the read pointer is incremented, when the retired store data is written to 
DC bank(s), DMQ, or DEQ.   

The STB entry can be canceled and invalidated by the ROB. In this case the write pointer is decremented 
accordingly.  If the cancelled STB entry is a cache miss, then the corresponding entry in the MRQ is also 
canceled (but not removed) since the linked-list in MRQ is hard to adjust. 

10.4 Load Buffer (LDB) 

Load instructions can be executed OOO with respect to prior store instructions with an invalid address in 
the STB. The entries in the LDB are marked with “OOO.” When a store address is known, perhaps as late 
as when it is retired by the ROB, the store address is compared to all later entries in the LDB and an 
address match will mark the load instruction in ROB as “re-fetch” in the ROB.  Eventually, the ROB will 
send the “re-fetch” load instruction to the IFU/BPU and will flush all subsequent instructions. 

The second major task of the LDB: the older load cannot get newer data than then younger load: 

• 

• 

• 

• 

The older has data dependency and cannot calculate the address while the younger load 
executed out-of-order and completed fetching data from DC to PRF.  Both older and younger 
loads have the same address 
Snoop invalidate happens of the cache line address 
The older load has valid address and match with the younger load 
The younger load is invalidated in the ROB which will cause the younger load to be re-fetched 
from the IFU and subsequent instructions are flushed. 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               133 

Cuzco Microarchitecture Specification, v0.2 

 

 

Name 

Bits   Description 

Valid  

Address Valid 

Cache Miss 

XCL 

1 

1 

1 

1 

Entry valid, written when the load instruction is issued in ID1 stage 

When the load instruction in LS2 stage, after TLB translation 

Data cache miss/hit 

Unaligned access: crossing cache line, straddling 2 cache lines.  This is valid 
bit for Address 1 

Address 

64 

The physical address from the tag array and written along with tag hit/miss 
and tag way 

Address 1 

59 

The physical address for crossing cache line 

LS Control bits 

Done 

Snoop 
Invalidate 

Snoop 
Invalidate 1 

OOO Store 

STBid 

ROBid 

Exception 

4 
 

1 

1 

1 

1 

5 

7 

1 

Control bits written when the valid bit is set. Include: execution control bits, 
RS2 PRF register; where Ex_ctrl = (3) load/store, (2) reserved, (1:0) store size.  
This is used for address comparison with store address and store data width 

When the data is written to PRF 

The snoop address for invalidated cache line matches with Address 

The snoop address for invalidated cache line matches with Address 1 

1 = complete, no comparison to early store is needed 

0 = should be compared to earlier stores until retired 

Corresponding to store instructions in STB 

Corresponding to ROB entry 

Exception – set exception in ROB 

Table 10-3. LDB Bit Fields 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               134 

Cuzco Microarchitecture Specification, v0.2 

 

 

A cache-miss load can be retired by the ROB to allow other load/store instructions to also be retired. A 
cache-miss load can have a bus error exception which is imprecise. If the load instruction is retired from 
the ROB, then it can be removed from the LDB 

A DMQ entry cannot write data into the cache until all earlier entries in the STB and LDB have valid 
addresses to ensure that the replaced/evicted cache line(s) was accessed by the load(s) and/or store(s) in 
a proper, sequentially consistent, order.  In current design, the decode issue and the MRQ is responsible 
to keep the loads and stores in order but the LDB is designed to allow the out-of-order in MRQ, the load is 
invalidated if it is out-of-order with respect to the earlier store (same address).  The ROB entry can be 
valid only if the load/store address is valid. 

More details will be added later when the LDB is implemented. 

•  Because a load can be speculative with respect to a store with no valid address yet, the load 

buffer is implemented for proper forwarding of store to load data. The LDB is implemented as a 
FIFO and load entries are retired synchronized with the ROB. Similar to the STB, the ROB sends 
signals to retire load entries. 

• 

For performance, a load can bypass one or more earlier stores which do not yet have valid 
addresses. When the address of a store is calculated, it should be compared to any later loads.  A 
match causes both the load instruction to be re-fetched from the IFU and the flushing of all 
subsequent instructions by the ROB.  The matched load address access the RSB to retrieve the 
ROB pointer to set the “load-re-fetch” bit in the ROB. 

•  Because of speculation of load instructions with respect to stores instructions, the load buffer is 

implemented.  As load addresses are compared to store addresses in the STB, the store entry 
with invalid store address will set a check bit which requires the store address to be compared to 
entries in the LDB.  Similar to the STB, the LDB entries have a bit (OOO store) to indicate that the 
load is speculative and only speculative entries are compared to the retired store addresses.   

•  A LDB entry can be canceled and invalidated by the ROB. In this case the LDB write pointer is 

decremented accordingly.  If the LDB entry is a cache miss, then the corresponding entry in the 
MRQ is also canceled (but not removed) since the link-list in MRQ is hard to adjust.  

ROB cancel protocol: the cancel protocol is very much similar to the STB, the logic can be copied to cancel 
the entries. 

Snoop protocol: the snoop invalidate address is received and compared to all entries in LDB.  The “cache 
line” address match, will set the “Snoop Invalidate” bit. Signals will be added from the coherency specs of 
L2 cache. 

Procedure: 

1. 

ID1: Receive valid load, write to an entry (setting valid bit, write STBid and ROBid) and increment 
the write pointer.  The LDB write pointer is sent to the idu_d1.sv to attach to the store 
instructions.  When the store instruction address is calculated in LS1 stage of the LSU pipeline, 
the store address is compared to all later entries with “OOO store = 0” from the stb_ldbptr entry 
to the LDB write pointer 

2.  LS2: receiving addresses from load and store instructions: 

•  Receive valid load address, write to the corresponding entry in the LDB: 

i.  Write Address(es), Address valid bit, LS control, Cache miss bit, XCL bit 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               135 

Cuzco Microarchitecture Specification, v0.2 

 

 

ii.  The load address is compared to all later entries; from the load entry to the LDB 

write pointer. 

iii.  Matched cache line address with Snoop Invalidate, Done, Address Valid bits are 

set, then the entry should be re-fetched. 

iv.  The ROBid is used to write “re-fetch” to ROB 

•  Receive valid store address: 

i.  The store address is compared to all later entries; from the stb_ldbptr to the LDB 

write pointer. 

ii.  Matched exact address with Done, Address Valid, and OOO store bits are set, then 

the entry should be re-fetched. Note the exact address has at least 1 byte match 
by using the LS control bits. 

iii.  The ROBid is used to write “re-fetch” to ROB 

3.  LS4: set the Done bit when the load successfully writes data to the PRF.  This can be from many 

types of load write backs – basically from prf_wb_valid with LDB pointer.  

Note:  The LDB pointer needs to be added to all load and store instructions, in ID1, ID2, XEQ, STB, MRQ in 
order to track the load instructions for memory address data dependency. 

Miss-Request Queue (MRQ) with DMQ and DEQ 

Miss requests enter the MRQ in order.  For each cache miss, if there is an earlier STB or LDB entry without 
a valid address, then the cache miss will be replayed from the LS XEQ with “unknown read time” status.  A 
load hit can get the data from DC OOO and be cancelled at a later time, while a DC miss must be 
processed in order. 

Name 

Valid  

Last 

Store  

Retire 

Cancel 

Data valid 

AXI valid 

Data 

Address 

LS control bits 

LS XEQ id 

AXI ID 

Order 
 

Bits   Description 

1 

1 

1 

1 

1 

1 

1 

64 

64 

5 

5 

4 

4 

Entry valid bit 

Last entry of the link-list for all entries with match cache line address. Last is set if there is no address 
match 

Store miss, in absent, load miss 

The entry is retired from ROB, use for only store to indicate that data is valid 

The STB or LDB entry is removed by ROB on branch misprediction.  The cancel bit is set since it is difficult 
to adjust the link list 

Store data is valid, set when STB write data 

AXI ID is valid 

Store data 

Load/store miss address 

The opcode for LS instruction, store bit could be extracted from these control bits 

Corresponding entry in LS XEQ 

Assigned by BIU 

First entry = 0, second entry with address match = 1, … First entry is detected by order==0, then 
increment for next entry, last=1 is the last entry.  This is for all entries with the same AXI ID 

Table 10-4. MRQ Bit Fields 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               136 

Cuzco Microarchitecture Specification, v0.2 

 

 

The protocol for handling of a DC miss and DC line replacement maintains a linked list for misses to the 
same cache line: 

•  A DC miss is sent to the miss request queue (MRQ)  

o 

o 

If the cache miss address does not match any MRQ entry, then set Last=1 and Order=0.  
The cache miss request is sent to external memory and acquires its AXI ID to write into 
its MRQ entry. 
If the cache miss address matches with an MRQ entry with Last=1, then reset the Last 
(i.e., Last=0) of the matched entry and read the AXI ID and Order bits from the matched 
entry to set Last=1, Order=Order+1, and AXI_ID. 

o  The store bit, address, LS control bits, and LS XEQ entry are always written to a new 

MRQ entry. 

• 

The MRQ entry is sent to the STB for store miss, the STB uses the MRQ ID to write store data, 
retire, and cancel bit to MRQ at later time. 

o  Data are returned from external memory to the DMQ. The DMQ entry width is the 

cache line width and has 8 entries.  All 8 DMQ entries can access the MRQ at the same 
time. The logic to send MRQ entries to the DMQ is duplicated. 

▪ 

The AXI ID is sent to MRQ which processes all entries with the matching AXI ID: 
starting with entry with Order=0 (first entry) and incremented the order until 
the entry with Last=1. 

▪  A store entry with both “retire” and “data valid” bits set sends data to write to 

DMQ, otherwise the MRQ stalls until the retire and data valid bits are set. 

▪  A load entry is sent to both LS XEQ and DMQ where the load data are 

processed.  
Cancelled entries are invalidated without sending any information to the DMQ. 
The first entry sends its valid address to the DMQ to write to the DC. 

▪ 
▪ 

DMQ operation: A cache line is received and processed with all valid entries from the MRQ, then the 
DMQ cache line can be written to the DC: 

• 

• 

• 

The DMQ requests the valid/dirty array to select a way for replacement. 
The replacement way and cache line index are compared to STB entrys for a match. If there is a 
STB hit, the replaced cache line will be evicted where the evict bit is set along with writing DEQ 
ID. The cache line is evicted if the dirty bit is set.  The DMQ monitors the tag and data arrays busy 
signals to read the address and data arrays for writing to the DEQ.  For eviction, the DEQ can read 
the tag address and the cache line data independently. 
The DMQ monitors the tag and data arrays busy signals to write the address and data arrays. For 
writing to the DC, both tag arrays and data arrays must be ready at the same time. 

•  Only 1 load miss is serviced at a time from the MRQ to an DMQ entry, but store misses are 

serviced in parallel to multiple DMQ entries as long as the DMQ entry is different than the DMQ 
entry with the load miss.  In different scenarios: (1) if the first entry to of all the DMQ entries are 
store misses, then no load miss is serviced in the cycle, (2) if the first entry of all the DMQ entries 
are load misses, then only 1 load miss entry is serviced in the cycle. 

DEQ operation: Only the retiring STB entry can write to the DEQ and the STB must be cleared of evict bits 
before the DEQ can be written to external memory.  The protocol for writing includes waiting for a “write 
done” or “write error” signal from external memory.  The “write error” is an imprecise exception.  The 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               137 

Cuzco Microarchitecture Specification, v0.2 

 

 

DEQ can send multiple writes to external memory with a AXI ID to confirm the valid writing to external 
memory.  As noted in the STB section, the new load/store instructions with an address matching with that 
in the DEQ are treated as cache misses unless the load address matches with DEQ without any STB hit.  
The new load/store instruction must wait for the DEQ to complete the write before sending the miss 
request to external memory. 

Miss Request Queue (MRQ) – The MRQ is used for both load and store misses and has 48 entries. Since 
all load misses (not counting completed load misses with data forwarding) must remain in the LS XEQ, the 
MRQ must be larger than the LS XEQ.  The store-miss entries are not in the LS XEQ. If the MRQ size is 
equal to LS_XEQ+STB, then the MRQ can never be full which avoids the complication of back-pressure to 
the issue stage. The MRQ is the main control for all load and store misses. 

• 

• 

• 

The MRQ holds an address for every DC miss. 
For multiple load and/or store misses to the same address, the STB store data valid bit is copied 
here in-order for the MRQ to move to the next entry.  If the STB store data is not valid, then the 
MRQ will wait and the DMQ cache line will also wait. 
Each subsequent load instruction must check the MRQ for a hit. If the store data is valid and 64-
bits (last store with the same address), then the store data is forwarded to the load, otherwise 
the load enters the MRQ to wait for the store data to become valid. There are a few possibilities: 
o  Either the load data is unaligned and from the same cache line, then the unaligned load 

o 

can read data from the DMQ at 1 time. 
If the unaligned load crosses the boundary between two cache lines, then a multiple 
load data request is sent to the LS XEQ where the data is merged, aligned, and written 
to the PRF. 

Data miss queue (DMQ) – The DMQ consists of 8 cache-line entries. Data are received OoO from external 
memory. The AXI ID must be checked with the MRQ to get the address and retire information before 
writing to the DC. If there is more than 1 entry with the same address in the MRQ, then the MRQ uses the 
order number (in the above table) to process all the matched retire entries before writing to the DC. 

•  Holds data from external memory. All retire store-miss merging and forwarding are done before 

• 

• 

writing to the data array. The MRQ will stall if the retire store data is not valid. The first cache-
miss MRQ entry must be retired by the ROB before data can be written to the DC. 
Independent with accessing the MRQ, the DMQ accesses the DC valid and dirty arrays for 
replacement way. If eviction is necessary, then the evicted data is read from the DC to the DEQ. 
Eviction should be done before DMQ can write to the DC 
The DMQ has the first priority and is opportunistic to write to the DC. The ready signals from 
both tag arrays and cache data banks are used by the DMQ to assert write signals to tag and data 
arrays.  

Data eviction queue (DEQ) – The DEQ consists of 2 cache-line entries. The DEQ is implemented in the 
same module with the DMQ which is referred to as DMEQ.  The DEQ address may have pending hits in 
both the STB and LDB. This is possible even with a PLRU DC replacement algorithm since all ways can be 
locked except for the single replacement way.  The DEQ entry must remain valid until all pending hits in 
the STB and LDB are processed.  The STB entry cannot be processed until the entry is retired which could 
be a long time.  This is probably an uncommon case.  Once the hit entries are processed, the DEQ can 
request a write to external memory.  The evicted entry remains valid in the DEQ until the “write done” 
signal is received from external memory.  The evicted cache line can be processed OOO.  New load and or 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               138 

Cuzco Microarchitecture Specification, v0.2 

 

 

store instructions can hit with the DEQ address. These cases are discussed in above sections 6.3 and 6.4.  
Data are received OoO from external memory. The AXI ID must be checked with the MRQ to get the 
address and other information before writing into the DC. If there is more than 1 entry with the same 
address, then the MRQ uses a linked list to process all the matched entries before writing to the DC.  

(Later implementation) The DEQ can also be used for non-cacheable stores to external memory. 

Atomic Memory Operations 

Both reservations and atomic load-op-store instructions for 32-bit words and 64-bit doublewords are 
defined. No AMOs are defined for smaller, larger, or non-integer data types. 

The implementation must specify the reservation set size; typically a cache line. The reservation set must 
be naturally aligned, equal to or larger than the reserved word or double word it contains, and smaller 
than or equal to a virtual memory page. 

All LRs and SCs must be performed in sequential pairs. 

All (not only LR & SC) atomic memory operations include acquire and release fields to simplify 
implementation of the Release Consistency with Sequentially Consistency (RCsc) memory ordering model. 
These fields essentially cause constrained FENCE behavior in either but not both the system main memory 
or I/O address ranges based on the AMO address. For example, a LRaq to an I/O address does not enforce 
a fence with main memory loads or stores. 

•  Acquire (not Release) = No subsequent loads or stores may be seen to occur prior to the AMO. 
•  Release (not Acquire) = The AMO can not be seen to occur before any prior loads and stores. 
•  Acquire and Release = The AMO is only executed after all prior memory instructions and prior to 

any following memory instructions. 

Atomic load-op-store instructions that have Rd=x0 can be performed without stalling the hart (i.e., 
requesting processor core) in the memory hierarchy (i.e., addressed cache line’s home agent).  

Deadlock is prevented by requiring cache memory support for miss-less execution of a LR - SC pair with 
associated compares and branches all within a 64-byte instruction cache line. 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               139 

Cuzco Microarchitecture Specification, v0.2 

 

 

11 L2 & L3 Caches & CHI Bus Interface 

The primary goal is to sustain 64bytes total per cycle between both the processor cores and a L3 cache. 
This will be tested with a memory copy operation of a size that exceeds the L2 capacity and takes less 
than (1.1 * bytes copied / 32 bytes) processor clock cycles. This bandwidth will likely be implemented as 
32bytes per cycle full-duplex between the core(s) and the L2 and the L2 and the L3. 

Figure 11.1: Overall L2 Cache, L3 Cache, and CHI BIU shared by one or more cores 

11.1 L2 Cache Overview 

It is unlikely that the initial Cuzco core will generate more than 16bytes per cycle downstream. So the L2 
will naturally support 2 Cuzco cores at their maximum store bandwidth. Cuzco cores will be scalable to 
both higher and lower performance configurations.  Thus, more than 2 Cuzco cores can share one L2 
cache and the amount of cores that a L2 can support will be configurable. The L2 will be interfaced to zero 
(i.e., directly to the BIU) or one L3 cache. 

It is not yet determined if the L1 data cache will operate as write-back or write-through. The L2 will be 
inclusive of all the L1 caches to reduce the L1 error control overhead to simple detection. The L2 tags and 
data will be DECTED (or SECDED) protected since a dirty line with an erroneous tag would require a 
machine error. Since the L2 is inclusive of the L1 caches, it serves as a snoop filter for them, and thus 
should track which L1s may need to invalidate a line involved in a coherence event. 

It is not yet determined if the L2 is either a system point of coherence (PoC) or unification (PoU). In ARM’s 
terminology: The PoC is the point at which all blocks, for example, cores, DSPs, or DMA engines, that can 
access memory are guaranteed to see the same copy of a memory location. Typically, the PoC will be the 
main external system memory or the home agent that manages the addresses involved. The PoU is the 
point at which the all the instruction and data caches of all associated cores are guaranteed to see the 
same copy of a memory location. For example, a unified level 2 cache would be the point of unification in 
a system with Harvard (i.e., split) level 1 caches and a TLB for caching translation table entries. The PoU 
enables self-modifying code to ensure future instruction fetches are correctly made from the modified 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               140 

Cuzco Microarchitecture Specification, v0.2 

 

 

version of the code. Code modification employs a two-stage process with the requisite fences: 1. Clean 
the relevant data cache entries by address; 2. Invalidate instruction cache entries by address. 

It is not yet decided if externally generated TLB shootdown operations will pass through the L2 or have a 
separate path from the BIU to the core.  

The L2 capacity will be configurable in powers of 2 from 256KB to 2MB. The associativity may or may not 
be configurable but will be at least as associative as the combined L1 caches to avoid L1 invalidations and 
misses due to L2 castouts due to set conflicts. The L2 line size will not be configurable but is not yet 
defined. It will likely be 64bytes (possibly each composed of two 32bytes sectors) or 128 bytes (possibly 
composed of four 32bytes sectors or two 64bytes sectors). 

The L2 miss manager capacity will be configurable from 32, 48, or 64 outstanding requests to enable 
integration with a range of customer application memory latencies.  

The cache policy (i.e., allocation, promotion, and victim selection) is not yet determined but will likely 
involve core indications of stall-inducing instruction cache misses, page table, data stack, streaming, and 
prefetching accesses. The L2 will determine which prefetches it will allocate, which will allocate into the 
L3, and which will be dropped. Cache line locking is not yet determined. Cache injection (or stashing) 
initiated by external agents is not yet determined. 

The L2 may be used by the debug facility to pre-load a memory image for processor testing and 
debugging. Thus, in addition to the interfaces to the core(s) and L3(s) and its algorithmic BIST, the debug 
facility must be enabled, possibly through one of these interfaces, to write a portion of or all of the L2 tag 
and data contents. 

11.2 L3 Cache Overview 

The L3 may be directly attached to one or more L2 cache(s) and to the BIU and possibly to other L3 
caches. The amount of L2s that a L3 supports will be configurable. 

The L3 capacity will be configurable as either ~1KB (a very low capacity L3 to buffer L2 castouts awaiting 
BIU service) or in powers of 2 from 256KB to 4MB. The L3 associativity may or may not be configurable. 
The L3 line size will match that of the L2 and not be configurable. It will likely be 64bytes (possibly each 
line composed of two 32bytes sectors) or 128 bytes (possibly composed of four 32bytes sectors or two 
64bytes sectors). The L3 data and tags will both be SECDED protected unless dirty L2 victims are 
simultaneously written back (i.e., cleaned) to their home agent(s), in which case L3 tags need only be 
parity checked.  

The L3 is operated as a victim cache and prefetch buffer for the L2. (The L2 will determine which 
prefetches it will allocate, which will allocate into the L3, and which will be dropped). It may also act as a 
victim cache for one or more L3 caches. The cache policy (i.e., allocation, promotion, and victim selection) 
is not yet determined but will likely match that of the L2. Cache line locking is not yet determined. Cache 
injection (or stashing) initiated by external agents is not yet determined. 

The L3 may be used to filter snoops before presenting them for L2 access arbitration to reduce the snoop 
load on the performance critical L2. The L2 can determine which coherence granules are contested or not 
so that when they are castout from the L2 to the hot granules can be prioritized. 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               141 

Cuzco Microarchitecture Specification, v0.2 

 

 

The L3 may be implemented on a separate power plane from the core so that it can also be used for 
holding core architectural state when the core is powered down. 

The L3 should be architected for optional capacity expansion via a stacked die (e.g., AMD VCache). Other 
vendors claim this is useful for workloads such as Finite Element Analysis, Fluid Dynamics, and Electronic 
Design Automation. (e.g., Ansys, OpenFOAM) 

11.3 Bus Interface Overview 

The system level interface will be CHI5 compliant with a maximum data bandwidth of 64bytes per 
processor cycle. It is  

 

 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               142 

Cuzco Microarchitecture Specification, v0.2 

 

 

12 Adding Functional Units 

A functional unit can be floating-point functional units, CSR, or a custom functional unit.  The new 
functional unit includes new instruction opcode and perhaps additional source operand register for FP 
instructions.  The procedure for adding a new functional unit in general: 

1.  Adding new architectural registers to existing PRF 
2. 

Instruction decode: the new instruction can be added to the current decode tables 
(inst_decode_RV32IAM_d0stage.xlsx and inst_decode_RV32IAM_d2stage.xlsx) or creating new 
decode tables.  The cz_d0_idec.sv is set up to add a new set of decode table and mux in new 
decode parameters base on legal_inst from the decode tables.  The legal_inst from the decode 
tables should be 1-hot  

• 

• 

The d0stage decode table: (1) valid source/destination registers, (2) encoding new 
functional unit, ex_type, – the new functional unit should be set in the define.vh file, (3) 
throughput and latency times, (4) any earlier information for use in decode and issue (i.e. 
uncond for PC calculation).  All valid instruction should set legal_inst, lacking of legal_inst 
is illegal instruction which is exception.  The ex_type increases by 1 bit to have a total of 
16 functional units 
The d2stage decode table: (1) immediate valid bit and generating immediate data – 
setting to zero if no immediate data, (2) functional unit opcode encoding, ex_ctrl, 5-bit is 
sufficient for all functional units, and (3) any information for execution of instruction (i.e. 
unsign).  Instruction decode signals are written into the XEQ. 

3.  Selecting read time from RSB, worse case time from all 3 source operands and load/store times to 

ensure ordering of load/store instructions during execution 

4.  Adding TRM for the new functional unit 
5.  Adding XEQs for the new functional unit 
6.  The read ports and write ports take care of sending the source operand data and grabbing the 

result data from functional unit to write to PRF 

 

12.1 Adding CSR 

Procedure to add CSR: 

1.  The CSR instructions are already included in the decode tables, 

inst_decode_RV32IAM_d0stage.xlsx and inst_decode_RV32IAM_d2stage.xlsx, a new functional 
unit should be used for CSR and the read/write of integer register must be confirmed.  There are 
several CSRs which take more than 1 cycle to read/write a CSR, the CSR latency time should in 
the decode table.   

2.  Further decode of the CSR registers must be detected in D0 decode table to detect the type of 

CSR and whether serialization is needed.  The detection of the CSR register type can be done 
independently from the instruction decode table. Some suggestions for the different CSR types 
are listed below. 
In D1 stage, if serialization is needed, then the instruction will be stalled in D1 stage and wait for 
ROB empty.  Note that the CSR instructions are written into ROB if it is not stalled in this cycle.  
The read/write time of the CSR instructions from the RSB is also done in this cycle 

3. 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               143 

Cuzco Microarchitecture Specification, v0.2 

 

 

4. 

In D2 stage, the CSR instructions are issued to the XEQ-CSR which will be synchronized with the 
RPQ and WPQ to read and write the register files. 

Different types of CSR, ex_ctrl, for different types: 

• 

• 

• 

• 

• 

Pre-serialization: read CSR (status) must wait for ROB empty to ensure that the status is updated. 
Write CSR (control) must wait for ROB empty to ensure that the control information affects only 
the later instructions. 
Post-serialization: write CSR (control) to ensure that the control bits are set before the later 
instructions are in execution. 
CSR is part of a command, i.e. cache maintenance operation.  All the SRAM in the core are 
subjected to cache maintenance operation for reading/writing.  The writing can be part of the 
prefetch operation.  The operation may takes more cycles: 

o  The SRAM arrays should be disabled to avoid any conflict with the maintenance 

operations.  All the SRAM array, BTB, L2 TLB, GHT, IC, and DC must implement disable 
option where the read requests to the arrays are replied with cache misses and the 
write requests are ignored. 

o  The read operation should read the data from the SRAM array to a CSR data register 

where another CSR read operation is necessary for data extraction. 

Post-serialization and stalling of later instructions: stall later instructions until the CSR is 
completed; i.e. setting up data breakpoint, DTLB, PMA, PMP,  
Flushing of later instructions:  assert full to stop the frontend from fetching any more instruction 
(for power).  Set exception for the CSR in ROB to flush instructions; cache maintenance for Icache 
or setting up instruction breakpoint, ITLB, … 

•  No serializations: (1) performance monitor, (2) timer, (3) interrupt registers, (4) clock gate 

enable, .. 

o  Clock gating should be enabled during initialization and can be disabled 

The CSR may have a single latency and throughput. Further decoding is needed for different types.  
Stalling is implemented independently to stall instructions.  “CSR_full” should be used in D1 stage 

12.2 Adding FPU 

FPU is a more complex functional unit to add to the core. In a certain way, a FPU is similar to an ALU 
which requires instruction decode in D0 and D2 stages, accessing RSB in D1 stage, and TRM/XEQ for FPU.  
Requirements for implementing FPU: 

•  Adding decode tables for FP instructions similar to decode tables for ALU instructions. 
• 

The FPU architecture registers are added to ART and RAT.  The PRF and RFL are flexible to 
accommodate both integer and FP registers or increase according to the required performance. 
The FPU may have many functional units, the ex_type must increase by 1 bit to allow a total of 
16 functional units. 
The FPU have a third read port which must be added to the RSB. 

• 

• 

•  A TRM is implemented for each FPU functional unit. 
•  Note: the order of source operand data is according to the instruction opcode. If the FPU 

requires swapping of the source operand data (i.e. FMADD and FMACC), then the condition 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               144 

Cuzco Microarchitecture Specification, v0.2 

 

 

should be in the D2 stage decode.  It is much simpler to swap the source registers than muxing of 
64-bit source operand data in both efficiency and timing. 

•  A XEQ is implemented fro each FPU functional unit, for more efficient design, a single XEQ can be 

shared between all FPU functional units.  Note: the FP divide instruction has the throughput of 
many cycles (similar to the integer divide with 33 and 66 throughput cycles).  The FP divide 
instruction should be set with throughput time of 1.  Consecutive FP divide instructions in the FP 
DIVIDE XEQ will be replayed in the XEQ.  This mechanism is much more efficient than using the 
TRM in its true blocking mechanism. 

• 

•  Adding necessary FP CSR (mode, status, error).  Initially, the CSR can be implemented as part of 
the FPU, perhaps part of the FP MISC functional unit. In this case, the FPU can be independently 
verified. 
FPU functional units are added to the execution unit.  These functional units can be re-used from 
AX45. Note: the input source operands can be switched from one port to another port.  For 
example rs1 can be input data for the third port and rs3 can be input data for the first port.  It is 
much simpler for instruction decode to swap the ports and remove an unnecessary data mux in 
the FPU functional units.   

Once all the above functions are implemented and connected in the core, the FPU can start verification.  
The ultimate performance goal is having 4 sets of FPU in the design.  The FPU takes a much larger area.  It 
is possible to have fewer FPU functional units by forcing the FPU TRM to be busy at all times.  If the 
instruction in issued position 3 cannot be issued than it will be shifted to the earlier position in next cycle. 

The replay mechanism (including unknown) for XEQ-FPU is the same as other functional units.  For 
unknown, the ROB will send the ROB pointer along with ROB type to indicate that the instruction should 
be replayed at this time.  The unknown replay include these steps: 

1.  Send the source operand register (if any) to the register scoreboard to check if register valid bit is 

reset which indicates that the result data from previous instruction has written to the register 
file. 

2.  The source operand data should be read in the next clock cycle, thus the rport1_busy should be 

available in next clock cycle.  The read port should be selected to send to the RPQ and XEQ 

3.  The functional unit should be available in 2 clock cycles by checking trm_falu2_busy 
4.  The write port should be available in 4 clock cycles by checking wport4_busy.  The write port 
should be selected and send to the WPQ and XEQ.  Note that the number associated with the 
busy is the number of cycle from the current time_count. 
If all above conditions are satisfied, then the instruction is issued to the XEQ, RPQ, WPQ 

5. 
6.  The unknown instruction is sent from XEQ to FPU in next clock cycle. 

 

12.3 Procedures for Implementation and Verification 

For verification purpose, the CSR and FPU can be limited to only slice 0 by setting TRM of other slices to 
be busy at all times.  The CSR/FPU instruction in the other slices are stalled and shifted eventually to issue 
position 0 to issue to slice 0.  The verification process is simple for a single slice. 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               145 

Cuzco Microarchitecture Specification, v0.2 

 

 

Eventually, the number of FPU functional units should be incremented to slice 0 and slice 1 for 
verifications.  The performance modelling eventually will make final decision on the number of FPU 
functional units. 

Since there is no performance impact related to CSR, the CSR XEQ should be implemented only in slice 0. 

 

 

 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               146 

Cuzco Microarchitecture Specification, v0.2 

 

 

13 System Configurations 

With above high performance core, L2 and L3 as a basis, myriad system configurations are possible. 

Some examples are listed below for illustrations: 

•  A mobile SoC: a single high performance SoC with 8 cores: 2 big, 2 medium and 4 small, alog with 

a range of GPU, display processor, modem interface, etc. 

Sharing can be one to two L2s 

o 
o  A large L3 catering to diverse workloads on multiple cores 
o  Possibly a scratch pad for non-compute data 
o  a memory controller 
o  high speed interfaces towards memory and IOs 

•  An edge server with ML applications: A set of high performance cores, a large array of small cores 

or ML accelerator PEs 

o  A compute complex with multiple L3 clusters, each connecting to a few L2 clusters, 

which may have single or multiple cores attached 

o  ML accelerator is an array of PEs, sharing data at L3 level: 

▪ 

▪ 

▪ 

Coherent (CXL type 1 or type 2): PE array is attached to the compute cluster 
and sharing is defined by either the accelerator can participate in coherency as 
a peer or the processor provides the concurrency behavior for the accelerator 
as a slave. 
CXL Type3: PE array is closer to memory controller. The PE array can participate 
as a peer in coherency, or is in the non-coherent domain.  
In each of the cases, a large scratch pad at the PE array serves as a conduit for 
the data to and from the processor to the PE array as well as for the data to 
and from a variety of memory storage modules, such as DRAMs, NVMs, SSDs, 
hard disks etc. 

o  A chiplet approach may be possible so that compute complex is in high end technology 

node and accelerator or L3 or scratch pad memories as well as type 3 memory are on 
different technology nodes meeting the cost, performance, power constraints.  

•  A rack server: a large number of high performance cores on each SoC, connected through a large 

network on each board, multiple boards forming a rack, and beyond. 

o  Each compute complex is a set of interconnected L4 or L3 clusters with distributed 

shared memory. If present, each L3 cluster has multiple L3 clusters. Each L3 cluster has 
multiple L2 clusters, which would connect to a large number of cores, privately or 
shared among a small number of cores. 
Interconnection form L3 to L4 and beyond will include data movement using, sometimes 
referred as SmartNICs. 

o 

o  High speed SerDes based protocols such as UCIe between chiplets for the SoC, and PCIe 

for interchip and inter-rack communication over electrical or photonic links would be 
necessary. 

Relevance to the Cuzco design are the considerations as follows: 

Typical L3 latencies between 15-30 cycles (sizes and cluster configurations affect these) 
L4 latencies 25-40 cycles 

• 
• 
•  Minimum memory latencies of 50-100 ns 
•  Remote memory latencies increment by 10-30ns per hop 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               147 

Cuzco Microarchitecture Specification, v0.2 

 

• 

The lower numbers can be aggressive but are needed to optimize the design for high 
performance. 

BigCore1 

MedCore0 

MedCoreN 

SmallCore0 

SmallCoreM 

GPU 

L2,1 

L2,P 

L3,0 

ScratchPad Buffer 

Memory Controller 

DPU 

DDR Channels 

CXL, UCIe/PCIe and High Speed Interfaces 

 

 

 

 

13.1 Synchronizations, Barriers at System Level 

Cuzco design supports multiple fast synchronization mechanisms at the system level. 

1.  A thread initiating the synchronization operation should be able to send messages to all cores 

and threads incurring lowest possible latency. 

2.  Each responding core and thread needs to be able to complete their operations before the 

synchronization and, when necessary, respond back with shortest possible latency to the 
requestor. 

Corresponding support is required from intermediate caches and NoCs to forward necessary messages, 
while holding on to later operations. Multiple levels of spawn and merge operations will facilitate 
achieving lower synchronization latencies. 

 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               148 

Cuzco Microarchitecture Specification, v0.2 

 

 

14 References/Citations 

The text is from the IEEE template. 
 
The template will number citations consecutively within brackets [1]. The sentence punctuation follows 
the bracket [2]. Refer simply to the reference number, as in [3]—do not use “Ref. [3]” or “reference [3]” 
except at the beginning of a sentence: “Reference [3] was the first ...” 

Number footnotes separately in superscripts. Place the actual footnote at the bottom of the column in 
which it was cited. Do not put footnotes in the reference list. Use letters for table footnotes. 

Unless there are six authors or more give all authors’ names; do not use “et al.”. Papers that have not 
been published, even if they have been submitted for publication, should be cited as “unpublished” [4]. 
Papers that have been accepted for publication should be cited as “in press” [5]. Capitalize only the first 
word in a paper title, except for proper nouns and element symbols. 

For papers published in translation journals, please give the English citation first, followed by the original 
foreign-language citation [6]. 

[1]  G. Eason, B. Noble, and I. N. Sneddon, “On certain integrals of Lipschitz-Hankel type involving products of Bessel functions,” 

Phil. Trans. Roy. Soc. London, vol. A247, pp. 529–551, April 1955. (references) 

[2] 

[3] 

J. Clerk Maxwell, A Treatise on Electricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, 1892, pp.68–73. 

I. S. Jacobs and C. P. Bean, “Fine particles, thin films and exchange anisotropy,” in Magnetism, vol. III, G. T. Rado and H. Suhl, 
Eds. New York: Academic, 1963, pp. 271–350. 

[4]  K. Elissa, “Title of paper if known,” unpublished. 

[5]  R. Nicole, “Title of paper with only first word capitalized,” J. Name Stand. Abbrev., in press. 

[6]  Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, “Electron spectroscopy studies on magneto-optical media and plastic  substrate 
interface,” IEEE Transl. J. Magn. Japan, vol. 2, pp. 740–741, August 1987 [Digests 9th Annual Conf. Magnetics Japan, p. 301, 
1982]. 

[7]  M. Young, The Technical Writer’s Handbook. Mill Valley, CA: University Science, 1989. 

 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               149 

Cuzco Microarchitecture Specification, v0.2 

 

 

15 Appendices 

 

15.1 Appendix 1 

 

Condor Computing Inc., 2023, <DRAFT> – <HIGHLY CONFIDENTIAL>                                               150 

